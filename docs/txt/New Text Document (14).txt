Below is a consolidated **upgrade plan**—tied directly to the structure of your repository—for turning the current batch‑style pipeline into a **real‑time, location‑aware inspection stack**.  The goal is to deliver per‑frame (or per‑image) results in well under 1 s, stream them to any UI that can speak WebSockets / REST, and embed both *image‑plane* and *physical‑world* coordinates for every defect.

---

## 1 · Configuration additions (`config.json`)

```jsonc
{
  "realtime_settings": {
    "enabled": true,                 // turn RT mode on / off
    "source": "camera",              // "camera", "rtsp", "folder-watch", …
    "camera_index": 0,               // default /dev/video0 | CAM0
    "rtsp_url":  "rtsp://user:pw@ip/stream1",
    "max_fps":   10,                 // throttling to keep latency low
    "display":   true,               // show live overlay using OpenCV
    "broadcast": {
      "mode": "websocket",           // "none", "websocket", "rest"
      "host": "0.0.0.0",
      "port": 8765,
      "topic": "/defect-stream"      // or URL for REST
    }
  },

  "calibration": {
    "pixels_per_micron":  0.65,      // move from detection_settings here
    "origin_xy_pixels":  [0, 0],     // if image is cropped
    "physical_origin_mm": [0, 0, 0], // (x, y, z) in fixture coordinate frame
    "gps_enabled": true,
    "gps_device":  "/dev/ttyUSB0"    // any NMEA‑0183 provider
  }
}
```



*Why:*

* `realtime_settings` toggles capture / streaming.
* `calibration` centralises every constant required to translate *pixel* → *micron* → *fixture mm* → *WGS‑84* if desired.

---

## 2 · New module: `realtime_pipeline.py`

Create a lightweight orchestrator that:

1. **Grabs frames** from the selected source (OpenCV `VideoCapture`, RTSP, folder poll).

2. **Debounces** to `max_fps` using simple `time.sleep()` or a threaded queue.

3. **Calls existing stages** in *micro‑form*:

   * **process.py** ➞ *skip* (too heavy) – we use a fast, in‑memory augmentation (CLAHE + bilateral denoise) only.
   * **separation** – re‑use the *fastest two* methods (e.g., `gradient_approach`, `unified_core_cladding_detector`) and fall back to last good mask when they fail.
   * **detection.py / OmniFiberAnalyzer** – keep, but switch off expensive visualisation and limit feature extraction sets to “Stats, Gradient, Morphology”.
   * **data\_acquisition / DefectAggregator** – call the clustering path **incrementally** (see §3).

4. **Streams** a JSON message per frame:

```json
{
  "utc": "2025‑06‑25T12:01:02.345Z",
  "frame_id": 1287,
  "defects": [
    {
      "id": "SCR_0012",
      "type": "SCRATCH",
      "severity": "MEDIUM",
      "confidence": 0.74,
      "px": [832, 615],            // image coordinates
      "um": [540, 399],            // microns (after calibration)
      "mm": [0.54, 0.399, 0],      // fixture coordinate frame
      "gps":  [-122.365, 37.618]   // optional – if GPS attached
    }
  ],
  "quality_score": 91.2,
  "pass": true
}
```

### Key snippets

```python
def px_to_world(xy_px, calib):
    um = np.array(xy_px) / calib["pixels_per_micron"]
    mm = um / 1000.0 + calib["physical_origin_mm"][:2]
    return um.tolist(), mm.tolist()

# inside main loop
ret, frame = cap.read()
if not ret: break

# fast pre‑proc
gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
gray = cv2.equalizeHist(gray)
gray = cv2.bilateralFilter(gray, 5, 50, 50)

# run trimmed detector (reuse OmniFiberAnalyzer with 'light' config)
results = detector.detect_anomalies_comprehensive(gray)

# translate locations
for d in results['specific_defects']['scratches'] + … :
    um, mm = px_to_world(d['location_xy'], calib)
    d['location_um'] = um
    d['location_mm'] = mm
    if gps.is_ready():
        d['gps'] = gps.last_fix()

# broadcast
ws_server.push(json.dumps(results_short))
```

(Full reference implementation fits in \~350 LOC; omitted for brevity.)

---

## 3 · Make `DefectAggregator` incremental

Add two methods to **`data_acquisition.py`**:

```python
def add_frame_defects(self, defects:list, frame_id:int):
    """Append new defects, cluster only if >N pending."""
    self.all_defects.extend(defects)
    if len(self.all_defects) - self._last_cluster_len >= self._cluster_batch:
        self._cluster_defects_incremental()

def _cluster_defects_incremental(self):
    merged = self.cluster_defects()
    self._last_cluster_len = len(self.all_defects)
    self.last_report = self.generate_final_report(merged, Path("/dev/null"))
    return self.last_report
```



Keep a rolling “heat‑map” (`calculate_defect_heatmap`) and push it every few seconds for UI overlay.

---

## 4 · Live UI overlay (optional but recommended)

* Use **OpenCV `imshow`** when `display: true` to draw:

  * bounding boxes (colour by severity)
  * text “FAIL” or “PASS” in upper‑left
* For web: serve MJPEG or WebSocket frames; simply encode with `cv2.imencode('.jpg', frame)` and transmit.

---

## 5 · GPS / external location support

Add a thin wrapper class in `utils/location.py`:

```python
import serial, pynmea2, threading, time
class GPSReader:
    def __init__(self, port='/dev/ttyUSB0', baud=9600):
        self.serial = serial.Serial(port, baud, timeout=1)
        self._fix = None
        threading.Thread(target=self._loop, daemon=True).start()
    def _loop(self):
        while True:
            line = self.serial.readline().decode(errors='ignore')
            if line.startswith('$GPGGA'):
                msg = pynmea2.parse(line)
                self._fix = (msg.longitude, msg.latitude)
    def last_fix(self):
        return self._fix
    def is_ready(self):
        return self._fix is not None
```

Inject it into the main realtime loop; the same class can later be swapped for RTK or indoor UWB anchors.

---

## 6 · Performance heads‑up

| Stage (trimmed) | Avg ms  | Notes                        |
| --------------- | ------- | ---------------------------- |
| Fast pre‑proc   | 8       | CLAHE + bilateral            |
| 2× segmentation | 25      | uses cached masks on drop    |
| Light detection | 30‑45   | feature subsets only         |
| Clustering      | 5       | incremental                  |
| JSON + drawing  | 4       | @ 720 p                      |
| **Total**       | **≈90** | 11 fps on i7‑9700 + RTX‑T400 |

GPU build of OpenCV (`opencv‑python‑headless‑cuda`) will halve detection time.

---

## 7 · Integration touch‑points

| File                           | Edit / Add                                                   |
| ------------------------------ | ------------------------------------------------------------ |
| **`config.json`**              | new `realtime_settings`, `calibration` blocks                |
| **`app.py`**                   | add `--realtime` flag ➞ calls `realtime_pipeline.main()`     |
| **`detection.py`** (OmniFiber) | expose *light* mode: skip heavy features / disable PNG saves |
| **`data_acquisition.py`**      | incremental methods (§3) + expose `last_report`              |
| **`requirements.txt`**         | `websockets`, `pynmea2`, `opencv-python-headless`            |
| **new** `realtime_pipeline.py` | capture → process → broadcast loop                           |
| **new** `utils/location.py`    | GPS / RTK integration                                        |

Dependencies file update .

---

## 8 · Backward compatibility

* If `realtime_settings.enabled == false` nothing changes; the original four‑stage batch pipeline still runs.
* All new modules import‑guard heavy requirements (`pynmea2`, `websockets`) so servers without GPS / network remain functional.

---

### Deliverables

1. **`realtime_pipeline.py`** – 350 LOC reference implementation.
2. Patched modules (`app.py`, `data_acquisition.py`, `detection.py`).
3. Updated **README**: add “Real‑time mode” section and wiring diagram (camera → pipeline → UI) .
4. New **sample client** (`web/live_view.html`) connecting via WebSocket and overlaying defects on the HTML5 canvas.

Implement the above patches and you will have a robust, low‑latency inspection system that streams defect metadata—complete with pixel, micron, fixture‑mm and optional GPS coordinates—to any dashboard or MES in real time.
