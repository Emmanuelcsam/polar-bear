
========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/proton.py
========================================

#!/usr/bin/env python3
import argparse
import subprocess
import sys
import logging
import shutil
import traceback
import json
import os
import re
import time
import textwrap
from collections import OrderedDict

__version__ = "1.8"

# Path to your virtual environment activation script and Photon script
VENV_PATH = os.path.expanduser("~/newvenv/bin/activate")
PHOTON_SCRIPT = os.path.join(os.path.dirname(os.path.realpath(__file__)), "photon.py")

def check_command(command):
    """Verify if a command is available in the system PATH."""
    if shutil.which(command) is None:
        logging.error(f"'{command}' is not installed. Please install it to proceed.")
        sys.exit(1)
    logging.debug(f"'{command}' is available.")

def pretty_print_json(output):
    """Attempt to parse and pretty-print JSON output; if it fails, return raw output."""
    try:
        data = json.loads(output)
        return json.dumps(data, indent=2)
    except json.JSONDecodeError:
        return output

def run_command(command, label, debug=False, use_venv=False, ignore_errors=False):
    """
    Execute a command, optionally within a virtual environment.
    Returns the command output as a string.
    """
    try:
        if use_venv:
            # For commands like Photon that must run inside your virtualenv,
            # we assume python3 is available.
            check_command("python3")
        else:
            check_command(command[0])
        logging.debug(f"Executing command: {' '.join(command)}")
        
        if use_venv:
            # Combine the activation command with the target command
            full_command = f"source {VENV_PATH} && " + " ".join(command)
            result = subprocess.run(full_command, shell=True, capture_output=True, text=True, executable="/bin/bash")
        else:
            result = subprocess.run(command, capture_output=True, text=True)

        if result.returncode != 0:
            logging.error(f"Error running {label}: {result.stderr.strip()}")
            if debug:
                logging.debug("Full stderr: " + result.stderr)
            if ignore_errors:
                # Return empty result but don't fail completely
                return ""
            return None
        logging.debug(f"{label} output: {result.stdout.strip()}")
        return result.stdout.strip()
    except Exception as exc:
        logging.error(f"Exception running {label}: {exc}")
        if debug:
            traceback.print_exc()
        return None

def calculate_resource_values(depth):
    """
    Calculate appropriate resource values based on the depth parameter (1-100).
    Returns a dictionary with calibrated values for different tools.
    
    Depth scale:
    1-20: Light/Quick search
    21-50: Medium search
    51-80: Thorough search
    81-100: Deep/Extensive search
    """
    # Ensure depth is within valid range (now 1-100)
    depth = max(1, min(100, depth))
    
    # Calculate ddgr results - maintain a reasonable page size
    # We'll use constant page size of 10 for better UX, but adjust total results
    ddgr_page_size = 10
    
    # Calculate ddgr total results based on depth
    if depth <= 20:
        ddgr_results = 10  # Light search
    elif depth <= 50:
        ddgr_results = 15  # Medium search
    elif depth <= 80:
        ddgr_results = 20  # Thorough search
    else:
        ddgr_results = 25  # Deep search (max for ddgr)
    
    # Calculate Photon crawl parameters
    # Level: 1 to 5 (Photon's internal crawl depth)
    photon_level = 1 + int((depth - 1) / 20)  # Ranges from 1-5
    
    # Threads: 2 to 50 (threading for parallel processing)
    photon_threads = 2 + int((depth - 1) * 0.48)  # Linear scaling
    
    # Timeout: 5s to 30s (longer for deeper crawls)
    photon_timeout = 5 + int((depth - 1) * 0.25)  # Linear scaling
    
    # URLs to crawl per page: proportional to depth
    if depth <= 20:
        urls_percent = 0.3  # Crawl 30% of results for light search
    elif depth <= 50:
        urls_percent = 0.5  # Crawl 50% of results for medium search
    elif depth <= 80:
        urls_percent = 0.7  # Crawl 70% of results for thorough search
    else:
        urls_percent = 1.0  # Crawl 100% of results for deep search
    
    urls_to_crawl = max(1, min(ddgr_page_size, round(ddgr_page_size * urls_percent)))
    
    return {
        "ddgr_results": ddgr_results,
        "ddgr_page_size": ddgr_page_size,
        "photon_threads": photon_threads,
        "photon_level": photon_level,
        "photon_timeout": photon_timeout,
        "urls_to_crawl": urls_to_crawl
    }

def build_ddgr_command(query, ddgr_args, page_size=10):
    """Build the ddgr command with all user-supplied arguments."""
    command = ['ddgr', '--json']
    
    # Ensure we're using a proper page size for ddgr
    command.extend(['--num', str(page_size)])
    
    # Add all ddgr-specific arguments
    if ddgr_args.get('region'):
        command.extend(['--reg', ddgr_args.get('region')])
    if ddgr_args.get('colorize'):
        command.extend(['--colorize', ddgr_args.get('colorize')])
    if ddgr_args.get('nocolor'):
        command.append('--nocolor')
    if ddgr_args.get('colors'):
        command.extend(['--colors', ddgr_args.get('colors')])
    if ddgr_args.get('time'):
        command.extend(['--time', ddgr_args.get('time')])
    if ddgr_args.get('site'):
        for site in ddgr_args.get('site'):
            command.extend(['--site', site])
    if ddgr_args.get('expand'):
        command.append('--expand')
    if ddgr_args.get('proxy'):
        command.extend(['--proxy', ddgr_args.get('proxy')])
    if ddgr_args.get('unsafe'):
        command.append('--unsafe')
    if ddgr_args.get('noua'):
        command.append('--noua')
    
    # Add the query
    command.append(query)
    
    return command

def run_ddgr_with_pagination(query, ddgr_args, depth=30, debug=False, page=1):
    """
    Run DuckDuckGo search using ddgr with proper pagination support.
    This function uses ddgr's interactive mode and simulates pagination commands.
    
    Parameters:
    - query: The search query string
    - ddgr_args: Dictionary of ddgr-specific arguments
    - depth: Depth parameter (1-100)
    - debug: Whether to enable debug output
    - page: Page number (1-based for user display)
    
    Returns:
    - List of URLs from search results
    - Boolean indicating if there might be more results
    """
    resources = calculate_resource_values(depth)
    page_size = resources["ddgr_page_size"]
    
    # For first page, search normally
    if page == 1:
        print(f"\n[+] DuckDuckGo (ddgr) search results for: {query}")
        print(f"[+] Search depth: {depth}/100 (Page {page})\n")
        command = build_ddgr_command(query, ddgr_args, page_size)
    else:
        print(f"\n[+] DuckDuckGo (ddgr) search results - Page {page} for: {query}")
        print(f"[+] Search depth: {depth}/100\n")
        
        # For subsequent pages, we need to simulate pagination:
        # 1. Run ddgr in non-interactive mode first to get initial results
        # 2. Then run multiple "next page" commands to get to the desired page
        
        # First, run the initial query with more results to move through pages faster
        # Use the max limit of 25 to reduce the number of pagination steps needed
        initial_results = 25
        command = ['ddgr', '--json', '--num', str(initial_results), query]
        # Add other args
        if ddgr_args.get('region'):
            command.extend(['--reg', ddgr_args.get('region')])
        if ddgr_args.get('time'):
            command.extend(['--time', ddgr_args.get('time')])
        if ddgr_args.get('site'):
            for site in ddgr_args.get('site'):
                command.extend(['--site', site])
        if ddgr_args.get('unsafe'):
            command.append('--unsafe')
    
    output = run_command(command, "ddgr", debug)
    if not output:
        print("[-] No results from ddgr search")
        return [], False
    
    urls = []
    try:
        results = json.loads(output)
        
        # If we're on a later page, we need to extract the correct subset of results
        result_offset = 0
        if page > 1:
            # For page 2, we want results 10-19 (assuming page_size=10)
            # For page 3, we want results 20-29, etc.
            result_offset = (page - 1) * page_size
            
            # If offset is beyond available results, no more results
            if result_offset >= len(results):
                print("[-] No more results available.")
                return [], False
            
            # Get the slice of results for this page
            end_offset = min(result_offset + page_size, len(results))
            page_results = results[result_offset:end_offset]
        else:
            # First page, just take the first page_size results
            page_results = results[:page_size]
        
        # Process and display the results
        for i, result in enumerate(page_results, 1):
            title = result.get("title", "No Title")
            url = result.get("url", "No URL")
            abstract = result.get("abstract", "")
            
            # Add URLs to the list for Photon crawling
            urls.append(url)
            
            # Display result with correct global index
            global_index = result_offset + i
            print(f"{global_index}. {title}")
            print(f"   {url}")
            if abstract:
                wrapped_abstract = textwrap.fill(abstract, width=80, initial_indent="   ", subsequent_indent="   ")
                print(f"{wrapped_abstract}\n")
            else:
                print()  # Empty line for spacing
        
        # Determine if there might be more results
        # We consider there are more if:
        # 1. We got a full page of results, or
        # 2. We know there are more results in our fetched batch
        has_more = (len(page_results) == page_size) or (result_offset + len(page_results) < len(results))
        
        return urls, has_more
    
    except json.JSONDecodeError:
        logging.error("Failed to parse ddgr JSON output.")
        print(output)
        return [], False

def build_photon_command(target, photon_args, output_dir):
    """Build the Photon command with all user-supplied arguments."""
    command = [
        'python3', PHOTON_SCRIPT,
        '-u', target,
        '-o', output_dir
    ]
    
    # Add all photon-specific arguments
    if photon_args.get('level') is not None:
        command.extend(['-l', str(photon_args.get('level'))])
    if photon_args.get('threads') is not None:
        command.extend(['-t', str(photon_args.get('threads'))])
    if photon_args.get('delay') is not None:
        command.extend(['-d', str(photon_args.get('delay'))])
    if photon_args.get('timeout') is not None:
        command.extend(['--timeout', str(photon_args.get('timeout'))])
    if photon_args.get('cookie'):
        command.extend(['-c', photon_args.get('cookie')])
    if photon_args.get('regex'):
        command.extend(['-r', photon_args.get('regex')])
    if photon_args.get('export'):
        command.extend(['-e', photon_args.get('export')])
    if photon_args.get('seeds'):
        command.extend(['-s'] + photon_args.get('seeds'))
    if photon_args.get('user_agent'):
        command.extend(['--user-agent', photon_args.get('user_agent')])
    if photon_args.get('exclude'):
        command.extend(['--exclude', photon_args.get('exclude')])
    if photon_args.get('proxy'):
        command.extend(['-p', photon_args.get('proxy')])
    
    # Add boolean flags
    if photon_args.get('verbose'):
        command.append('-v')
    if photon_args.get('headers'):
        command.append('--headers')
    if photon_args.get('dns'):
        command.append('--dns')
    if photon_args.get('keys'):
        command.append('--keys')
    if photon_args.get('only_urls'):
        command.append('--only-urls')
    if photon_args.get('wayback'):
        command.append('--wayback')
    
    return command

def run_photon_on_single_target(target, photon_args, depth=30, debug=False, index=None, total=None):
    """Run Photon OSINT on a single target URL with user-specified arguments."""
    resources = calculate_resource_values(depth)
    
    # Use calculated resources if not provided in photon_args
    level = photon_args.get('level') or resources["photon_level"]
    threads = photon_args.get('threads') or resources["photon_threads"]
    timeout = photon_args.get('timeout') or resources["photon_timeout"]
    
    # Create a progress indicator if we're processing multiple URLs
    progress_str = ""
    if index is not None and total is not None:
        progress_str = f"[{index}/{total}] "
    
    print(f"\n[+] {progress_str}Photon crawling target: {target}")
    print(f"[+] Crawl depth: {depth}/100 (level: {level}, threads: {threads}, timeout: {timeout}s)")
    
    if not os.path.isfile(PHOTON_SCRIPT):
        logging.error(f"Photon script not found at {PHOTON_SCRIPT}. Please ensure photon.py is available.")
        return None
    
    # Create organized output folder structure
    main_output_dir = "photon_results"
    if not os.path.exists(main_output_dir):
        os.makedirs(main_output_dir)
    
    # Get domain name for subfolder
    domain = target.replace("https://", "").replace("http://", "").split("/")[0]
    domain_safe = re.sub(r'[^\w\-_]', '_', domain)  # Make domain name safe for filesystem
    
    # Create a unique subfolder for this target
    timestamp = int(time.time())
    target_dir = f"{domain_safe}_{timestamp}"
    output_dir = os.path.join(main_output_dir, target_dir)
    
    # Build the Photon command
    command = build_photon_command(target, photon_args, output_dir)
    
    # Run the command with the proper settings and handle failures gracefully
    output = run_command(command, f"Photon OSINT on {target}", debug, use_venv=True, ignore_errors=True)
    
    # Create the output directory even if the command failed
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        
    if output is not None:
        print(f"[+] Completed scanning {target}")
        print(f"[+] Results saved to {output_dir}/")
        return output_dir
    
    # If the output is None, it means the command completely failed
    print(f"[!] Issues encountered while scanning {target}, but continuing...")
    print(f"[+] Partial results may be available in {output_dir}/")
    return output_dir

def run_photon_on_multiple_targets(targets, photon_args, depth=30, debug=False):
    """Run Photon OSINT on multiple target URLs with user-specified arguments."""
    if not targets:
        print("[-] No targets to crawl with Photon.")
        return
    
    resources = calculate_resource_values(depth)
    max_targets = photon_args.get('max_targets') or resources["urls_to_crawl"]
    
    # Deduplicate targets while preserving order
    unique_targets = list(OrderedDict.fromkeys(targets))
    
    if len(unique_targets) > max_targets:
        print(f"[*] Limiting Photon crawl to top {max_targets} targets based on depth setting {depth}/100")
        targets_to_crawl = unique_targets[:max_targets]
    else:
        targets_to_crawl = unique_targets
    
    print(f"\n[+] Starting Photon crawler on {len(targets_to_crawl)} targets")
    
    results = []
    for i, target in enumerate(targets_to_crawl, 1):
        result_dir = run_photon_on_single_target(
            target, photon_args, depth, debug, i, len(targets_to_crawl)
        )
        if result_dir:
            results.append((target, result_dir))
    
    if results:
        print("\n[+] Photon crawling complete. Summary:")
        print(f"[+] All results saved to the 'photon_results/' directory")
        for target, output_dir in results:
            print(f"  - {target} -> {os.path.basename(output_dir)}/")
    else:
        print("\n[-] No successful Photon crawls.")

def ask_for_more():
    """Ask the user if they want more results."""
    while True:
        answer = input("\nMore results? (y/n): ").lower().strip()
        if answer in ['y', 'yes']:
            return True
        elif answer in ['n', 'no']:
            return False
        else:
            print("Please answer 'y' or 'n'.")

def is_url(text):
    """Check if the given text is a URL."""
    url_pattern = re.compile(r'^https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+')
    return bool(url_pattern.match(text))

def main():
    # Create argument parser with extensive options for both ddgr and photon
    parser = argparse.ArgumentParser(
        description="Enhanced OSINT tool using Photon OSINT and ddgr with pagination support.",
        epilog=("Examples:\n"
                "  python3 proton.py \"search keywords\" 25                          # Search and crawl results with medium depth\n"
                "  python3 proton.py \"search keywords\" \"https://example.com\" 45    # Search, crawl specific site and search results\n"
                "  python3 proton.py --query 'osint tools' --depth 75 --no-crawl   # Search only, no crawling\n"
                "  python3 proton.py --target 'https://example.com' --depth 100     # Crawl only a specific site deeply\n"
                "\nDepth Values (1-100):\n"
                "  1-20:   Quick/light crawl (fewer results, shallow depth, faster)\n"
                "  21-50:  Medium crawl (moderate results and depth)\n"
                "  51-80:  Thorough crawl (more results, deeper level)\n"
                "  81-100: Deep crawl (maximum results and depth level, longer runtime)"),
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    # Basic options
    basic_group = parser.add_argument_group('Basic Options')
    basic_group.add_argument('--query', type=str, 
                    help="Search query for DuckDuckGo (ddgr)")
    basic_group.add_argument('--target', type=str, 
                    help="Target URL for Photon OSINT scraping")
    basic_group.add_argument('--depth', type=int, default=30, 
                    help="Search depth (1-100): 1=quickest, 100=deepest. Controls number of results, crawl depth, and threads")
    basic_group.add_argument('--debug', action='store_true', 
                    help="Enable debug mode for detailed output and error messages")
    basic_group.add_argument('--no-crawl', action='store_true',
                    help="Disable automatic crawling of search results (search only)")
    basic_group.add_argument('--no-pagination', action='store_true',
                    help="Disable pagination ('more' prompt at the end)")
    basic_group.add_argument('--version', action='version', 
                    version=f"%(prog)s {__version__}")
    
    # ddgr-specific options
    ddgr_group = parser.add_argument_group('DuckDuckGo (ddgr) Options')
    ddgr_group.add_argument('--ddgr-region', type=str, metavar='REG', default='us-en',
                    help="region-specific search e.g. 'us-en' for US (default)")
    ddgr_group.add_argument('--ddgr-colorize', type=str, choices=['auto', 'always', 'never'], default='auto',
                    help="whether to colorize output")
    ddgr_group.add_argument('--ddgr-nocolor', action='store_true',
                    help="equivalent to --ddgr-colorize=never")
    ddgr_group.add_argument('--ddgr-colors', type=str, metavar='COLORS',
                    help="set output colors")
    ddgr_group.add_argument('--ddgr-time', type=str, metavar='SPAN', choices=('d', 'w', 'm', 'y'),
                    help="time limit search [d (1 day), w (1 wk), m (1 month), y (1 year)]")
    ddgr_group.add_argument('--ddgr-site', type=str, metavar='SITE', action='append',
                    help="search sites using DuckDuckGo")
    ddgr_group.add_argument('--ddgr-expand', action='store_true',
                    help="Show complete url in search results")
    ddgr_group.add_argument('--ddgr-proxy', type=str, metavar='URI',
                    help="tunnel traffic through an HTTPS proxy; URI format: [http[s]://][user:pwd@]host[:port]")
    ddgr_group.add_argument('--ddgr-unsafe', action='store_true',
                    help="disable safe search")
    ddgr_group.add_argument('--ddgr-noua', action='store_true',
                    help="disable user agent")
    
    # Photon-specific options
    photon_group = parser.add_argument_group('Photon OSINT Options')
    photon_group.add_argument('--photon-level', type=int, metavar='LEVEL', 
                    help="levels to crawl (1-5)")
    photon_group.add_argument('--photon-threads', type=int, metavar='THREADS',
                    help="number of threads")
    photon_group.add_argument('--photon-delay', type=float, metavar='DELAY',
                    help="delay between requests")
    photon_group.add_argument('--photon-timeout', type=float, metavar='TIMEOUT',
                    help="http request timeout")
    photon_group.add_argument('--photon-cookie', type=str, metavar='COOKIE',
                    help="cookie")
    photon_group.add_argument('--photon-regex', type=str, metavar='REGEX',
                    help="regex pattern")
    photon_group.add_argument('--photon-export', type=str, metavar='FORMAT', choices=['csv', 'json'],
                    help="export format (csv, json)")
    photon_group.add_argument('--photon-seeds', type=str, metavar='SEEDS', action='append',
                    help="additional seed URLs")
    photon_group.add_argument('--photon-user-agent', type=str, metavar='UA',
                    help="custom user agent(s)")
    photon_group.add_argument('--photon-exclude', type=str, metavar='REGEX',
                    help="exclude URLs matching this regex")
    photon_group.add_argument('--photon-proxy', type=str, metavar='PROXY',
                    help="Proxy server IP:PORT or DOMAIN:PORT")
    photon_group.add_argument('--photon-verbose', action='store_true',
                    help="verbose output")
    photon_group.add_argument('--photon-headers', action='store_true',
                    help="add headers")
    photon_group.add_argument('--photon-dns', action='store_true',
                    help="enumerate subdomains and DNS data")
    photon_group.add_argument('--photon-keys', action='store_true',
                    help="find secret keys")
    photon_group.add_argument('--photon-only-urls', action='store_true',
                    help="only extract URLs")
    photon_group.add_argument('--photon-wayback', action='store_true',
                    help="fetch URLs from archive.org as seeds")
    photon_group.add_argument('--photon-max-targets', type=int, metavar='N',
                    help="maximum number of targets to crawl (overrides automatic scaling)")
    
    # Positional arguments (for simpler command line usage)
    parser.add_argument('keywords', type=str, nargs='?', 
                    help="Search keywords (e.g., \"George Washington\")")
    parser.add_argument('url_or_depth', type=str, nargs='?',
                    help="Either a URL to crawl (e.g., \"https://example.com\") or depth value (e.g., \"45\")")
    parser.add_argument('positional_depth', type=str, nargs='?',
                    help="Depth value when URL is provided (e.g., \"45\" when using format: \"keywords URL depth\")")
    
    args = parser.parse_args()

    # Configure logging based on the debug flag
    logging_level = logging.DEBUG if args.debug else logging.INFO
    logging.basicConfig(level=logging_level, format='[%(levelname)s] %(message)s')
    
    if args.debug:
        logging.debug("Debug mode enabled.")

    # Process positional arguments if provided
    if args.keywords:
        # We have at least the keywords argument
        query = args.keywords
        target = None
        depth = args.depth  # Default to named argument depth
        
        if args.url_or_depth:
            # Second argument could be either a URL or depth
            if is_url(args.url_or_depth):
                # It's a URL
                target = args.url_or_depth
                # If we have a third positional argument, it's the depth
                if args.positional_depth and args.positional_depth.isdigit():
                    depth = int(args.positional_depth)
            elif args.url_or_depth.isdigit():
                # It's a depth value
                depth = int(args.url_or_depth)
    else:
        # Use named arguments
        query = args.query
        target = args.target
        depth = args.depth
    
    # Ensure depth is within valid range
    if depth is not None:
        depth = max(1, min(100, depth))
    
    # Ensure we have at least one action to perform
    if not query and not target:
        parser.error("You must provide either keywords, a URL, or use --query/--target arguments.")
    
    # Initialize list of URLs to crawl with Photon
    urls_to_crawl = []
    
    # Add specifically provided target URL if any
    if target:
        urls_to_crawl.append(target)
    
    # Prepare ddgr arguments
    ddgr_args = {
        'region': args.ddgr_region,
        'colorize': args.ddgr_colorize,
        'nocolor': args.ddgr_nocolor,
        'colors': args.ddgr_colors,
        'time': args.ddgr_time,
        'site': args.ddgr_site,
        'expand': args.ddgr_expand,
        'proxy': args.ddgr_proxy,
        'unsafe': args.ddgr_unsafe,
        'noua': args.ddgr_noua
    }
    
    # Prepare photon arguments
    photon_args = {
        'level': args.photon_level,
        'threads': args.photon_threads,
        'delay': args.photon_delay,
        'timeout': args.photon_timeout,
        'cookie': args.photon_cookie,
        'regex': args.photon_regex,
        'export': args.photon_export,
        'seeds': args.photon_seeds,
        'user_agent': args.photon_user_agent,
        'exclude': args.photon_exclude,
        'proxy': args.photon_proxy,
        'verbose': args.photon_verbose,
        'headers': args.photon_headers,
        'dns': args.photon_dns,
        'keys': args.photon_keys,
        'only_urls': args.photon_only_urls,
        'wayback': args.photon_wayback,
        'max_targets': args.photon_max_targets
    }
    
    # Run ddgr search if query is provided
    if query:
        page = 1  # Start at page 1 (1-based for user display)
        has_more = True
        
        while has_more:
            # Run search for current page with proper pagination
            search_urls, has_more = run_ddgr_with_pagination(query, ddgr_args, depth, args.debug, page)
            
            # Add search results to crawl list if auto-crawl is enabled
            if not args.no_crawl and search_urls:
                # Run Photon on collected URLs for this page
                run_photon_on_multiple_targets(search_urls, photon_args, depth, args.debug)
            
            # Check if pagination is disabled or if we've reached the end
            if args.no_pagination or not has_more:
                break
                
            # Ask for more results
            if not ask_for_more():
                break
                
            # Move to next page
            page += 1
            
    elif target and not args.no_crawl:
        # If no query but a target was provided, crawl that target
        run_photon_on_single_target(target, photon_args, depth, args.debug)

if __name__ == '__main__':
    main()

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/photon.py
========================================

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""The Photon main part."""
from __future__ import print_function

import argparse
import os
import re
import requests
import sys
import time
import warnings
import random

from core.colors import good, info, run, green, red, white, end, bad

# Just a fancy ass banner
print('''%s      ____  __          __
     / %s__%s \/ /_  ____  / /_____  ____
    / %s/_/%s / __ \/ %s__%s \/ __/ %s__%s \/ __ \\
   / ____/ / / / %s/_/%s / /_/ %s/_/%s / / / /
  /_/   /_/ /_/\____/\__/\____/_/ /_/ %sv1.3.2%s\n''' %
      (red, white, red, white, red, white, red, white, red, white, red, white,
       red, white, end))

try:
    from urllib.parse import urlparse  # For Python 3
except ImportError:
    print('%s Photon runs only on Python 3.2 and above.' % info)
    quit()

import core.config
from core.config import INTELS
from core.flash import flash
from core.mirror import mirror
from core.prompt import prompt
from core.requester import requester
from core.updater import updater
from core.utils import (luhn,
                        proxy_type,
                        is_good_proxy,
                        top_level,
                        extract_headers,
                        verb, is_link,
                        entropy, regxy,
                        remove_regex,
                        timer,
                        writer)
from core.regex import rintels, rendpoint, rhref, rscript, rentropy

from core.zap import zap

# Disable SSL related warnings
warnings.filterwarnings('ignore')

# Processing command line arguments
parser = argparse.ArgumentParser()
# Options
parser.add_argument('-u', '--url', help='root url', dest='root')
parser.add_argument('-c', '--cookie', help='cookie', dest='cook')
parser.add_argument('-r', '--regex', help='regex pattern', dest='regex')
parser.add_argument('-e', '--export', help='export format', dest='export', choices=['csv', 'json'])
parser.add_argument('-o', '--output', help='output directory', dest='output')
parser.add_argument('-l', '--level', help='levels to crawl', dest='level',
                    type=int)
parser.add_argument('-t', '--threads', help='number of threads', dest='threads',
                    type=int)
parser.add_argument('-d', '--delay', help='delay between requests',
                    dest='delay', type=float)
parser.add_argument('-v', '--verbose', help='verbose output', dest='verbose',
                    action='store_true')
parser.add_argument('-s', '--seeds', help='additional seed URLs', dest='seeds',
                    nargs="+", default=[])
parser.add_argument('--stdout', help='send variables to stdout', dest='std')
parser.add_argument('--user-agent', help='custom user agent(s)',
                    dest='user_agent')
parser.add_argument('--exclude', help='exclude URLs matching this regex',
                    dest='exclude')
parser.add_argument('--timeout', help='http request timeout', dest='timeout',
                    type=float)
parser.add_argument('-p', '--proxy', help='Proxy server IP:PORT or DOMAIN:PORT', dest='proxies',
                    type=proxy_type)

# Switches
parser.add_argument('--clone', help='clone the website locally', dest='clone',
                    action='store_true')
parser.add_argument('--headers', help='add headers', dest='headers',
                    action='store_true')
parser.add_argument('--dns', help='enumerate subdomains and DNS data',
                    dest='dns', action='store_true')
parser.add_argument('--keys', help='find secret keys', dest='api',
                    action='store_true')
parser.add_argument('--update', help='update photon', dest='update',
                    action='store_true')
parser.add_argument('--only-urls', help='only extract URLs', dest='only_urls',
                    action='store_true')
parser.add_argument('--wayback', help='fetch URLs from archive.org as seeds',
                    dest='archive', action='store_true')
args = parser.parse_args()


# If the user has supplied --update argument
if args.update:
    updater()
    quit()

# If the user has supplied a URL
if args.root:
    main_inp = args.root
    if main_inp.endswith('/'):
        # We will remove it as it can cause problems later in the code
        main_inp = main_inp[:-1]
# If the user hasn't supplied an URL
else:
    print('\n' + parser.format_help().lower())
    quit()

clone = args.clone
headers = args.headers  # prompt for headers
verbose = args.verbose  # verbose output
delay = args.delay or 0  # Delay between requests
timeout = args.timeout or 6  # HTTP request timeout
cook = args.cook or None  # Cookie
api = bool(args.api)  # Extract high entropy strings i.e. API keys and stuff

proxies = []
if args.proxies:
    print("%s Testing proxies, can take a while..." % info)
    for proxy in args.proxies:
        if is_good_proxy(proxy):
            proxies.append(proxy)
        else:
            print("%s Proxy %s doesn't seem to work or timedout" %
                  (bad, proxy['http']))
    print("%s Done" % info)
    if not proxies:
        print("%s no working proxies, quitting!" % bad)
        exit()
else:
    proxies.append(None)

crawl_level = args.level or 2  # Crawling level
thread_count = args.threads or 2  # Number of threads
only_urls = bool(args.only_urls)  # Only URLs mode is off by default

# Variables we are gonna use later to store stuff
keys = set()  # High entropy strings, prolly secret keys
files = set()  # The pdf, css, png, etc files.
intel = set()  # The email addresses, website accounts, AWS buckets etc.
robots = set()  # The entries of robots.txt
custom = set()  # Strings extracted by custom regex pattern
failed = set()  # URLs that photon failed to crawl
scripts = set()  # THe Javascript files
external = set()  # URLs that don't belong to the target i.e. out-of-scope
# URLs that have get params in them e.g. example.com/page.php?id=2
fuzzable = set()
endpoints = set()  # URLs found from javascript files
processed = set(['dummy'])  # URLs that have been crawled
# URLs that belong to the target i.e. in-scope
internal = set(args.seeds)

everything = []
bad_scripts = set()  # Unclean javascript file urls
bad_intel = set() # needed for intel filtering

core.config.verbose = verbose

if headers:
    try:
        prompt = prompt()
    except FileNotFoundError as e:
        print('Could not load headers prompt: {}'.format(e))
        quit()
    headers = extract_headers(prompt)

# If the user hasn't supplied the root URL with http(s), we will handle it
if main_inp.startswith('http'):
    main_url = main_inp
else:
    try:
        requests.get('https://' + main_inp, proxies=random.choice(proxies))
        main_url = 'https://' + main_inp
    except:
        main_url = 'http://' + main_inp

schema = main_url.split('//')[0] # https: or http:?
# Adding the root URL to internal for crawling
internal.add(main_url)
# Extracts host out of the URL
host = urlparse(main_url).netloc

output_dir = args.output or host

try:
    domain = top_level(main_url)
except:
    domain = host

if args.user_agent:
    user_agents = args.user_agent.split(',')
else:
    with open(sys.path[0] + '/core/user-agents.txt', 'r') as uas:
        user_agents = [agent.strip('\n') for agent in uas]


supress_regex = False

def intel_extractor(url, response):
    """Extract intel from the response body."""
    for rintel in rintels:
        res = re.sub(r'<(script).*?</\1>(?s)', '', response)
        res = re.sub(r'<[^<]+?>', '', res)
        matches = rintel[0].findall(res)
        if matches:
            for match in matches:
                verb('Intel', match)
                bad_intel.add((match, rintel[1], url))


def js_extractor(response):
    """Extract js files from the response body"""
    # Extract .js files
    matches = rscript.findall(response)
    for match in matches:
        match = match[2].replace('\'', '').replace('"', '')
        verb('JS file', match)
        bad_scripts.add(match)

def remove_file(url):
    if url.count('/') > 2:
        replacable = re.search(r'/[^/]*?$', url).group()
        if replacable != '/':
            return url.replace(replacable, '')
        else:
            return url
    else:
        return url

def extractor(url):
    """Extract details from the response body."""
    response = requester(url, main_url, delay, cook, headers, timeout, host, proxies, user_agents, failed, processed)
    if clone:
        mirror(url, response)
    matches = rhref.findall(response)
    for link in matches:
        # Remove everything after a "#" to deal with in-page anchors
        link = link[1].replace('\'', '').replace('"', '').split('#')[0]
        # Checks if the URLs should be crawled
        if is_link(link, processed, files):
            if link[:4] == 'http':
                if link.startswith(main_url):
                    verb('Internal page', link)
                    internal.add(link)
                else:
                    verb('External page', link)
                    external.add(link)
            elif link[:2] == '//':
                if link.split('/')[2].startswith(host):
                    verb('Internal page', link)
                    internal.add(schema + '://' + link)
                else:
                    verb('External page', link)
                    external.add(link)
            elif link[:1] == '/':
                verb('Internal page', link)
                internal.add(remove_file(url) + link)
            else:
                verb('Internal page', link)
                usable_url = remove_file(url)
                if usable_url.endswith('/'):
                    internal.add(usable_url + link)
                elif link.startswith('/'):
                    internal.add(usable_url + link)
                else:
                    internal.add(usable_url + '/' + link)

    if not only_urls:
        intel_extractor(url, response)
        js_extractor(response)
    if args.regex and not supress_regex:
        regxy(args.regex, response, supress_regex, custom)
    if api:
        matches = rentropy.findall(response)
        for match in matches:
            if entropy(match) >= 4:
                verb('Key', match)
                keys.add(url + ': ' + match)


def jscanner(url):
    """Extract endpoints from JavaScript code."""
    response = requester(url, main_url, delay, cook, headers, timeout, host, proxies, user_agents, failed, processed)
    # Extract URLs/endpoints
    matches = rendpoint.findall(response)
    # Iterate over the matches, match is a tuple
    for match in matches:
        # Combining the items because one of them is always empty
        match = match[0] + match[1]
        # Making sure it's not some JavaScript code
        if not re.search(r'[}{><"\']', match) and not match == '/':
            verb('JS endpoint', match)
            endpoints.add(match)


# Records the time at which crawling started
then = time.time()

# Step 1. Extract urls from robots.txt & sitemap.xml
zap(main_url, args.archive, domain, host, internal, robots, proxies)

# This is so the level 1 emails are parsed as well
internal = set(remove_regex(internal, args.exclude))

# Step 2. Crawl recursively to the limit specified in "crawl_level"
for level in range(crawl_level):
    # Links to crawl = (all links - already crawled links) - links not to crawl
    links = remove_regex(internal - processed, args.exclude)
    # If links to crawl are 0 i.e. all links have been crawled
    if not links:
        break
    # if crawled links are somehow more than all links. Possible? ;/
    elif len(internal) <= len(processed):
        if len(internal) > 2 + len(args.seeds):
            break
    print('%s Level %i: %i URLs' % (run, level + 1, len(links)))
    try:
        flash(extractor, links, thread_count)
    except KeyboardInterrupt:
        print('')
        break

if not only_urls:
    for match in bad_scripts:
        if match.startswith(main_url):
            scripts.add(match)
        elif match.startswith('/') and not match.startswith('//'):
            scripts.add(main_url + match)
        elif not match.startswith('http') and not match.startswith('//'):
            scripts.add(main_url + '/' + match)
    # Step 3. Scan the JavaScript files for endpoints
    print('%s Crawling %i JavaScript files' % (run, len(scripts)))
    flash(jscanner, scripts, thread_count)

    for url in internal:
        if '=' in url:
            fuzzable.add(url)

    for match, intel_name, url in bad_intel:
        if isinstance(match, tuple):
            for x in match:  # Because "match" is a tuple
                if x != '':  # If the value isn't empty
                    if intel_name == "CREDIT_CARD":
                        if not luhn(match):
                            # garbage number
                            continue
                    intel.add("%s:%s" % (intel_name, x))
        else:
            if intel_name == "CREDIT_CARD":
                if not luhn(match):
                    # garbage number
                    continue
            intel.add("%s:%s:%s" % (url, intel_name, match))
        for url in external:
            try:
                if top_level(url, fix_protocol=True) in INTELS:
                    intel.add(url)
            except:
                pass

# Records the time at which crawling stopped
now = time.time()
# Finds total time taken
diff = (now - then)
minutes, seconds, time_per_request = timer(diff, processed)

# Step 4. Save the results
if not os.path.exists(output_dir): # if the directory doesn't exist
    os.mkdir(output_dir) # create a new directory

datasets = [files, intel, robots, custom, failed, internal, scripts,
            external, fuzzable, endpoints, keys]
dataset_names = ['files', 'intel', 'robots', 'custom', 'failed', 'internal',
                 'scripts', 'external', 'fuzzable', 'endpoints', 'keys']

writer(datasets, dataset_names, output_dir)
# Printing out results
print(('%s-%s' % (red, end)) * 50)
for dataset, dataset_name in zip(datasets, dataset_names):
    if dataset:
        print('%s %s: %s' % (good, dataset_name.capitalize(), len(dataset)))
print(('%s-%s' % (red, end)) * 50)

print('%s Total requests made: %i' % (info, len(processed)))
print('%s Total time taken: %i minutes %i seconds' % (info, minutes, seconds))
print('%s Requests per second: %i' % (info, int(len(processed) / diff)))

datasets = {
    'files': list(files), 'intel': list(intel), 'robots': list(robots),
    'custom': list(custom), 'failed': list(failed), 'internal': list(internal),
    'scripts': list(scripts), 'external': list(external),
    'fuzzable': list(fuzzable), 'endpoints': list(endpoints),
    'keys': list(keys)
}

if args.dns:
    print('%s Enumerating subdomains' % run)
    from plugins.find_subdomains import find_subdomains
    subdomains = find_subdomains(domain)
    print('%s %i subdomains found' % (info, len(subdomains)))
    writer([subdomains], ['subdomains'], output_dir)
    datasets['subdomains'] = subdomains
    from plugins.dnsdumpster import dnsdumpster
    print('%s Generating DNS map' % run)
    dnsdumpster(domain, output_dir)

if args.export:
    from plugins.exporter import exporter
    # exporter(directory, format, datasets)
    exporter(output_dir, args.export, datasets)

print('%s Results saved in %s%s%s directory' % (good, green, output_dir, end))

if args.std:
    for string in datasets[args.std]:
        sys.stdout.write(string + '\n')

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/summary_collector.py
========================================

#!/usr/bin/env python3

import os
import glob

# Define the path to search
search_path = "/home/jarvis/photon_results"
# Define the output file
output_file = "summary_collection.txt"

# Find all summary.txt files
summary_files = glob.glob(os.path.join(search_path, "**", "summary.txt"), recursive=True)

# Open the output file for writing
with open(output_file, "w") as outfile:
    for file_path in summary_files:
        outfile.write(f"## Contents from: {file_path}\n")
        
        # Read and write the contents of each summary file
        try:
            with open(file_path, "r") as infile:
                outfile.write(infile.read())
        except UnicodeDecodeError:
            outfile.write("Error: Could not read file (possibly binary data)\n")
        
        outfile.write("\n\n")

print(f"Summary collection has been created at {output_file}")

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/protonizer.py
========================================

#!/usr/bin/env python3
"""
WebSum - Photon Results Summarizer

This script:
1. Traverses through /home/jarvis/photon_results and its subfolders
2. Extracts URLs from text files in these folders
3. Fetches and extracts content from these URLs using BeautifulSoup
4. Summarizes content using various methods:
   - Built-in TextRank summarizer (no API required)
   - Ollama models (local)
   - ChatGPT (OpenAI API)
   - Hugging Face models
5. Processes URLs in batches, showing individual summaries and an overall batch summary
6. Asks user if they want to continue after each batch
7. Saves the summaries to a summary.txt file in each subfolder
"""

import os
import re
import json
import time
import argparse
import requests
import sys
import logging
import math
import heapq
import string
import nltk
from bs4 import BeautifulSoup
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import Counter

# Try to download nltk data if it's not already available
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    print("Downloading NLTK punkt tokenizer...")
    nltk.download('punkt', quiet=True)

__version__ = "1.1"

# Default configuration
PHOTON_ROOT = "/home/jarvis/photon_results"
BATCH_SIZE = 10
DEFAULT_OLLAMA_API_URL = "http://localhost:11434/api/generate"
DEFAULT_OPENAI_API_URL = "https://api.openai.com/v1/chat/completions"
DEFAULT_HF_API_URL = "https://api-inference.huggingface.co/models/"
DEFAULT_TIMEOUT = 90  # seconds
MAX_CONTENT_LENGTH = 8000  # characters

# Default API keys (can be overridden by environment variables or user input)
DEFAULT_OPENAI_API_KEY = ""
DEFAULT_HF_API_KEY = ""

# Regular expression for extracting URLs
URL_PATTERN = re.compile(r'https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+(?:/[^)\s]*)?')

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class TextRankSummarizer:
    """Implementation of TextRank algorithm for text summarization"""

    def __init__(self):
        self.stop_words = set([
            'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from',
            'has', 'he', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the',
            'to', 'was', 'were', 'will', 'with'
        ])

    def _clean_text(self, text):
        """Remove special characters and convert to lowercase"""
        text = re.sub(r'\s+', ' ', text)  # Replace multiple spaces with single space
        text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
        return text.lower()

    def _extract_sentences(self, text):
        """Extract sentences from text"""
        return nltk.sent_tokenize(text)

    def _compute_word_frequencies(self, text):
        """Compute word frequencies in the text"""
        word_freq = Counter()
        clean_text = self._clean_text(text)

        for word in clean_text.split():
            if word not in self.stop_words:
                word_freq[word] += 1

        # Normalize frequencies
        max_freq = max(word_freq.values()) if word_freq else 1
        for word in word_freq:
            word_freq[word] = word_freq[word] / max_freq

        return word_freq

    def _score_sentences(self, sentences, word_freq):
        """Score sentences based on word frequency"""
        sentence_scores = {}

        for i, sentence in enumerate(sentences):
            clean_sentence = self._clean_text(sentence)

            # Skip very short sentences (likely not meaningful content)
            if len(clean_sentence.split()) <= 3:
                continue

            for word in clean_sentence.split():
                if word in word_freq:
                    if i not in sentence_scores:
                        sentence_scores[i] = 0
                    sentence_scores[i] += word_freq[word]

        return sentence_scores

    def summarize(self, text, ratio=0.2):
        """Generate a summary using the TextRank algorithm"""
        if not text or len(text.strip()) == 0:
            return ""

        # Extract sentences
        sentences = self._extract_sentences(text)

        # Handle very short texts
        if len(sentences) <= 3:
            return text

        # Calculate word frequencies
        word_freq = self._compute_word_frequencies(text)

        # Score sentences
        sentence_scores = self._score_sentences(sentences, word_freq)

        # Calculate number of sentences for the summary
        num_sentences = max(1, math.ceil(len(sentences) * ratio))

        # Get top-scoring sentence indices
        top_sentence_indices = heapq.nlargest(num_sentences,
                                           sentence_scores,
                                           key=sentence_scores.get)

        # Sort indices to maintain original order
        top_sentence_indices = sorted(top_sentence_indices)

        # Construct summary
        summary = ' '.join([sentences[i] for i in top_sentence_indices])

        return summary

    def extract_keywords(self, text, ratio=0.1):
        """Extract keywords from the text"""
        word_freq = self._compute_word_frequencies(text)

        # Calculate number of keywords
        num_keywords = max(5, math.ceil(len(word_freq) * ratio))

        # Get top words
        keywords = heapq.nlargest(num_keywords, word_freq, key=word_freq.get)

        return ', '.join(keywords)

def setup_argparse():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Extract, fetch, and summarize URLs from Photon results",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument("--root-dir", type=str, default=PHOTON_ROOT,
                        help="Root directory containing Photon results")
    parser.add_argument("--batch-size", type=int, default=BATCH_SIZE,
                        help="Number of URLs to process in each batch")
    parser.add_argument("--timeout", type=int, default=DEFAULT_TIMEOUT,
                        help="Timeout for HTTP requests and API calls (seconds)")
    parser.add_argument("--ollama-url", type=str, default=DEFAULT_OLLAMA_API_URL,
                        help="Ollama API endpoint URL")
    parser.add_argument("--max-content", type=int, default=MAX_CONTENT_LENGTH,
                        help="Maximum content length to send for summarization")
    parser.add_argument("--verbose", action="store_true",
                        help="Enable verbose logging")
    parser.add_argument("--debug", action="store_true",
                        help="Enable debug mode with additional information")
    parser.add_argument("--auto", action="store_true",
                        help="Start in automatic mode (no prompts between batches)")

    return parser.parse_args()

def configure_logging(verbose, debug):
    """Configure logging level based on command line arguments."""
    if debug:
        logger.setLevel(logging.DEBUG)
    elif verbose:
        logger.setLevel(logging.INFO)
    else:
        logger.setLevel(logging.WARNING)

def print_header(text):
    """Print a formatted header."""
    try:
        width = min(os.get_terminal_size().columns, 80)
    except OSError:
        width = 80
    print(f"\n\033[1;36m{'=' * width}\n{text.center(width)}\n{'=' * width}\033[0m\n")

def extract_urls_from_file(file_path):
    """Extract URLs from a file using regex."""
    try:
        with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
            content = f.read()

        # Find all URLs in the content
        urls = URL_PATTERN.findall(content)

        # Remove duplicates while preserving order
        clean_urls = []
        seen = set()
        for url in urls:
            # Clean up the URL to remove trailing punctuation
            url = url.rstrip('.,;:\'\"!?)')

            # Skip duplicates
            if url not in seen:
                seen.add(url)
                clean_urls.append(url)

        return clean_urls
    except Exception as e:
        logger.error(f"Error reading {file_path}: {e}")
        return []

def get_domain(url):
    """Extract domain from URL."""
    parsed = urlparse(url)
    domain = parsed.netloc
    if domain.startswith('www.'):
        domain = domain[4:]
    return domain

def fetch_webpage_content(url, timeout=DEFAULT_TIMEOUT):
    """Fetch and extract text content from a webpage."""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }

        # Make the request but don't raise for status yet
        response = requests.get(url, timeout=timeout, headers=headers)

        # Check specifically for 404 status
        if response.status_code == 404:
            logger.warning(f"404 Not Found: {url}")
            return {
                "title": "404 Not Found",
                "text": "",
                "url": url,
                "status_code": 404,
                "error": True
            }

        # Check for other error status codes
        if response.status_code >= 400:
            logger.warning(f"HTTP Error {response.status_code}: {url}")
            return {
                "title": f"HTTP Error {response.status_code}",
                "text": "",
                "url": url,
                "status_code": response.status_code,
                "error": True
            }

        # If we got here, status code is good, raise for other errors
        response.raise_for_status()

        # Check content type
        content_type = response.headers.get('Content-Type', '').lower()
        if 'text/html' not in content_type and 'application/json' not in content_type:
            return {
                "title": f"Unsupported content type: {content_type}",
                "text": "",
                "url": url,
                "status_code": response.status_code,
                "error": True
            }

        soup = BeautifulSoup(response.content, "html.parser")

        # Extract title
        title_tag = soup.find('title')
        title = title_tag.get_text(strip=True) if title_tag else "No title found"

        # Remove script, style, nav, etc.
        for tag in soup(["script", "style", "nav", "footer", "header"]):
            tag.extract()

        # Extract text from paragraph tags first
        paragraphs = soup.find_all("p")
        if paragraphs:
            page_text = "\n\n".join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
        else:
            # If no paragraphs, try other common content containers
            content_containers = soup.select("article, .content, .main, #content, #main")
            if content_containers:
                page_text = "\n\n".join([el.get_text(strip=True) for el in content_containers])
            else:
                # Fallback to all text
                page_text = soup.get_text(separator="\n\n", strip=True)

        # Clean up text
        page_text = re.sub(r'\n{3,}', '\n\n', page_text)
        page_text = re.sub(r'\s{2,}', ' ', page_text)

        # Return content with success status
        return {
            "title": title,
            "text": page_text,
            "url": url,
            "status_code": response.status_code,
            "error": False
        }
    except Exception as e:
        logger.error(f"Error fetching {url}: {str(e)}")
        return {
            "title": f"Error: {str(e)[:100]}",
            "text": "",
            "url": url,
            "status_code": 0,  # Use 0 to indicate a connection/non-HTTP error
            "error": True
        }

def check_ollama_connection(url, timeout=5):
    """Check if Ollama server is running and reachable."""
    try:
        base_url = url
        if "/api/generate" in url:
            base_url = url.split("/api/generate")[0]

        response = requests.get(f"{base_url}/api/tags", timeout=timeout)
        response.raise_for_status()

        # Check if we got a valid response with models
        data = response.json()
        if "models" in data and len(data["models"]) > 0:
            logger.debug(f"Available Ollama models: {[m['name'] for m in data['models']]}")
            return True
        else:
            logger.warning("No models found in Ollama")
            return False
    except Exception as e:
        logger.error(f"Ollama connection failed: {e}")
        return False

def create_spinner():
    """Create a simple spinner to indicate progress during API calls."""
    import itertools
    import threading

    spinner_active = [True]
    spinner_thread = None

    def spin():
        for c in itertools.cycle('|/-\\'):
            if not spinner_active[0]:
                break
            sys.stdout.write(f"\r{c} ")
            sys.stdout.flush()
            time.sleep(0.1)
        sys.stdout.write('\r')
        sys.stdout.flush()

    spinner_thread = threading.Thread(target=spin)
    spinner_thread.daemon = True
    spinner_thread.start()

    def stop_spinner():
        spinner_active[0] = False
        if spinner_thread:
            spinner_thread.join(0.5)

    return stop_spinner

def summarize_with_textrank(content, depth, timeout=DEFAULT_TIMEOUT):
    """Summarize content using TextRank algorithm."""
    text = content["text"]
    if not text:
        return "No content to summarize."

    print("Generating summary with TextRank...", end="", flush=True)
    stop_spinner = create_spinner()

    try:
        # Create TextRank summarizer instance
        summarizer = TextRankSummarizer()

        # Adjust ratio based on depth parameter
        if depth == "short":
            ratio = 0.05  # Very short summary
        elif depth == "medium":
            ratio = 0.1   # Medium summary
        else:  # detailed
            ratio = 0.3   # Detailed summary

        # Apply summarization
        summary = summarizer.summarize(text, ratio=ratio)

        # If summary is empty (happens with short texts), return the original text
        if not summary:
            if len(text) > 200:
                summary = text[:200] + "..."
            else:
                summary = text

        # Add keywords if the depth is medium or detailed
        if depth in ["medium", "detailed"]:
            try:
                key_terms = summarizer.extract_keywords(text, ratio=0.01)
                if key_terms:
                    summary += f"\n\nKey terms: {key_terms}"
            except Exception as e:
                logger.warning(f"Error extracting keywords: {e}")

        stop_spinner()
        print(" Done!")
        return summary
    except Exception as e:
        stop_spinner()
        print(f" Error: {str(e)[:100]}")
        logger.error(f"TextRank summarization error: {e}")
        return f"Error generating summary: {str(e)[:100]}"

def summarize_batch_with_textrank(summaries):
    """Generate an overall summary of a batch using TextRank summarization."""
    # Combine all the individual summaries into one text
    combined_text = "\n\n".join([f"Title: {s['title']}\nSummary: {s['summary']}"
                                for s in summaries
                                if s["summary"] and not s["summary"].startswith("Error")])

    if not combined_text:
        return "No valid summaries to create an overall summary."

    try:
        # Use the TextRank algorithm to extract the key points
        summarizer = TextRankSummarizer()
        overall_summary = summarizer.summarize(combined_text, ratio=0.3)

        # If summary is empty, use a default message
        if not overall_summary:
            overall_summary = "The batch contains varied content without clear unifying themes."

        # Add keywords to provide additional context
        try:
            key_terms = summarizer.extract_keywords(combined_text, ratio=0.02)
            if key_terms:
                overall_summary += f"\n\nCommon themes across this batch: {key_terms}"
        except Exception:
            pass  # Ignore keyword errors in batch summary

        return overall_summary
    except Exception as e:
        logger.error(f"Error generating batch summary with TextRank: {e}")
        return "Could not generate an overall batch summary."

def summarize_with_ollama(content, model, depth, api_url=DEFAULT_OLLAMA_API_URL, timeout=DEFAULT_TIMEOUT):
    """Summarize content using Ollama API."""
    # Truncate content if too long
    text = content["text"]
    if len(text) > MAX_CONTENT_LENGTH:
        text = text[:MAX_CONTENT_LENGTH] + "... [truncated]"

    # Build prompt based on depth
    if depth == "short":
        prompt = f"Summarize the following content in 1-2 concise sentences. Provide any additional context or knowledge you have that's relevant to the topic:\n\n{text}"
    elif depth == "medium":
        prompt = f"Summarize the following content in one paragraph (3-5 sentences). Include important details and provide any additional context or knowledge you have that's relevant to the topic:\n\n{text}"
    else:  # detailed
        prompt = f"Provide a detailed summary of the following content, including important details, relevant facts, and any additional context or knowledge you have that's relevant to the topic:\n\n{text}"

    print("Generating summary with Ollama...", end="", flush=True)
    stop_spinner = create_spinner()

    try:
        response = requests.post(
            api_url,
            json={"model": model, "prompt": prompt, "stream": False},
            timeout=timeout
        )
        response.raise_for_status()
        result = response.json()
        summary = result.get("response", "No summary generated.")

        stop_spinner()
        print(" Done!")
        return summary.strip()
    except Exception as e:
        stop_spinner()
        print(f" Error: {str(e)[:100]}")
        logger.error(f"Ollama API error: {e}")
        return f"Error generating summary: {str(e)[:100]}"

def summarize_with_openai(content, api_key, depth, timeout=DEFAULT_TIMEOUT):
    """Summarize content using OpenAI API."""
    text = content["text"]
    if len(text) > MAX_CONTENT_LENGTH:
        text = text[:MAX_CONTENT_LENGTH] + "... [truncated]"

    if depth == "short":
        system_prompt = "You are a skilled summarizer. Create a 1-2 sentence summary capturing the key points. Include any additional context or knowledge you have that's relevant to the topic."
    elif depth == "medium":
        system_prompt = "You are a skilled summarizer. Create a single paragraph summary (3-5 sentences) capturing the main ideas. Include any additional context or knowledge you have that's relevant to the topic."
    else:  # detailed
        system_prompt = "You are a skilled summarizer. Create a detailed summary capturing important details and relevant facts. Include any additional context or knowledge you have that's relevant to the topic."

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }

    data = {
        "model": "gpt-3.5-turbo",
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": text}
        ]
    }

    print("Generating summary with ChatGPT...", end="", flush=True)
    stop_spinner = create_spinner()

    try:
        response = requests.post(
            DEFAULT_OPENAI_API_URL,
            headers=headers,
            json=data,
            timeout=timeout
        )
        response.raise_for_status()
        result = response.json()
        summary = result["choices"][0]["message"]["content"]

        stop_spinner()
        print(" Done!")
        return summary.strip()
    except Exception as e:
        stop_spinner()
        print(f" Error: {str(e)[:100]}")
        logger.error(f"OpenAI API error: {e}")
        return f"Error generating summary: {str(e)[:100]}"

def summarize_with_huggingface(content, api_key, model=None, depth="medium", timeout=DEFAULT_TIMEOUT):
    """Summarize content using Hugging Face API."""
    if not model:
        model = "facebook/bart-large-cnn"

    text = content["text"]
    if len(text) > MAX_CONTENT_LENGTH:
        text = text[:MAX_CONTENT_LENGTH] + "... [truncated]"

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    # Adjust parameters based on depth
    parameters = {}
    if depth == "short":
        parameters = {"max_length": 50, "min_length": 20}
    elif depth == "medium":
        parameters = {"max_length": 100, "min_length": 50}
    else:  # detailed
        parameters = {"max_length": 200, "min_length": 100}

    payload = {
        "inputs": text,
        "parameters": parameters
    }

    print(f"Generating summary with Hugging Face ({model})...", end="", flush=True)
    stop_spinner = create_spinner()

    try:
        response = requests.post(
            f"{DEFAULT_HF_API_URL}{model}",
            headers=headers,
            json=payload,
            timeout=timeout
        )
        response.raise_for_status()
        result = response.json()

        stop_spinner()
        print(" Done!")

        if isinstance(result, list) and len(result) > 0:
            return result[0].get("summary_text", "No summary generated.").strip()
        else:
            return "No summary generated."
    except Exception as e:
        stop_spinner()
        print(f" Error: {str(e)[:100]}")
        logger.error(f"Hugging Face API error: {e}")
        return f"Error generating summary: {str(e)[:100]}"

def summarize_content(content, provider, model, depth, api_keys, api_urls, timeout=DEFAULT_TIMEOUT):
    """Summarize content using the selected provider."""
    if not content["text"]:
        return f"No content available for {content['url']}"

    if provider == "textrank":
        return summarize_with_textrank(content, depth, timeout)
    elif provider == "ollama":
        return summarize_with_ollama(content, model, depth, api_urls["ollama"], timeout)
    elif provider == "openai":
        if not api_keys["openai"]:
            return "Error: OpenAI API key not provided."
        return summarize_with_openai(content, api_keys["openai"], depth, timeout)
    elif provider == "huggingface":
        if not api_keys["huggingface"]:
            return "Error: Hugging Face API key not provided."
        return summarize_with_huggingface(content, api_keys["huggingface"], model, depth, timeout)
    else:
        return f"Error: Unknown provider '{provider}'."

def process_batch(urls, provider, model, depth, api_keys, api_urls, timeout=DEFAULT_TIMEOUT, automatic_mode=False):
    """Process a batch of URLs: fetch, summarize, and display results."""
    valid_contents = []
    invalid_urls = []
    summaries = []
    all_404 = True  # Flag to track if all URLs result in 404 errors

    print(f"\nProcessing {len(urls)} URLs...")

    # Fetch content from URLs
    contents = []
    with ThreadPoolExecutor(max_workers=min(len(urls), 5)) as executor:
        future_to_url = {executor.submit(fetch_webpage_content, url, timeout): url for url in urls}
        for i, future in enumerate(as_completed(future_to_url), 1):
            url = future_to_url[future]
            try:
                content = future.result()
                # Print appropriate message based on content status
                if content["error"]:
                    if content["status_code"] == 404:
                        # In automatic mode, don't show 404 messages
                        if not automatic_mode:
                            print(f"[{i}/{len(urls)}] Skipping 404 Not Found: {url}")
                        invalid_urls.append(url)
                    else:
                        print(f"[{i}/{len(urls)}] Error: {content['title']} for {url}")
                        contents.append(content)
                        all_404 = False  # Found an error that's not a 404
                else:
                    print(f"[{i}/{len(urls)}] Fetched: {url} ({len(content['text'])} chars)")
                    contents.append(content)
                    valid_contents.append(content)
                    all_404 = False  # Found at least one valid URL
            except Exception as e:
                logger.error(f"Error processing {url}: {e}")
                print(f"[{i}/{len(urls)}] Error fetching {url}: {str(e)[:100]}")
                contents.append({
                    "title": f"Error fetching {url}",
                    "text": "",
                    "url": url,
                    "status_code": 0,
                    "error": True
                })

    # Log the number of URLs being skipped due to 404
    if invalid_urls:
        logger.info(f"Skipping {len(invalid_urls)} URLs due to 404 errors")
        if not automatic_mode:  # Only show in non-automatic mode
            print(f"\n\033[1;33mSkipping {len(invalid_urls)} URLs due to 404 errors\033[0m")

    # Summarize content (excluding 404 pages)
    for i, content in enumerate([c for c in contents if not (c.get("status_code") == 404)], 1):
        if not content["text"]:
            print(f"\n\033[1;33m[{i}/{len(contents) - len(invalid_urls)}] Error: Could not extract content from {content['url']}\033[0m")
            summaries.append({
                "title": content["title"],
                "url": content["url"],
                "summary": "No content could be extracted from this URL."
            })
            continue

        print(f"\n\033[1;33m[{i}/{len(contents) - len(invalid_urls)}] Summarizing: {content['title']}\033[0m")
        summary = summarize_content(content, provider, model, depth, api_keys, api_urls, timeout)

        domain = get_domain(content["url"])
        print(f"\n\033[1;32m[{i}/{len(contents) - len(invalid_urls)}] Summary for {content['title']}\033[0m")
        print(f"\033[0;36m{content['url']}\033[0m \033[0;90m({domain})\033[0m")
        print(f"{summary}")
        print("-" * 60)

        summaries.append({
            "title": content["title"],
            "url": content["url"],
            "summary": summary
        })

    # Generate overall summary for the batch (only for valid contents)
    if valid_contents and summaries and any(s["summary"] and not s["summary"].startswith("Error") for s in summaries):
        print("\nGenerating overall summary for this batch...")

        # Use TextRank's specialized batch summarization if provider is textrank
        if provider == "textrank":
            overall_summary = summarize_batch_with_textrank(summaries)
        else:
            # Create a prompt with all individual summaries for other providers
            overall_prompt = {
                "title": f"Overall summary for batch of {len(summaries)} websites",
                "text": "Based on the following summaries, provide an overall summary that highlights common themes and important points:\n\n" +
                       "\n\n".join([f"Title: {s['title']}\nSummary: {s['summary']}" for s in summaries if s["summary"] and not s["summary"].startswith("Error")])
            }

            overall_summary = summarize_content(overall_prompt, provider, model, depth, api_keys, api_urls, timeout)

        print("\n\033[1;32mOVERALL BATCH SUMMARY:\033[0m")
        print(overall_summary)
        print("=" * 60)

        return summaries, overall_summary, all_404

    return summaries, "No valid summaries generated for this batch.", all_404

def collect_urls_from_folders(root_dir):
    """Collect URLs from all subfolders in the root directory."""
    folder_urls = {}

    if not os.path.isdir(root_dir):
        logger.error(f"Root directory {root_dir} does not exist.")
        return folder_urls

    # List all subfolders
    subfolders = [f for f in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, f))]
    if not subfolders:
        logger.warning(f"No subfolders found in {root_dir}.")
        return folder_urls

    # Process each subfolder
    for subfolder in subfolders:
        subfolder_path = os.path.join(root_dir, subfolder)
        urls = []

        # Get all text files in the subfolder
        text_files = [f for f in os.listdir(subfolder_path)
                      if f.endswith('.txt') and f != 'summary.txt' and os.path.isfile(os.path.join(subfolder_path, f))]

        # Extract URLs from each text file
        for text_file in text_files:
            file_path = os.path.join(subfolder_path, text_file)
            file_urls = extract_urls_from_file(file_path)
            urls.extend(file_urls)

        # Remove duplicates while preserving order
        unique_urls = []
        seen = set()
        for url in urls:
            if url not in seen:
                seen.add(url)
                unique_urls.append(url)

        if unique_urls:
            folder_urls[subfolder] = unique_urls

    return folder_urls

def choose_provider():
    """Let user choose the summarization provider."""
    print("\nChoose a summarization provider:")
    print("1) TextRank (built-in, no API required)")
    print("2) Ollama (local)")
    print("3) OpenAI (ChatGPT)")
    print("4) Hugging Face")

    while True:
        choice = input("Enter your choice (1-4): ").strip()
        if choice == "1":
            return "textrank"
        elif choice == "2":
            return "ollama"
        elif choice == "3":
            return "openai"
        elif choice == "4":
            return "huggingface"
        else:
            print("Invalid choice. Please enter 1, 2, 3, or 4.")

def choose_model(provider):
    """Let user choose the model for the selected provider."""
    if provider == "textrank":
        # TextRank has no model options in this implementation
        return "textrank"

    elif provider == "ollama":
        print("\nChoose an Ollama model:")
        print("1) gemma3:12b")
        print("2) llama3:latest")
        print("3) deepseek-r1:8b")
        print("4) Other (enter name)")

        choice = input("Enter your choice (1-4): ").strip()
        if choice == "1":
            return "gemma3:12b"
        elif choice == "2":
            return "llama3:latest"
        elif choice == "3":
            return "deepseek-r1:8b"
        elif choice == "4":
            return input("Enter the model name: ").strip()
        else:
            print("Invalid choice. Using gemma3:12b.")
            return "gemma3:12b"

    elif provider == "openai":
        print("\nChoose an OpenAI model:")
        print("1) gpt-3.5-turbo")
        print("2) gpt-4 (if available)")

        choice = input("Enter your choice (1-2): ").strip()
        if choice == "1":
            return "gpt-3.5-turbo"
        elif choice == "2":
            return "gpt-4"
        else:
            print("Invalid choice. Using gpt-3.5-turbo.")
            return "gpt-3.5-turbo"

    elif provider == "huggingface":
        print("\nChoose a Hugging Face model:")
        print("1) facebook/bart-large-cnn (summarization)")
        print("2) sshleifer/distilbart-cnn-12-6 (faster summarization)")
        print("3) Other (enter name)")

        choice = input("Enter your choice (1-3): ").strip()
        if choice == "1":
            return "facebook/bart-large-cnn"
        elif choice == "2":
            return "sshleifer/distilbart-cnn-12-6"
        elif choice == "3":
            return input("Enter the model name: ").strip()
        else:
            print("Invalid choice. Using facebook/bart-large-cnn.")
            return "facebook/bart-large-cnn"

    return None

def choose_depth():
    """Let user choose the summarization depth."""
    print("\nChoose summarization depth:")
    print("1) Short (1-2 sentences)")
    print("2) Medium (one paragraph)")
    print("3) Detailed (comprehensive)")

    while True:
        choice = input("Enter your choice (1-3): ").strip()
        if choice == "1":
            return "short"
        elif choice == "2":
            return "medium"
        elif choice == "3":
            return "detailed"
        else:
            print("Invalid choice. Please enter 1, 2, or 3.")

def get_api_keys(provider):
    """Get API keys for the selected provider."""
    api_keys = {
        "openai": DEFAULT_OPENAI_API_KEY,
        "huggingface": DEFAULT_HF_API_KEY
    }

    # No API keys needed for textrank
    if provider == "textrank":
        return api_keys

    if provider == "openai":
        key_input = input(f"\nEnter your OpenAI API key (or press Enter to use default/env var): ").strip()
        if key_input:
            api_keys["openai"] = key_input
        elif os.environ.get("OPENAI_API_KEY"):
            api_keys["openai"] = os.environ.get("OPENAI_API_KEY")

    elif provider == "huggingface":
        key_input = input(f"\nEnter your Hugging Face API key (or press Enter to use default/env var): ").strip()
        if key_input:
            api_keys["huggingface"] = key_input
        elif os.environ.get("HF_API_KEY"):
            api_keys["huggingface"] = os.environ.get("HF_API_KEY")

    return api_keys

def save_summary_to_file(folder_path, batch_summaries, batch_overall_summaries):
    """Save summaries to a file in the folder."""
    summary_path = os.path.join(folder_path, "summary.txt")

    try:
        with open(summary_path, "w", encoding="utf-8") as f:
            f.write("# WEBSITE SUMMARIES\n\n")

            for batch_idx, (summaries, overall) in enumerate(zip(batch_summaries, batch_overall_summaries), 1):
                f.write(f"## Batch {batch_idx}\n\n")

                for i, summary in enumerate(summaries, 1):
                    f.write(f"### {i}. {summary['title']}\n")
                    f.write(f"URL: {summary['url']}\n\n")
                    f.write(f"{summary['summary']}\n\n")

                f.write("### Overall Batch Summary\n\n")
                f.write(f"{overall}\n\n")
                f.write("-" * 40 + "\n\n")

            f.write("# COMPLETE FOLDER SUMMARY\n\n")

            # Create a summary of all batch overall summaries
            if batch_overall_summaries:
                f.write("Based on all batches processed, the websites in this folder cover:\n\n")
                for batch_idx, overall in enumerate(batch_overall_summaries, 1):
                    f.write(f"Batch {batch_idx}: {overall}\n\n")

        print(f"\nSummary saved to: {summary_path}")
        return True
    except Exception as e:
        logger.error(f"Error saving summary to {summary_path}: {e}")
        print(f"Error saving summary: {str(e)}")
        return False

def check_for_key_press():
    """Check if a key has been pressed without blocking execution."""
    try:
        # Check if there's any input ready (non-blocking)
        import select
        ready, _, _ = select.select([sys.stdin], [], [], 0)
        if ready:
            # There's input ready, read it
            key = sys.stdin.readline().strip().lower()
            return key
        return None
    except Exception:
        return None

def main():
    """Main function."""
    args = setup_argparse()
    configure_logging(args.verbose, args.debug)

    print_header("WEBSUMMARIZER - PHOTON RESULTS SUMMARIZER")
    print(f"Version: {__version__}")

    # Choose summarization provider, model, and depth
    provider = choose_provider()
    model = choose_model(provider)
    depth = choose_depth()
    api_keys = get_api_keys(provider)

    api_urls = {
        "ollama": args.ollama_url,
        "openai": DEFAULT_OPENAI_API_URL,
        "huggingface": DEFAULT_HF_API_URL
    }

    # Check Ollama connection if needed
    if provider == "ollama" and not check_ollama_connection(args.ollama_url):
        print("\nError: Cannot connect to Ollama. Make sure Ollama is running.")
        print("You can start Ollama with: ollama serve")
        print(f"And pull the model with: ollama pull {model}")
        return 1

    # Collect URLs from folders
    print(f"\nCollecting URLs from subfolders in {args.root_dir}...")
    folder_urls = collect_urls_from_folders(args.root_dir)

    if not folder_urls:
        print("No URLs found in any subfolder.")
        return 1

    print(f"\nFound {len(folder_urls)} subfolders with URLs:")
    for folder, urls in folder_urls.items():
        print(f"- {folder}: {len(urls)} URLs")

    # Track automatic mode state
    automatic_mode = args.auto
    if automatic_mode:
        print("\nAutomatic mode enabled. Processing will continue until complete or interrupted.")
        print(f"Using batch size of 20 for automatic mode (overrides default of {args.batch_size}).")
        print("Press 'n' then Enter at any time to exit automatic mode.")
        print("Or press Ctrl+C at any time to stop processing and save results.")

    # Process each subfolder
    for folder, urls in folder_urls.items():
        folder_path = os.path.join(args.root_dir, folder)
        print_header(f"Processing folder: {folder}")
        print(f"Found {len(urls)} URLs to process")

        # Process URLs in batches
        batch_summaries = []
        batch_overall_summaries = []

        i = 0
        while i < len(urls):
            # Use batch size of 20 when in automatic mode, otherwise use the specified batch size
            current_batch_size = 20 if automatic_mode else args.batch_size
            batch = urls[i:i + current_batch_size]

            batch_number = i // current_batch_size + 1
            print_header(f"Processing batch {batch_number} ({len(batch)} URLs)")

            summaries, overall_summary, all_404 = process_batch(
                batch, provider, model, depth, api_keys, api_urls, args.timeout, automatic_mode
            )

            batch_summaries.append(summaries)
            batch_overall_summaries.append(overall_summary)

            # Move to next batch
            i += current_batch_size

            # Check if we should continue to the next batch
            if i < len(urls):
                if all_404:
                    # Automatically continue to next batch if all URLs were 404s
                    if not automatic_mode:  # Only show in non-automatic mode
                        print("\nAll URLs in this batch resulted in 404 errors. Automatically continuing to next batch...")
                    continue
                elif automatic_mode:
                    # In fully automatic mode, just continue without prompting
                    # Just print a brief status message
                    print("\nAutomatic mode active - continuing to next batch automatically.")
                    print("(Press 'n' then Enter to exit automatic mode)")
                    # Brief pause to allow for reading messages
                    time.sleep(1.0)

                    # Non-blocking check if 'n' was pressed to exit automatic mode
                    key = check_for_key_press()
                    if key == 'n':
                        automatic_mode = False
                        print("Automatic mode disabled. Will prompt before each batch.")
                        # FIX: Make sure to clearly ask about continuing and enforce input validation
                        while True:
                            choice = input("Continue with next batch? (y/n): ").strip().lower()
                            if choice == 'y':
                                break
                            elif choice == 'n':
                                print("Stopping at user request.")
                                i = len(urls)  # This will exit the outer while loop
                                break
                            else:
                                print("Please enter 'y' or 'n'.")
                else:
                    # FIX: Implement better input validation to ensure only valid inputs are accepted
                    valid_response = False
                    while not valid_response:
                        choice = input("\nContinue with next batch? (y/n/a for automatic mode): ").strip().lower()
                        if choice == 'a':
                            automatic_mode = True
                            print("Automatic mode enabled. Processing will continue until complete or interrupted.")
                            print("Using batch size of 20 for remaining batches.")
                            print("Press 'n' then Enter at any time to exit automatic mode.")
                            valid_response = True
                        elif choice == 'y':
                            valid_response = True
                        elif choice == 'n':
                            print("Stopping at user request.")
                            i = len(urls)  # This will exit the outer while loop
                            valid_response = True
                        else:
                            print("Invalid input. Please enter 'y', 'n', or 'a'.")

        # Save summaries to file
        save_summary_to_file(folder_path, batch_summaries, batch_overall_summaries)

    print_header("SUMMARIZATION COMPLETE")
    return 0

if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        print("\n\nOperation interrupted by user.")
        print("Saving any completed summaries...")
        # Note: The summaries for the completed batches are already saved by the main loop
        print("Summary files have been saved for completed batches.")
        sys.exit(1)

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/wetails.py
========================================

import requests
from bs4 import BeautifulSoup

def extract_website_info(url):
    try:
        response = requests.get(url)
        response.raise_for_status()  # Raises an HTTPError if the HTTP request returned an unsuccessful status code
    except Exception as e:
        print(f"Error fetching the URL: {e}")
        return None

    # Parse the HTML content with BeautifulSoup
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # Extract the title of the page
    title = soup.title.string if soup.title else "No title found"

    # Extract the meta description (if available)
    meta_description = soup.find("meta", attrs={"name": "description"})
    description = meta_description['content'] if meta_description and meta_description.has_attr('content') else "No description available"

    # Extract text from all paragraph elements
    paragraphs = [p.get_text(strip=True) for p in soup.find_all("p")]
    
    return {
        "title": title,
        "description": description,
        "paragraphs": paragraphs,
    }

def main():
    url = input("Enter a website URL: ")
    info = extract_website_info(url)
    if info:
        print("\nWebsite Information:")
        print("Title:", info["title"])
        print("Meta Description:", info["description"])
        print("\nExtracted Paragraphs:\n")
        for p in info["paragraphs"]:
            print(p)
            print("-" * 80)

if __name__ == "__main__":
    main()

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/runner.py
========================================

#!/usr/bin/env python3
"""
OSINT Suite Runner

This script automates running the entire OSINT workflow:
1. Running proton.py to search and crawl websites
2. Running protonizer.py to analyze and summarize the results
3. Starting the app.py web server
4. Opening the frontend in a web browser

Usage:
    python3 osint_runner.py "search keywords" "example.com" 50
"""

import argparse
import subprocess
import time
import webbrowser
import os
import sys
import signal
from pathlib import Path

def parse_arguments():
    parser = argparse.ArgumentParser(description="Run the complete OSINT workflow")
    parser.add_argument("keyword", help="Search keyword for proton.py")
    parser.add_argument("site", nargs='?', default=None, help="Site to search/crawl (optional)")
    parser.add_argument("depth", type=int, help="Depth value (1-300)")
    parser.add_argument("--skip-proton", action="store_true", 
                        help="Skip running proton.py (use existing results)")
    parser.add_argument("--skip-protonizer", action="store_true", 
                        help="Skip running protonizer.py")
    parser.add_argument("--port", type=int, default=5000, 
                        help="Port for the web server (default: 5000)")
    
    # Handle the case where user provides only keyword and depth
    args = parser.parse_args()
    
    # If user provided just two arguments, the second is actually the depth
    if len(sys.argv) == 3 and args.site and args.site.isdigit() and args.depth is None:
        args.depth = int(args.site)
        args.site = None
    
    return args

def run_command(cmd, description):
    """Run a command and handle any errors."""
    print(f"\n[+] {description}...")
    try:
        print(f"[*] Running: {' '.join(cmd)}")
        subprocess.run(cmd, check=True)
        print(f"[+] Command completed successfully")
        return True
    except subprocess.CalledProcessError as e:
        print(f"[!] Error: {e}")
        return False
    except Exception as e:
        print(f"[!] Unexpected error: {e}")
        return False

def start_web_server(port=5000):
    """Start the Flask web server in the background."""
    print(f"\n[+] Starting web server on port {port}...")
    try:
        # Define the correct static directory path
        static_dir = Path("/home/jarvis/Proton/static")
        
        # Make sure the static directory exists
        static_dir.mkdir(exist_ok=True, parents=True)
        print(f"[*] Ensuring static directory exists: {static_dir}")
        
        # Copy frontend.html to the static directory if needed
        frontend_path = Path("frontend.html")
        static_index_path = static_dir / "index.html"
        
        if frontend_path.exists():
            print(f"[*] Copying frontend.html to {static_index_path}")
            with open(frontend_path, "r") as src, open(static_index_path, "w") as dst:
                dst.write(src.read())
        else:
            print(f"[!] Warning: frontend.html not found in current directory")
            # Check if it's already in the destination
            if not static_index_path.exists():
                print(f"[!] Warning: {static_index_path} doesn't exist either!")
                print(f"[*] The web interface may not function correctly")
        
        # Start the server as a background process
        cmd = ["python3", "app.py"]
        server_process = subprocess.Popen(cmd, 
                                         stdout=subprocess.PIPE,
                                         stderr=subprocess.PIPE)
        
        # Give the server a moment to start up
        time.sleep(2)
        
        # Check if the process is still running
        if server_process.poll() is None:
            print(f"[+] Web server started successfully (PID: {server_process.pid})")
            return server_process
        else:
            stdout, stderr = server_process.communicate()
            print(f"[!] Web server failed to start: {stderr.decode()}")
            return None
    except Exception as e:
        print(f"[!] Error starting web server: {e}")
        return None

def open_web_browser(port=5000):
    """Open the web browser to view the interface."""
    url = f"http://localhost:{port}"
    print(f"\n[+] Opening web browser to {url}...")
    try:
        webbrowser.open(url)
        print(f"[+] Browser opened successfully")
        return True
    except Exception as e:
        print(f"[!] Failed to open browser: {e}")
        print(f"[+] Please manually navigate to {url}")
        return False

def main():
    args = parse_arguments()
    
    # Run proton.py if not skipped
    if not args.skip_proton:
        # Build the command based on whether a site was provided
        if args.site:
            cmd = ["python3", "proton.py", args.keyword, args.site, str(args.depth)]
            description = f"Running proton.py for '{args.keyword}' on site '{args.site}' with depth {args.depth}"
        else:
            cmd = ["python3", "proton.py", args.keyword, str(args.depth)]
            description = f"Running proton.py for '{args.keyword}' with depth {args.depth}"
        
        success = run_command(cmd, description)
        if not success:
            choice = input("[!] proton.py encountered errors. Continue anyway? (y/n): ").lower()
            if choice != 'y':
                return
    else:
        print("\n[*] Skipping proton.py as requested")
    
    # Run protonizer.py if not skipped
    if not args.skip_protonizer:
        success = run_command(
            ["python3", "protonizer.py"],
            "Running protonizer.py to analyze and summarize the results"
        )
        if not success:
            choice = input("[!] protonizer.py encountered errors. Continue anyway? (y/n): ").lower()
            if choice != 'y':
                return
    else:
        print("\n[*] Skipping protonizer.py as requested")
    
    # Start the web server
    server_process = start_web_server(args.port)
    if not server_process:
        print("[!] Failed to start web server. Exiting.")
        return
    
    # Open the web browser
    open_web_browser(args.port)
    
    print("\n[+] OSINT suite is now running")
    print(f"[+] The web interface is available at http://localhost:{args.port}")
    print("[+] Press Ctrl+C to stop the server and exit")
    
    try:
        # Keep the script running until the user interrupts
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("\n[+] Stopping web server...")
        os.kill(server_process.pid, signal.SIGTERM)
        time.sleep(1)
        print("[+] Done. Goodbye!")

if __name__ == "__main__":
    main()

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/sumsearch.py
========================================

#!/usr/bin/env python3

import os
import re
import tkinter as tk
from tkinter import ttk, messagebox
import webbrowser
import threading
from functools import partial

# Change this path to the directory where you want to look for summary.txt files
BASE_DIRECTORY = "/home/jarvis/photon_results"

# Regular expression pattern to match URLs
URL_PATTERN = r'https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+'

def gather_summary_data(base_path):
    """
    Recursively walk through 'base_path' and gather text from all 'summary.txt' files.
    Returns a list of tuples (folder_path, file_contents, file_path).
    """
    all_summaries = []

    for root, dirs, files in os.walk(base_path):
        for file in files:
            if file.lower() == "summary.txt":
                filepath = os.path.join(root, file)
                try:
                    with open(filepath, "r", encoding="utf-8") as f:
                        content = f.read()
                    relative_folder = os.path.relpath(root, base_path)
                    all_summaries.append((relative_folder, content, filepath))
                except Exception as e:
                    print(f"Error reading {filepath}: {e}")

    return all_summaries

def open_file_location(filepath):
    """Open the file's directory in the system file explorer"""
    directory = os.path.dirname(filepath)
    
    # Platform-specific commands to open the file explorer
    if os.name == 'nt':  # Windows
        os.startfile(directory)
    elif os.name == 'posix':  # Linux, macOS
        try:
            # Try using xdg-open for Linux
            os.system(f'xdg-open "{directory}"')
        except:
            try:
                # Try using open for macOS
                os.system(f'open "{directory}"')
            except:
                messagebox.showinfo("Information", f"Path: {directory}")

class SummaryApp:
    def __init__(self, root):
        self.root = root
        self.root.title("Hacker Information Board")
        self.root.geometry("1100x750")
        
        # Define color scheme - hacker theme
        self.colors = {
            'bg_dark': "#0D0D0D",
            'bg_light': "#1A1A1A",
            'accent': "#00FF41",      # Matrix green
            'text_light': "#00FF41",  # Matrix green
            'text_dark': "#0D0D0D",
            'highlight': "#FFD700",   # Gold for highlights
            'error': "#FF3131"        # Red for errors
        }
        
        # Configure styles
        self.configure_styles()
        
        # Create UI elements
        self.create_ui()
        
        # Initialize variables
        self.summaries_data = {}
        self.search_results = []
        self.current_match_index = tk.IntVar(value=-1)
        
        # Load data
        self.load_summaries()
    
    def configure_styles(self):
        """Configure the ttk styles for the application"""
        style = ttk.Style()
        style.theme_use("clam")
        
        # Main frames
        style.configure(
            "Custom.TFrame", 
            background=self.colors['bg_dark']
        )
        style.configure(
            "Search.TFrame", 
            background=self.colors['bg_light']
        )
        
        # Labels
        style.configure(
            "Custom.TLabel", 
            background=self.colors['bg_dark'], 
            foreground=self.colors['text_light'], 
            font=("Courier New", 14, "bold")
        )
        style.configure(
            "Search.TLabel", 
            background=self.colors['bg_light'], 
            foreground=self.colors['text_light'], 
            font=("Courier New", 10)
        )
        
        # Buttons
        style.configure(
            "Custom.TButton",
            background=self.colors['accent'],
            foreground=self.colors['text_dark'],
            padding=6,
            font=("Courier New", 10, "bold")
        )
        style.map(
            "Custom.TButton",
            background=[('active', self.colors['highlight'])]
        )
        
        # Scrollbars
        style.configure(
            "Custom.Vertical.TScrollbar",
            troughcolor=self.colors['bg_light'],
            gripcount=0,
            background=self.colors['accent'],
            bordercolor=self.colors['bg_dark'],
            arrowcolor=self.colors['text_dark']
        )
        style.configure(
            "Custom.Horizontal.TScrollbar",
            troughcolor=self.colors['bg_light'],
            gripcount=0,
            background=self.colors['accent'],
            bordercolor=self.colors['bg_dark'],
            arrowcolor=self.colors['text_dark']
        )
        
        # Entry fields
        style.configure(
            "Custom.TEntry",
            fieldbackground=self.colors['bg_light'],
            foreground=self.colors['text_light'],
            bordercolor=self.colors['accent'],
            lightcolor=self.colors['accent'],
            darkcolor=self.colors['accent'],
        )
        
        # Checkbutton
        style.configure(
            "Search.TCheckbutton", 
            background=self.colors['bg_light'], 
            foreground=self.colors['text_light']
        )
        
        # Treeview
        style.configure(
            "Treeview", 
            background=self.colors['bg_light'], 
            foreground=self.colors['text_light'], 
            fieldbackground=self.colors['bg_light'],
            bordercolor=self.colors['bg_dark']
        )
        style.configure(
            "Treeview.Heading", 
            background=self.colors['bg_dark'], 
            foreground=self.colors['text_light'], 
            bordercolor=self.colors['bg_dark'],
            font=("Courier New", 10, "bold")
        )
        style.map(
            "Treeview", 
            background=[('selected', self.colors['accent'])],
            foreground=[('selected', self.colors['text_dark'])]
        )
    
    def create_ui(self):
        """Create the user interface"""
        # Main frame
        self.main_frame = ttk.Frame(self.root, style="Custom.TFrame")
        self.main_frame.pack(fill=tk.BOTH, expand=True)
        
        # Header
        self.create_header()
        
        # Search bar
        self.create_search_bar()
        
        # Split pane for list and content
        self.create_content_panes()
        
        # Status bar
        self.create_status_bar()
        
        # Bind keyboard shortcuts
        self.bind_shortcuts()
    
    def create_header(self):
        """Create the header section"""
        header_frame = ttk.Frame(self.main_frame, style="Custom.TFrame")
        header_frame.pack(fill=tk.X, padx=10, pady=10)
        
        title_label = ttk.Label(
            header_frame, 
            text="[ ACCESSING COMPROMISED DATA ARCHIVE ]", 
            style="Custom.TLabel"
        )
        title_label.pack(side=tk.LEFT, padx=5)
    
    def create_search_bar(self):
        """Create the search bar"""
        search_frame = ttk.Frame(self.main_frame, style="Search.TFrame")
        search_frame.pack(fill=tk.X, padx=10, pady=5)
        
        # Search label
        search_label = ttk.Label(search_frame, text="[ SEARCH ]:", style="Search.TLabel")
        search_label.pack(side=tk.LEFT, padx=5)
        
        # Search entry
        self.search_var = tk.StringVar()
        self.search_entry = ttk.Entry(search_frame, textvariable=self.search_var, width=40, style="Custom.TEntry")
        self.search_entry.pack(side=tk.LEFT, padx=5)
        
        # Case-sensitive search option
        self.case_sensitive_var = tk.BooleanVar(value=False)
        case_check = ttk.Checkbutton(
            search_frame, 
            text="Case Sensitive", 
            variable=self.case_sensitive_var,
            style="Search.TCheckbutton"
        )
        case_check.pack(side=tk.LEFT, padx=10)
        
        # Search count display
        self.search_count_var = tk.StringVar(value="")
        search_count_label = ttk.Label(
            search_frame, 
            textvariable=self.search_count_var, 
            style="Search.TLabel"
        )
        search_count_label.pack(side=tk.RIGHT, padx=10)
        
        # Navigation buttons
        self.search_prev_button = ttk.Button(
            search_frame, 
            text="◄ Prev", 
            style="Custom.TButton",
            command=self.prev_match
        )
        self.search_prev_button.pack(side=tk.RIGHT, padx=2)
        
        self.search_next_button = ttk.Button(
            search_frame, 
            text="Next ►", 
            style="Custom.TButton",
            command=self.next_match
        )
        self.search_next_button.pack(side=tk.RIGHT, padx=2)
        
        self.search_button = ttk.Button(
            search_frame, 
            text="Scan", 
            style="Custom.TButton",
            command=self.perform_search
        )
        self.search_button.pack(side=tk.RIGHT, padx=5)
    
    def create_content_panes(self):
        """Create the split pane for list and content"""
        # Create a paned window
        self.paned_window = ttk.PanedWindow(self.main_frame, orient=tk.HORIZONTAL)
        self.paned_window.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)
        
        # Left panel for summary list
        self.list_frame = ttk.Frame(self.paned_window, style="Custom.TFrame")
        
        # Create a treeview for the list of summaries
        list_columns = ('index', 'folder')
        self.summary_tree = ttk.Treeview(self.list_frame, columns=list_columns, show='headings')
        self.summary_tree.heading('index', text='#')
        self.summary_tree.column('index', width=40, anchor=tk.CENTER)
        self.summary_tree.heading('folder', text='Source Location')
        
        # Add scrollbar for the treeview
        list_scrollbar = ttk.Scrollbar(
            self.list_frame, 
            orient=tk.VERTICAL, 
            command=self.summary_tree.yview,
            style="Custom.Vertical.TScrollbar"
        )
        self.summary_tree.configure(yscrollcommand=list_scrollbar.set)
        
        # Pack the treeview and scrollbar
        self.summary_tree.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        list_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        
        # Right panel for summary content
        self.content_frame = ttk.Frame(self.paned_window, style="Custom.TFrame")
        
        # Add the panels to the paned window
        self.paned_window.add(self.list_frame, weight=1)
        self.paned_window.add(self.content_frame, weight=3)
        
        # Create a text widget with scrollbars for the content
        text_container = ttk.Frame(self.content_frame, style="Custom.TFrame")
        text_container.pack(fill=tk.BOTH, expand=True)
        
        # Vertical scrollbar
        v_scrollbar = ttk.Scrollbar(
            text_container, 
            orient=tk.VERTICAL, 
            style="Custom.Vertical.TScrollbar"
        )
        v_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        
        # Horizontal scrollbar
        h_scrollbar = ttk.Scrollbar(
            text_container, 
            orient=tk.HORIZONTAL, 
            style="Custom.Horizontal.TScrollbar"
        )
        h_scrollbar.pack(side=tk.BOTTOM, fill=tk.X)
        
        # Text widget with custom cursor
        self.text_widget = tk.Text(
            text_container,
            wrap="none",  # Allow horizontal scrolling
            yscrollcommand=v_scrollbar.set,
            xscrollcommand=h_scrollbar.set,
            bg=self.colors['bg_light'],
            fg=self.colors['text_light'],
            font=("Courier New", 12),
            padx=5,
            pady=5,
            insertbackground=self.colors['accent'],  # Cursor color
            selectbackground=self.colors['accent'],
            selectforeground=self.colors['text_dark'],
            cursor="arrow"
        )
        self.text_widget.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        
        # Configure scrollbars
        v_scrollbar.config(command=self.text_widget.yview)
        h_scrollbar.config(command=self.text_widget.xview)
        
        # Create text tags for formatting
        self.text_widget.tag_configure("title", font=("Courier New", 14, "bold"), foreground=self.colors['accent'])
        self.text_widget.tag_configure("location", font=("Courier New", 12, "italic"), foreground=self.colors['text_light'])
        self.text_widget.tag_configure("hyperlink", foreground=self.colors['accent'], underline=1)
        self.text_widget.tag_configure("search_match", background=self.colors['highlight'], foreground=self.colors['text_dark'])
        self.text_widget.tag_configure("current_match", background=self.colors['accent'], foreground=self.colors['text_dark'])
    
    def create_status_bar(self):
        """Create the status bar"""
        status_frame = ttk.Frame(self.main_frame, style="Search.TFrame")
        status_frame.pack(fill=tk.X, side=tk.BOTTOM)
        
        self.status_var = tk.StringVar(value="[ SYSTEM READY ]")
        status_label = ttk.Label(
            status_frame, 
            textvariable=self.status_var, 
            style="Search.TLabel"
        )
        status_label.pack(side=tk.LEFT, padx=10, pady=3)

        # Add font size controls
        font_frame = ttk.Frame(status_frame, style="Search.TFrame")
        font_frame.pack(side=tk.RIGHT, padx=10)
        
        font_label = ttk.Label(font_frame, text="Font Size:", style="Search.TLabel")
        font_label.pack(side=tk.LEFT, padx=(0, 5))
        
        font_decrease = ttk.Button(font_frame, text="-", width=2, style="Custom.TButton", command=self.decrease_font)
        font_decrease.pack(side=tk.LEFT, padx=2)
        
        font_increase = ttk.Button(font_frame, text="+", width=2, style="Custom.TButton", command=self.increase_font)
        font_increase.pack(side=tk.LEFT, padx=2)
        
        # Add word wrap toggle
        self.wrap_var = tk.BooleanVar(value=False)
        wrap_check = ttk.Checkbutton(
            status_frame, 
            text="Word Wrap", 
            variable=self.wrap_var,
            style="Search.TCheckbutton",
            command=self.toggle_word_wrap
        )
        wrap_check.pack(side=tk.RIGHT, padx=10)
    
    def bind_shortcuts(self):
        """Bind keyboard shortcuts"""
        self.search_entry.bind("<Key>", self.search_key_handler)
        self.summary_tree.bind("<<TreeviewSelect>>", self.display_summary)
        self.summary_tree.bind("<Button-3>", self.context_menu_for_tree)
        self.root.bind("<Control-f>", lambda e: self.search_entry.focus())
        self.root.bind("<F3>", lambda e: self.next_match())
        self.root.bind("<Shift-F3>", lambda e: self.prev_match())
    
    def load_summaries(self):
        """Load summary data in a background thread"""
        self.status_var.set("[ SCANNING FILE SYSTEM ]")
        
        def background_load():
            summaries = gather_summary_data(BASE_DIRECTORY)
            
            # Update the UI in the main thread
            self.root.after(0, lambda: self.populate_summary_list(summaries))
        
        # Start background thread
        thread = threading.Thread(target=background_load)
        thread.daemon = True
        thread.start()
    
    def populate_summary_list(self, summaries):
        """Populate the summary list with data"""
        if not summaries:
            self.text_widget.config(state=tk.NORMAL)
            self.text_widget.insert(tk.END, "[ NO DATA FOUND ]\n")
            self.text_widget.config(state=tk.DISABLED)
            self.status_var.set("[ NO DATA FOUND IN TARGET DIRECTORY ]")
            return
        
        # Clear existing items
        for item in self.summary_tree.get_children():
            self.summary_tree.delete(item)
        
        # Add new items
        for index, (subfolder, content, filepath) in enumerate(summaries, start=1):
            item_id = self.summary_tree.insert(
                '', 'end', 
                values=(index, subfolder)
            )
            # Store the summary data for reference
            self.summaries_data[item_id] = (subfolder, content, filepath)
        
        # Select first item
        if self.summary_tree.get_children():
            first_item = self.summary_tree.get_children()[0]
            self.summary_tree.selection_set(first_item)
            self.summary_tree.focus(first_item)
            self.display_summary()
        
        self.status_var.set(f"[ {len(summaries)} DATA ARCHIVES IDENTIFIED ]")
    
    def find_urls_in_text(self, text):
        """Find all URLs in the given text and return a list of (start, end, url) tuples"""
        urls = []
        for match in re.finditer(URL_PATTERN, text):
            start, end = match.span()
            url = match.group(0)
            urls.append((start, end, url))
        return urls
    
    def open_url(self, url):
        """Open a URL in the default web browser"""
        try:
            webbrowser.open(url)
            self.status_var.set(f"[ ACCESSING EXTERNAL RESOURCE: {url} ]")
        except Exception as e:
            self.status_var.set(f"[ CONNECTION ERROR: {str(e)} ]")
    
    def display_summary(self, event=None):
        """Display the selected summary in the text widget"""
        selected_items = self.summary_tree.selection()
        if not selected_items:
            return
        
        item_id = selected_items[0]
        item_data = self.summaries_data.get(item_id)
        if not item_data:
            return
        
        folder_path, content, full_path = item_data
        
        # Clear the text widget
        self.text_widget.config(state=tk.NORMAL)
        self.text_widget.delete(1.0, tk.END)
        
        # Insert title
        self.text_widget.insert(tk.END, f"[ DATA FROM: {os.path.basename(folder_path)} ]\n", "title")
        
        # Insert location with a button to open it
        location_text = f"Location: {folder_path}"
        self.text_widget.insert(tk.END, location_text + "\n\n", "location")
        
        # Make the location clickable
        location_start = "1.0 + 10c"
        location_end = f"1.0 + {10 + len(folder_path)}c"
        location_tag = "location_link"
        self.text_widget.tag_configure(location_tag, underline=1)
        self.text_widget.tag_bind(location_tag, "<Button-1>", lambda e: open_file_location(full_path))
        self.text_widget.tag_add(location_tag, location_start, location_end)
        
        # Insert content and process URLs
        start_pos = self.text_widget.index(tk.END)
        self.text_widget.insert(tk.END, content)
        
        # Find and mark URLs
        all_content = self.text_widget.get(start_pos, tk.END)
        url_positions = self.find_urls_in_text(all_content)
        
        for start, end, url in url_positions:
            # Calculate positions in the text widget
            start_index = f"{start_pos} + {start} chars"
            end_index = f"{start_pos} + {end} chars"
            
            # Apply hyperlink tag
            self.text_widget.tag_add("hyperlink", start_index, end_index)
            
            # Bind click event
            tag_name = f"url_{start}_{end}"
            self.text_widget.tag_configure(tag_name)
            self.text_widget.tag_bind(tag_name, "<Button-1>", lambda e, u=url: self.open_url(u))
            self.text_widget.tag_add(tag_name, start_index, end_index)
        
        self.text_widget.config(state=tk.DISABLED)
        
        # Update status
        self.status_var.set(f"[ DISPLAYING: {folder_path} ]")
        
        # Clear search highlights when changing summaries
        self.search_results.clear()
        self.current_match_index.set(-1)
        self.search_count_var.set("")
    
    def perform_search(self):
        """Search for text in the current summary"""
        search_text = self.search_var.get()
        if not search_text:
            self.status_var.set("[ ENTER SEARCH TERM ]")
            return
        
        # Clear previous search highlighting
        self.text_widget.tag_remove("search_match", "1.0", tk.END)
        self.text_widget.tag_remove("current_match", "1.0", tk.END)
        
        # Find all matches
        self.search_results.clear()
        self.current_match_index.set(-1)
        
        start_idx = "1.0"
        count_var = tk.StringVar()
        
        while True:
            # Find the next match
            match_pos = self.text_widget.search(
                search_text, start_idx, tk.END, 
                count=count_var,
                nocase=not self.case_sensitive_var.get()
            )
            
            if not match_pos:
                break
                
            # Calculate end index
            match_length = int(count_var.get())
            match_end = f"{match_pos}+{match_length}c"
            
            # Add to results
            self.search_results.append((match_pos, match_end))
            
            # Highlight match
            self.text_widget.tag_add("search_match", match_pos, match_end)
            
            # Move to next starting position
            start_idx = match_end
        
        # Update search count
        num_results = len(self.search_results)
        if num_results > 0:
            self.search_count_var.set(f"[ {num_results} MATCHES ]")
            self.go_to_match(0)  # Go to first match
        else:
            self.search_count_var.set("[ NO MATCHES ]")
            self.status_var.set(f"[ NO RESULTS FOR: '{search_text}' ]")
    
    def go_to_match(self, index):
        """Go to a specific search match by index"""
        if not self.search_results:
            return
        
        # Make sure index is in bounds
        num_results = len(self.search_results)
        if index < 0:
            index = num_results - 1
        elif index >= num_results:
            index = 0
        
        # Remove current match highlight
        self.text_widget.tag_remove("current_match", "1.0", tk.END)
        
        # Highlight current match
        match_pos, match_end = self.search_results[index]
        self.text_widget.tag_add("current_match", match_pos, match_end)
        
        # Ensure visibility
        self.text_widget.see(match_pos)
        
        # Update current match index
        self.current_match_index.set(index)
        
        # Update status
        self.status_var.set(f"[ MATCH {index + 1} OF {num_results} ]")
        self.search_count_var.set(f"[ {index + 1} / {num_results} ]")
    
    def next_match(self):
        """Go to the next search match"""
        current = self.current_match_index.get()
        if current >= 0:
            self.go_to_match(current + 1)
    
    def prev_match(self):
        """Go to the previous search match"""
        current = self.current_match_index.get()
        if current >= 0:
            self.go_to_match(current - 1)
    
    def search_key_handler(self, event):
        """Handle key events in the search entry"""
        if event.keysym == "Return":
            self.perform_search()
            return "break"
        elif event.keysym == "Escape":
            # Clear search
            self.search_var.set("")
            self.text_widget.tag_remove("search_match", "1.0", tk.END)
            self.text_widget.tag_remove("current_match", "1.0", tk.END)
            self.search_results.clear()
            self.current_match_index.set(-1)
            self.search_count_var.set("")
            return "break"
    
    def context_menu_for_tree(self, event):
        """Show context menu for treeview items"""
        # Get the item under the mouse
        item = self.summary_tree.identify('item', event.x, event.y)
        if item:
            # Select the item
            self.summary_tree.selection_set(item)
            
            # Create a context menu
            context_menu = tk.Menu(self.root, tearoff=0, bg=self.colors['bg_light'], fg=self.colors['text_light'])
            
            # Get the item data
            item_data = self.summaries_data.get(item)
            if item_data:
                subfolder, content, filepath = item_data
                
                # Add menu items
                context_menu.add_command(
                    label="Open File Location", 
                    command=lambda: open_file_location(filepath)
                )
                
                # Show the menu
                context_menu.post(event.x_root, event.y_root)
    
    def increase_font(self):
        """Increase the font size in the text widget"""
        current_font = self.text_widget['font']
        if isinstance(current_font, str):
            # Parse font string
            parts = current_font.split()
            family = parts[0]
            size = int(parts[1])
        else:
            # Font is a tuple
            family, size = current_font
        
        new_size = min(size + 2, 36)  # Maximum size 36
        self.text_widget.configure(font=(family, new_size))
    
    def decrease_font(self):
        """Decrease the font size in the text widget"""
        current_font = self.text_widget['font']
        if isinstance(current_font, str):
            # Parse font string
            parts = current_font.split()
            family = parts[0]
            size = int(parts[1])
        else:
            # Font is a tuple
            family, size = current_font
        
        new_size = max(size - 2, 8)  # Minimum size 8
        self.text_widget.configure(font=(family, new_size))
    
    def toggle_word_wrap(self):
        """Toggle word wrap in the text widget"""
        if self.wrap_var.get():
            self.text_widget.configure(wrap="word")
        else:
            self.text_widget.configure(wrap="none")


def create_ui():
    """Create the main application UI"""
    root = tk.Tk()
    app = SummaryApp(root)
    root.mainloop()


if __name__ == "__main__":
    create_ui()

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/miniproton.py
========================================

#!/usr/bin/env python3
import argparse
import subprocess
import sys
import logging
import shutil
import traceback
import json
import os

VENV_PATH = os.path.expanduser("~/newvenv/bin/activate")  # Adjust virtualenv path as needed

def check_command(command):
    """Verify if a command is available in the system PATH."""
    if shutil.which(command) is None:
        logging.error(f"'{command}' is not installed. Please install it to proceed.")
        sys.exit(1)
    logging.debug(f"'{command}' is available.")

def run_command(command, label, debug=False, use_venv=False):
    """Execute a command, optionally using a virtual environment."""
    try:
        check_command(command[0])
        logging.debug(f"Executing command: {' '.join(command)}")

        # If running Photon, ensure it is within the virtual environment
        if use_venv:
            command = f"source {VENV_PATH} && " + " ".join(command)
            result = subprocess.run(command, shell=True, capture_output=True, text=True, executable="/bin/bash")
        else:
            result = subprocess.run(command, capture_output=True, text=True)

        if result.returncode != 0:
            logging.error(f"Error running {label}: {result.stderr.strip()}")
            if debug:
                logging.debug("Full stderr: " + result.stderr)
            return None

        logging.debug(f"{label} output: {result.stdout.strip()}")
        return result.stdout.strip()

    except Exception as exc:
        logging.error(f"Exception running {label}: {exc}")
        if debug:
            traceback.print_exc()
        return None

def run_ddgr(query, debug=False):
    """Run ddgr with the given search query and output formatted results."""
    print(f"\n[+] DuckDuckGo (ddgr) search results for: {query}\n")
    output = run_command(['ddgr', '--json', query], "ddgr", debug)
    
    if output:
        try:
            results = json.loads(output)
            for result in results[:10]:  # Limit output to 10 results
                print(f"- {result.get('title', 'No Title')}\n  {result.get('url', 'No URL')}\n")
        except json.JSONDecodeError:
            logging.error("Failed to parse ddgr JSON output.")

def run_photon(target, debug=False):
    """Run Photon OSINT tool on the given target URL."""
    print(f"\n[+] Photon OSINT results for: {target}\n")
    output = run_command(['python3', 'photon.py', '-u', target, '-o', 'json'], "Photon OSINT", debug, use_venv=True)
    
    if output:
        print(output)

def main():
    parser = argparse.ArgumentParser(description="OSINT tool using Photon OSINT and ddgr.")
    parser.add_argument('--query', type=str, help="Search query for DuckDuckGo (ddgr)")
    parser.add_argument('--target', type=str, help="Target URL for Photon OSINT scraping")
    parser.add_argument('--debug', action='store_true', help="Enable debug mode for detailed output")
    args = parser.parse_args()

    # Configure logging based on the debug flag.
    logging_level = logging.DEBUG if args.debug else logging.INFO
    logging.basicConfig(level=logging_level, format='[%(levelname)s] %(message)s')

    if args.debug:
        logging.debug("Debug mode enabled.")

    if not args.query and not args.target:
        parser.error("You must provide at least --query or --target (or both).")

    if args.query:
        run_ddgr(args.query, args.debug)

    if args.target:
        run_photon(args.target, args.debug)

if __name__ == '__main__':
    main()

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/ app.py
========================================

#!/usr/bin/env python3

import os
import re
import subprocess
import webbrowser
from flask import Flask, jsonify, request, send_from_directory
from flask_cors import CORS

# Change this path to the directory where you want to look for summary.txt files
BASE_DIRECTORY = "/home/jarvis/photon_results"

app = Flask(__name__, static_folder='static')
CORS(app)  # Enable CORS for all routes

# Regular expression pattern to match URLs
URL_PATTERN = r'https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+'

def gather_summary_data(base_path):
    """
    Recursively walk through 'base_path' and gather information about all 'summary.txt' files.
    Returns a list of dictionaries with id, folder_path and file_path.
    """
    all_summaries = []
    id_counter = 1

    for root, dirs, files in os.walk(base_path):
        for file in files:
            if file.lower() == "summary.txt":
                filepath = os.path.join(root, file)
                try:
                    relative_folder = os.path.relpath(root, base_path)
                    all_summaries.append({
                        "id": id_counter,
                        "folder": relative_folder,
                        "filepath": filepath
                    })
                    id_counter += 1
                except Exception as e:
                    print(f"Error processing {filepath}: {e}")

    return all_summaries

def get_summary_content(filepath):
    """Read and return the content of a summary file"""
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            content = f.read()
        return content
    except Exception as e:
        print(f"Error reading {filepath}: {e}")
        return f"Error reading file: {str(e)}"

def open_file_location(filepath):
    """Open the file's directory in the system file explorer"""
    directory = os.path.dirname(filepath)
    
    # Platform-specific commands to open the file explorer
    try:
        if os.name == 'nt':  # Windows
            os.startfile(directory)
        elif os.name == 'posix':  # Linux, macOS
            if os.system(f'xdg-open "{directory}"') != 0:  # Try Linux first
                os.system(f'open "{directory}"')  # Try macOS
        return True
    except Exception as e:
        print(f"Error opening location: {e}")
        return False

# API Routes
@app.route('/')
def serve_index():
    return send_from_directory('static', 'index.html')

@app.route('/api/summaries')
def get_summaries():
    """API endpoint to get all summaries"""
    try:
        summaries = gather_summary_data(BASE_DIRECTORY)
        return jsonify(summaries)
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/summary/<int:summary_id>')
def get_summary(summary_id):
    """API endpoint to get a specific summary's content"""
    try:
        summaries = gather_summary_data(BASE_DIRECTORY)
        
        # Find the summary with the matching ID
        summary = next((s for s in summaries if s["id"] == summary_id), None)
        
        if not summary:
            return jsonify({"error": "Summary not found"}), 404
        
        content = get_summary_content(summary["filepath"])
        
        return jsonify({
            "summary": summary,
            "content": content
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/open-location', methods=['POST'])
def api_open_location():
    """API endpoint to open a file location"""
    try:
        data = request.json
        if not data or 'path' not in data:
            return jsonify({"error": "No path provided"}), 400
        
        filepath = data['path']
        success = open_file_location(filepath)
        
        if success:
            return jsonify({"status": "success"})
        else:
            return jsonify({"error": "Failed to open location"}), 500
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/open-url', methods=['POST'])
def api_open_url():
    """API endpoint to open a URL in the browser"""
    try:
        data = request.json
        if not data or 'url' not in data:
            return jsonify({"error": "No URL provided"}), 400
        
        url = data['url']
        # Validate URL
        if not re.match(URL_PATTERN, url):
            return jsonify({"error": "Invalid URL"}), 400
            
        webbrowser.open(url)
        return jsonify({"status": "success"})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == "__main__":
    # Create the static directory if it doesn't exist
    os.makedirs('static', exist_ok=True)
    
    # Copy the HTML file to the static directory (in a real app, you'd have a build process)
    # For now, assuming the HTML file is in the same directory as this script
    try:
        with open('frontend.html', 'r') as src, open('static/index.html', 'w') as dst:
            dst.write(src.read())
    except FileNotFoundError:
        print("Warning: frontend.html not found. Please place the HTML file in the static directory.")
    
    # Start the Flask server
    print(f"Starting server on http://localhost:5000")
    app.run(debug=True, host='0.0.0.0', port=5000)

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/deepsearch-proton.py
========================================

#!/usr/bin/env python3
import argparse
import subprocess
import sys
import logging
import shutil
import traceback
import json
import os
import re
import time
import textwrap
from collections import OrderedDict

__version__ = "2.0"

# Path to your virtual environment activation script and Photon script
VENV_PATH = os.path.expanduser("~/newvenv/bin/activate")
PHOTON_SCRIPT = os.path.join(os.path.dirname(os.path.realpath(__file__)), "photon.py")

def check_command(command):
    """Verify if a command is available in the system PATH."""
    if shutil.which(command) is None:
        logging.error(f"'{command}' is not installed. Please install it to proceed.")
        sys.exit(1)
    logging.debug(f"'{command}' is available.")

def pretty_print_json(output):
    """Attempt to parse and pretty-print JSON output; if it fails, return raw output."""
    try:
        data = json.loads(output)
        return json.dumps(data, indent=2)
    except json.JSONDecodeError:
        return output

def run_command(command, label, debug=False, use_venv=False, ignore_errors=False):
    """
    Execute a command, optionally within a virtual environment.
    Returns the command output as a string.
    """
    try:
        if use_venv:
            # For commands like Photon that must run inside your virtualenv,
            # we assume python3 is available.
            check_command("python3")
        else:
            check_command(command[0])
        logging.debug(f"Executing command: {' '.join(command)}")
        
        if use_venv:
            # Combine the activation command with the target command
            full_command = f"source {VENV_PATH} && " + " ".join(command)
            result = subprocess.run(full_command, shell=True, capture_output=True, text=True, executable="/bin/bash")
        else:
            result = subprocess.run(command, capture_output=True, text=True)

        if result.returncode != 0:
            logging.error(f"Error running {label}: {result.stderr.strip()}")
            if debug:
                logging.debug("Full stderr: " + result.stderr)
            if ignore_errors:
                # Return empty result but don't fail completely
                return ""
            return None
        logging.debug(f"{label} output: {result.stdout.strip()}")
        return result.stdout.strip()
    except Exception as exc:
        logging.error(f"Exception running {label}: {exc}")
        if debug:
            traceback.print_exc()
        return None

def calculate_resource_values(depth):
    """
    Calculate appropriate resource values based on the depth parameter (1-100).
    Returns a dictionary with calibrated values for different tools.
    
    Depth scale:
    1-20: Light/Quick search
    21-50: Medium search
    51-80: Thorough search
    81-100: Deep/Extensive search
    """
    # Ensure depth is within valid range (1-100)
    depth = max(1, min(100, depth))
    
    # Always use maximum page size for ddgr (25 results)
    ddgr_page_size = 25
    ddgr_results = 25
    
    # Calculate Photon crawl parameters
    # Level: 1 to 5 (Photon's internal crawl depth)
    photon_level = 1 + int((depth - 1) / 20)  # Ranges from 1-5
    
    # Threads: 2 to 50 (threading for parallel processing)
    photon_threads = 2 + int((depth - 1) * 0.48)  # Linear scaling
    
    # Timeout: 5s to 30s (longer for deeper crawls)
    photon_timeout = 5 + int((depth - 1) * 0.25)  # Linear scaling
    
    # URLs to crawl per page: proportional to depth
    if depth <= 20:
        urls_percent = 0.6  # Crawl 60% of results for light search
    elif depth <= 50:
        urls_percent = 0.8  # Crawl 80% of results for medium search
    elif depth <= 80:
        urls_percent = 0.9  # Crawl 90% of results for thorough search
    else:
        urls_percent = 1.0  # Crawl 100% of results for deep search
    
    urls_to_crawl = max(1, min(ddgr_page_size, round(ddgr_page_size * urls_percent)))
    
    return {
        "ddgr_results": ddgr_results,
        "ddgr_page_size": ddgr_page_size,
        "photon_threads": photon_threads,
        "photon_level": photon_level,
        "photon_timeout": photon_timeout,
        "urls_to_crawl": urls_to_crawl
    }

def deduplicate_urls(new_urls, existing_urls):
    """
    Add new URLs to the existing set while avoiding duplicates.
    
    Parameters:
    - new_urls: List of new URLs to add
    - existing_urls: Set of already processed URLs
    
    Returns:
    - List of URLs that aren't already in existing_urls
    """
    unique_new_urls = []
    for url in new_urls:
        if url not in existing_urls:
            existing_urls.add(url)
            unique_new_urls.append(url)
    
    return unique_new_urls

def build_ddgr_command(query, ddgr_args, page_size=25):
    """Build the ddgr command with all user-supplied arguments."""
    command = ['ddgr', '--json']
    
    # Ensure we're using maximum page size for ddgr
    command.extend(['--num', str(page_size)])
    
    # Add all ddgr-specific arguments
    if ddgr_args.get('region'):
        command.extend(['--reg', ddgr_args.get('region')])
    if ddgr_args.get('colorize'):
        command.extend(['--colorize', ddgr_args.get('colorize')])
    if ddgr_args.get('nocolor'):
        command.append('--nocolor')
    if ddgr_args.get('colors'):
        command.extend(['--colors', ddgr_args.get('colors')])
    if ddgr_args.get('time'):
        command.extend(['--time', ddgr_args.get('time')])
    if ddgr_args.get('site'):
        for site in ddgr_args.get('site'):
            command.extend(['--site', site])
    if ddgr_args.get('expand'):
        command.append('--expand')
    if ddgr_args.get('proxy'):
        command.extend(['--proxy', ddgr_args.get('proxy')])
    if ddgr_args.get('unsafe'):
        command.append('--unsafe')
    if ddgr_args.get('noua'):
        command.append('--noua')
    
    # Add the query
    command.append(query)
    
    return command

def run_ddgr_with_pagination(query, ddgr_args, depth=30, debug=False, page=1):
    """
    Run DuckDuckGo search using ddgr with proper pagination support.
    This function uses ddgr's interactive mode and simulates pagination commands.
    
    Parameters:
    - query: The search query string
    - ddgr_args: Dictionary of ddgr-specific arguments
    - depth: Depth parameter (1-100)
    - debug: Whether to enable debug output
    - page: Page number (1-based for user display)
    
    Returns:
    - List of URLs from search results
    - Boolean indicating if there might be more results
    """
    resources = calculate_resource_values(depth)
    page_size = resources["ddgr_page_size"]  # Now this will always be 25
    
    # For first page, search normally
    if page == 1:
        print(f"\n[+] DuckDuckGo (ddgr) search results for: {query}")
        print(f"[+] Search depth: {depth}/100 (Page {page}, retrieving {page_size} results)")
        command = build_ddgr_command(query, ddgr_args, page_size)
    else:
        print(f"\n[+] DuckDuckGo (ddgr) search results - Page {page} for: {query}")
        print(f"[+] Search depth: {depth}/100 (retrieving {page_size} results per page)")
        
        # For subsequent pages, we need to simulate pagination
        # Always use maximum results (25) to reduce pagination steps
        initial_results = 25  # This is the maximum for ddgr
        command = ['ddgr', '--json', '--num', str(initial_results), query]
        # Add other args
        if ddgr_args.get('region'):
            command.extend(['--reg', ddgr_args.get('region')])
        if ddgr_args.get('time'):
            command.extend(['--time', ddgr_args.get('time')])
        if ddgr_args.get('site'):
            for site in ddgr_args.get('site'):
                command.extend(['--site', site])
        if ddgr_args.get('unsafe'):
            command.append('--unsafe')
    
    output = run_command(command, "ddgr", debug)
    if not output:
        print("[-] No results from ddgr search")
        return [], False
    
    urls = []
    try:
        results = json.loads(output)
        
        # If we're on a later page, we need to extract the correct subset of results
        result_offset = 0
        if page > 1:
            # Calculate the offset for the current page
            # For page 2, we want results 25-49 (assuming page_size=25)
            # For page 3, we want results 50-74, etc.
            result_offset = (page - 1) * page_size
            
            # If offset is beyond available results, no more results
            if result_offset >= len(results):
                print("[-] No more results available.")
                return [], False
            
            # Get the slice of results for this page
            end_offset = min(result_offset + page_size, len(results))
            page_results = results[result_offset:end_offset]
        else:
            # First page, just take the first page_size results
            page_results = results[:page_size]
        
        # Process and display the results
        for i, result in enumerate(page_results, 1):
            title = result.get("title", "No Title")
            url = result.get("url", "No URL")
            abstract = result.get("abstract", "")
            
            # Add URLs to the list for Photon crawling
            urls.append(url)
            
            # Display result with correct global index
            global_index = result_offset + i
            print(f"{global_index}. {title}")
            print(f"   {url}")
            if abstract:
                wrapped_abstract = textwrap.fill(abstract, width=80, initial_indent="   ", subsequent_indent="   ")
                print(f"{wrapped_abstract}\n")
            else:
                print()  # Empty line for spacing
        
        # Determine if there might be more results
        # We consider there are more if:
        # 1. We got a full page of results, or
        # 2. We know there are more results in our fetched batch
        has_more = (len(page_results) == page_size) or (result_offset + len(page_results) < len(results))
        
        return urls, has_more
    
    except json.JSONDecodeError:
        logging.error("Failed to parse ddgr JSON output.")
        print(output)
        return [], False

def build_photon_command(target, photon_args, output_dir):
    """Build the Photon command with all user-supplied arguments."""
    command = [
        'python3', PHOTON_SCRIPT,
        '-u', target,
        '-o', output_dir
    ]
    
    # Add all photon-specific arguments
    if photon_args.get('level') is not None:
        command.extend(['-l', str(photon_args.get('level'))])
    if photon_args.get('threads') is not None:
        command.extend(['-t', str(photon_args.get('threads'))])
    if photon_args.get('delay') is not None:
        command.extend(['-d', str(photon_args.get('delay'))])
    if photon_args.get('timeout') is not None:
        command.extend(['--timeout', str(photon_args.get('timeout'))])
    if photon_args.get('cookie'):
        command.extend(['-c', photon_args.get('cookie')])
    if photon_args.get('regex'):
        command.extend(['-r', photon_args.get('regex')])
    if photon_args.get('export'):
        command.extend(['-e', photon_args.get('export')])
    if photon_args.get('seeds'):
        command.extend(['-s'] + photon_args.get('seeds'))
    if photon_args.get('user_agent'):
        command.extend(['--user-agent', photon_args.get('user_agent')])
    if photon_args.get('exclude'):
        command.extend(['--exclude', photon_args.get('exclude')])
    if photon_args.get('proxy'):
        command.extend(['-p', photon_args.get('proxy')])
    
    # Add boolean flags
    if photon_args.get('verbose'):
        command.append('-v')
    if photon_args.get('headers'):
        command.append('--headers')
    if photon_args.get('dns'):
        command.append('--dns')
    if photon_args.get('keys'):
        command.append('--keys')
    if photon_args.get('only_urls'):
        command.append('--only-urls')
    if photon_args.get('wayback'):
        command.append('--wayback')
    
    return command

def run_photon_on_single_target(target, photon_args, depth=30, debug=False, index=None, total=None):
    """Run Photon OSINT on a single target URL with user-specified arguments."""
    resources = calculate_resource_values(depth)
    
    # Use calculated resources if not provided in photon_args
    level = photon_args.get('level') or resources["photon_level"]
    threads = photon_args.get('threads') or resources["photon_threads"]
    timeout = photon_args.get('timeout') or resources["photon_timeout"]
    
    # Create a progress indicator if we're processing multiple URLs
    progress_str = ""
    if index is not None and total is not None:
        progress_str = f"[{index}/{total}] "
    
    print(f"\n[+] {progress_str}Photon crawling target: {target}")
    print(f"[+] Crawl depth: {depth}/100 (level: {level}, threads: {threads}, timeout: {timeout}s)")
    
    if not os.path.isfile(PHOTON_SCRIPT):
        logging.error(f"Photon script not found at {PHOTON_SCRIPT}. Please ensure photon.py is available.")
        return None
    
    # Create organized output folder structure
    main_output_dir = "photon_results"
    if not os.path.exists(main_output_dir):
        os.makedirs(main_output_dir)
    
    # Get domain name for subfolder
    domain = target.replace("https://", "").replace("http://", "").split("/")[0]
    domain_safe = re.sub(r'[^\w\-_]', '_', domain)  # Make domain name safe for filesystem
    
    # Create a unique subfolder for this target
    timestamp = int(time.time())
    target_dir = f"{domain_safe}_{timestamp}"
    output_dir = os.path.join(main_output_dir, target_dir)
    
    # Build the Photon command
    command = build_photon_command(target, photon_args, output_dir)
    
    # Run the command with the proper settings and handle failures gracefully
    output = run_command(command, f"Photon OSINT on {target}", debug, use_venv=True, ignore_errors=True)
    
    # Create the output directory even if the command failed
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        
    if output is not None:
        print(f"[+] Completed scanning {target}")
        print(f"[+] Results saved to {output_dir}/")
        return output_dir
    
    # If the output is None, it means the command completely failed
    print(f"[!] Issues encountered while scanning {target}, but continuing...")
    print(f"[+] Partial results may be available in {output_dir}/")
    return output_dir

def run_photon_on_multiple_targets(targets, photon_args, depth=30, debug=False):
    """Run Photon OSINT on multiple target URLs with user-specified arguments."""
    if not targets:
        print("[-] No targets to crawl with Photon.")
        return
    
    resources = calculate_resource_values(depth)
    max_targets = photon_args.get('max_targets') or resources["urls_to_crawl"]
    
    # Deduplicate targets while preserving order
    unique_targets = list(OrderedDict.fromkeys(targets))
    
    if len(unique_targets) > max_targets:
        print(f"[*] Limiting Photon crawl to top {max_targets} targets based on depth setting {depth}/100")
        targets_to_crawl = unique_targets[:max_targets]
    else:
        targets_to_crawl = unique_targets
    
    print(f"\n[+] Starting Photon crawler on {len(targets_to_crawl)} targets")
    
    results = []
    for i, target in enumerate(targets_to_crawl, 1):
        result_dir = run_photon_on_single_target(
            target, photon_args, depth, debug, i, len(targets_to_crawl)
        )
        if result_dir:
            results.append((target, result_dir))
    
    if results:
        print("\n[+] Photon crawling complete. Summary:")
        print(f"[+] All results saved to the 'photon_results/' directory")
        for target, output_dir in results:
            print(f"  - {target} -> {os.path.basename(output_dir)}/")
    else:
        print("\n[-] No successful Photon crawls.")

def ask_for_more():
    """Ask the user if they want more results."""
    while True:
        answer = input("\nMore results? (y/n): ").lower().strip()
        if answer in ['y', 'yes']:
            return True
        elif answer in ['n', 'no']:
            return False
        else:
            print("Please answer 'y' or 'n'.")

def is_url(text):
    """Check if the given text is a URL."""
    url_pattern = re.compile(r'^https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+')
    return bool(url_pattern.match(text))

def main():
    # Create argument parser with extensive options for both ddgr and photon
    parser = argparse.ArgumentParser(
        description="Enhanced Deep Search OSINT tool using Photon OSINT and ddgr with maximum results mode.",
        epilog=("Examples:\n"
                "  python3 deepsearch-proton.py \"search keywords\" 25                          # Search and crawl results with medium depth\n"
                "  python3 deepsearch-proton.py \"search keywords\" \"https://example.com\" 45    # Search, crawl specific site and search results\n"
                "  python3 deepsearch-proton.py --query 'osint tools' --depth 75 --no-crawl   # Search only, no crawling\n"
                "  python3 deepsearch-proton.py --target 'https://example.com' --depth 100     # Crawl only a specific site deeply\n"
                "  python3 deepsearch-proton.py \"keywords\" --max-pages 10 --auto-paginate     # Auto-paginate through 10 pages of results\n"
                "\nDepth Values (1-100):\n"
                "  1-20:   Quick/light crawl (fewer results, shallow depth, faster)\n"
                "  21-50:  Medium crawl (moderate results and depth)\n"
                "  51-80:  Thorough crawl (more results, deeper level)\n"
                "  81-100: Deep crawl (maximum results and depth level, longer runtime)"),
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    # Basic options
    basic_group = parser.add_argument_group('Basic Options')
    basic_group.add_argument('--query', type=str, 
                    help="Search query for DuckDuckGo (ddgr)")
    basic_group.add_argument('--target', type=str, 
                    help="Target URL for Photon OSINT scraping")
    basic_group.add_argument('--depth', type=int, default=30, 
                    help="Search depth (1-100): 1=quickest, 100=deepest. Controls number of results, crawl depth, and threads")
    basic_group.add_argument('--debug', action='store_true', 
                    help="Enable debug mode for detailed output and error messages")
    basic_group.add_argument('--no-crawl', action='store_true',
                    help="Disable automatic crawling of search results (search only)")
    basic_group.add_argument('--no-pagination', action='store_true',
                    help="Disable pagination ('more' prompt at the end)")
    basic_group.add_argument('--max-pages', type=int, metavar='N', default=0,
                    help="Maximum number of search result pages to fetch (0 for unlimited)")
    basic_group.add_argument('--auto-paginate', action='store_true',
                    help="Automatically fetch all pages without prompting")
    basic_group.add_argument('--version', action='version', 
                    version=f"%(prog)s {__version__}")
    
    # ddgr-specific options
    ddgr_group = parser.add_argument_group('DuckDuckGo (ddgr) Options')
    ddgr_group.add_argument('--ddgr-region', type=str, metavar='REG', default='us-en',
                    help="region-specific search e.g. 'us-en' for US (default)")
    ddgr_group.add_argument('--ddgr-colorize', type=str, choices=['auto', 'always', 'never'], default='auto',
                    help="whether to colorize output")
    ddgr_group.add_argument('--ddgr-nocolor', action='store_true',
                    help="equivalent to --ddgr-colorize=never")
    ddgr_group.add_argument('--ddgr-colors', type=str, metavar='COLORS',
                    help="set output colors")
    ddgr_group.add_argument('--ddgr-time', type=str, metavar='SPAN', choices=('d', 'w', 'm', 'y'),
                    help="time limit search [d (1 day), w (1 wk), m (1 month), y (1 year)]")
    ddgr_group.add_argument('--ddgr-site', type=str, metavar='SITE', action='append',
                    help="search sites using DuckDuckGo")
    ddgr_group.add_argument('--ddgr-expand', action='store_true',
                    help="Show complete url in search results")
    ddgr_group.add_argument('--ddgr-proxy', type=str, metavar='URI',
                    help="tunnel traffic through an HTTPS proxy; URI format: [http[s]://][user:pwd@]host[:port]")
    ddgr_group.add_argument('--ddgr-unsafe', action='store_true',
                    help="disable safe search")
    ddgr_group.add_argument('--ddgr-noua', action='store_true',
                    help="disable user agent")
    
    # Photon-specific options
    photon_group = parser.add_argument_group('Photon OSINT Options')
    photon_group.add_argument('--photon-level', type=int, metavar='LEVEL', 
                    help="levels to crawl (1-5)")
    photon_group.add_argument('--photon-threads', type=int, metavar='THREADS',
                    help="number of threads")
    photon_group.add_argument('--photon-delay', type=float, metavar='DELAY',
                    help="delay between requests")
    photon_group.add_argument('--photon-timeout', type=float, metavar='TIMEOUT',
                    help="http request timeout")
    photon_group.add_argument('--photon-cookie', type=str, metavar='COOKIE',
                    help="cookie")
    photon_group.add_argument('--photon-regex', type=str, metavar='REGEX',
                    help="regex pattern")
    photon_group.add_argument('--photon-export', type=str, metavar='FORMAT', choices=['csv', 'json'],
                    help="export format (csv, json)")
    photon_group.add_argument('--photon-seeds', type=str, metavar='SEEDS', action='append',
                    help="additional seed URLs")
    photon_group.add_argument('--photon-user-agent', type=str, metavar='UA',
                    help="custom user agent(s)")
    photon_group.add_argument('--photon-exclude', type=str, metavar='REGEX',
                    help="exclude URLs matching this regex")
    photon_group.add_argument('--photon-proxy', type=str, metavar='PROXY',
                    help="Proxy server IP:PORT or DOMAIN:PORT")
    photon_group.add_argument('--photon-verbose', action='store_true',
                    help="verbose output")
    photon_group.add_argument('--photon-headers', action='store_true',
                    help="add headers")
    photon_group.add_argument('--photon-dns', action='store_true',
                    help="enumerate subdomains and DNS data")
    photon_group.add_argument('--photon-keys', action='store_true',
                    help="find secret keys")
    photon_group.add_argument('--photon-only-urls', action='store_true',
                    help="only extract URLs")
    photon_group.add_argument('--photon-wayback', action='store_true',
                    help="fetch URLs from archive.org as seeds")
    photon_group.add_argument('--photon-max-targets', type=int, metavar='N',
                    help="maximum number of targets to crawl (overrides automatic scaling)")
    
    # Positional arguments (for simpler command line usage)
    parser.add_argument('keywords', type=str, nargs='?', 
                    help="Search keywords (e.g., \"George Washington\")")
    parser.add_argument('url_or_depth', type=str, nargs='?',
                    help="Either a URL to crawl (e.g., \"https://example.com\") or depth value (e.g., \"45\")")
    parser.add_argument('positional_depth', type=str, nargs='?',
                    help="Depth value when URL is provided (e.g., \"45\" when using format: \"keywords URL depth\")")
    
    args = parser.parse_args()

    # Configure logging based on the debug flag
    logging_level = logging.DEBUG if args.debug else logging.INFO
    logging.basicConfig(level=logging_level, format='[%(levelname)s] %(message)s')
    
    if args.debug:
        logging.debug("Debug mode enabled.")

    # Process positional arguments if provided
    if args.keywords:
        # We have at least the keywords argument
        query = args.keywords
        target = None
        depth = args.depth  # Default to named argument depth
        
        if args.url_or_depth:
            # Second argument could be either a URL or depth
            if is_url(args.url_or_depth):
                # It's a URL
                target = args.url_or_depth
                # If we have a third positional argument, it's the depth
                if args.positional_depth and args.positional_depth.isdigit():
                    depth = int(args.positional_depth)
            elif args.url_or_depth.isdigit():
                # It's a depth value
                depth = int(args.url_or_depth)
    else:
        # Use named arguments
        query = args.query
        target = args.target
        depth = args.depth
    
    # Ensure depth is within valid range
    if depth is not None:
        depth = max(1, min(100, depth))
    
    # Ensure we have at least one action to perform
    if not query and not target:
        parser.error("You must provide either keywords, a URL, or use --query/--target arguments.")
    
    # Initialize list of URLs to crawl with Photon
    urls_to_crawl = []
    
    # Add specifically provided target URL if any
    if target:
        urls_to_crawl.append(target)
    
    # Prepare ddgr arguments
    ddgr_args = {
        'region': args.ddgr_region,
        'colorize': args.ddgr_colorize,
        'nocolor': args.ddgr_nocolor,
        'colors': args.ddgr_colors,
        'time': args.ddgr_time,
        'site': args.ddgr_site,
        'expand': args.ddgr_expand,
        'proxy': args.ddgr_proxy,
        'unsafe': args.ddgr_unsafe,
        'noua': args.ddgr_noua
    }
    
    # Prepare photon arguments
    photon_args = {
        'level': args.photon_level,
        'threads': args.photon_threads,
        'delay': args.photon_delay,
        'timeout': args.photon_timeout,
        'cookie': args.photon_cookie,
        'regex': args.photon_regex,
        'export': args.photon_export,
        'seeds': args.photon_seeds,
        'user_agent': args.photon_user_agent,
        'exclude': args.photon_exclude,
        'proxy': args.photon_proxy,
        'verbose': args.photon_verbose,
        'headers': args.photon_headers,
        'dns': args.photon_dns,
        'keys': args.photon_keys,
        'only_urls': args.photon_only_urls,
        'wayback': args.photon_wayback,
        'max_targets': args.photon_max_targets
    }
    
    # Run ddgr search if query is provided
    if query:
        page = 1  # Start at page 1 (1-based for user display)
        has_more = True
        processed_urls = set()  # Keep track of all URLs found (as a set to avoid duplicates)
        stagnant_page_count = 0  # Track how many consecutive pages yield no new URLs
        
        print(f"\n[+] Running enhanced deep search for: {query}")
        print(f"[+] Search depth: {depth}/100 (Maximum results mode enabled)")
        
        while has_more:
            # Run search for current page with proper pagination
            search_urls, has_more = run_ddgr_with_pagination(query, ddgr_args, depth, args.debug, page)
            
            # Track if we found new URLs in this page
            found_new_urls = False
            
            # Process search results and add to deduplication set
            if search_urls:
                # Get only unique new URLs
                unique_new_urls = deduplicate_urls(search_urls, processed_urls)
                
                if unique_new_urls:
                    found_new_urls = True
                    print(f"[+] Found {len(unique_new_urls)} new unique URLs on page {page}")
                    print(f"[+] Total unique URLs found so far: {len(processed_urls)}")
                    
                    # Add search results to crawl list if auto-crawl is enabled
                    if not args.no_crawl:
                        # Run Photon only on the unique new URLs
                        run_photon_on_multiple_targets(unique_new_urls, photon_args, depth, args.debug)
                else:
                    print(f"[+] No new unique URLs found on page {page}")
                    stagnant_page_count += 1
            else:
                print(f"[+] No URLs found on page {page}")
                stagnant_page_count += 1
            
            # Intelligent pagination stop conditions:
            
            # 1. Check if we've had too many consecutive pages with no new results
            if stagnant_page_count >= 3:
                print(f"[+] No new unique URLs found for {stagnant_page_count} consecutive pages, stopping pagination")
                break
                
            # 2. Check if we've reached the maximum page limit
            if args.max_pages > 0 and page >= args.max_pages:
                print(f"[+] Reached maximum page limit ({args.max_pages} pages)")
                break
                
            # 3. Check if pagination is disabled or if we've reached the end
            if args.no_pagination or not has_more:
                break
                
            # 4. Reset stagnant page counter if we found new URLs
            if found_new_urls:
                stagnant_page_count = 0
            
            # 5. If auto-paginate is enabled, continue without asking
            if args.auto_paginate:
                page += 1
                print(f"[+] Automatically proceeding to page {page}...")
                continue
                
            # 6. Ask for more results
            if not ask_for_more():
                break
                
            # Move to next page
            page += 1
            
        print(f"\n[+] Search complete. Processed {page} page(s) with {len(processed_urls)} unique URLs.")
        
    elif target and not args.no_crawl:
        # If no query but a target was provided, crawl that target
        run_photon_on_single_target(target, photon_args, depth, args.debug)

if __name__ == '__main__':
    main()

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/plugins/find_subdomains.py
========================================

"""Support for findsubdomains.com."""
from re import findall

from requests import get


def find_subdomains(domain):
    """Find subdomains according to the TLD."""
    result = set()
    response = get('https://findsubdomains.com/subdomains-of/' + domain).text
    matches = findall(r'(?s)<div class="domains js-domain-name">(.*?)</div>', response)
    for match in matches:
        result.add(match.replace(' ', '').replace('\n', ''))
    return list(result)

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/plugins/wayback.py
========================================

"""Support for archive.org."""
import datetime
import json

from requests import get


def time_machine(host, mode):
    """Query archive.org."""
    now = datetime.datetime.now()
    to = str(now.year) + str(now.day) + str(now.month)
    if now.month > 6:
    	fro = str(now.year) + str(now.day) + str(now.month - 6)
    else:
    	fro = str(now.year - 1) + str(now.day) + str(now.month + 6)
    url = "http://web.archive.org/cdx/search?url=%s&matchType=%s&collapse=urlkey&fl=original&filter=mimetype:text/html&filter=statuscode:200&output=json&from=%s&to=%s" % (host, mode, fro, to)
    response = get(url).text
    parsed = json.loads(response)[1:]
    urls = []
    for item in parsed:
        urls.append(item[0])
    return urls

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/plugins/exporter.py
========================================

"""Support for exporting the results."""
import csv
import json


def exporter(directory, method, datasets):
    """Export the results."""
    if method.lower() == 'json':
        # Convert json_dict to a JSON styled string
        json_string = json.dumps(datasets, indent=4)
        savefile = open('{}/exported.json'.format(directory), 'w+')
        savefile.write(json_string)
        savefile.close()

    if method.lower() == 'csv':
        with open('{}/exported.csv'.format(directory), 'w+') as csvfile:
            csv_writer = csv.writer(
                csvfile, delimiter=',', quoting=csv.QUOTE_MINIMAL)
            for key, values in datasets.items():
                if values is None:
                    csv_writer.writerow([key])
                else:
                    csv_writer.writerow([key] + values)
        csvfile.close()

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/plugins/__init__.py
========================================

"""Plugins for Photon."""

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/plugins/dnsdumpster.py
========================================

"""Support for dnsdumpster.com."""
import re

import requests


def dnsdumpster(domain, output_dir):
    """Query dnsdumpster.com."""
    response = requests.Session().get('https://dnsdumpster.com/').text
    csrf_token = re.search(
        r'name=\"csrfmiddlewaretoken\" value=\"(.*?)\"', response).group(1)

    cookies = {'csrftoken': csrf_token}
    headers = {'Referer': 'https://dnsdumpster.com/'}
    data = {'csrfmiddlewaretoken': csrf_token, 'targetip': domain}
    response = requests.Session().post(
        'https://dnsdumpster.com/', cookies=cookies, data=data, headers=headers)

    image = requests.get('https://dnsdumpster.com/static/map/%s.png' % domain)
    if image.status_code == 200:
        with open('%s/%s.png' % (output_dir, domain), 'wb') as f:
            f.write(image.content)

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/sumsearchweb/ app.py
========================================

#!/usr/bin/env python3

import os
import re
import subprocess
import webbrowser
from flask import Flask, jsonify, request, send_from_directory
from flask_cors import CORS

# Change this path to the directory where you want to look for summary.txt files
BASE_DIRECTORY = "/home/jarvis/photon_results"

app = Flask(__name__, static_folder='static')
CORS(app)  # Enable CORS for all routes

# Regular expression pattern to match URLs
URL_PATTERN = r'https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+'

def gather_summary_data(base_path):
    """
    Recursively walk through 'base_path' and gather information about all 'summary.txt' files.
    Returns a list of dictionaries with id, folder_path and file_path.
    """
    all_summaries = []
    id_counter = 1

    for root, dirs, files in os.walk(base_path):
        for file in files:
            if file.lower() == "summary.txt":
                filepath = os.path.join(root, file)
                try:
                    relative_folder = os.path.relpath(root, base_path)
                    all_summaries.append({
                        "id": id_counter,
                        "folder": relative_folder,
                        "filepath": filepath
                    })
                    id_counter += 1
                except Exception as e:
                    print(f"Error processing {filepath}: {e}")

    return all_summaries

def get_summary_content(filepath):
    """Read and return the content of a summary file"""
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            content = f.read()
        return content
    except Exception as e:
        print(f"Error reading {filepath}: {e}")
        return f"Error reading file: {str(e)}"

def open_file_location(filepath):
    """Open the file's directory in the system file explorer"""
    directory = os.path.dirname(filepath)
    
    # Platform-specific commands to open the file explorer
    try:
        if os.name == 'nt':  # Windows
            os.startfile(directory)
        elif os.name == 'posix':  # Linux, macOS
            if os.system(f'xdg-open "{directory}"') != 0:  # Try Linux first
                os.system(f'open "{directory}"')  # Try macOS
        return True
    except Exception as e:
        print(f"Error opening location: {e}")
        return False

# API Routes
@app.route('/')
def serve_index():
    return send_from_directory('static', 'index.html')

@app.route('/api/summaries')
def get_summaries():
    """API endpoint to get all summaries"""
    try:
        summaries = gather_summary_data(BASE_DIRECTORY)
        return jsonify(summaries)
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/summary/<int:summary_id>')
def get_summary(summary_id):
    """API endpoint to get a specific summary's content"""
    try:
        summaries = gather_summary_data(BASE_DIRECTORY)
        
        # Find the summary with the matching ID
        summary = next((s for s in summaries if s["id"] == summary_id), None)
        
        if not summary:
            return jsonify({"error": "Summary not found"}), 404
        
        content = get_summary_content(summary["filepath"])
        
        return jsonify({
            "summary": summary,
            "content": content
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/open-location', methods=['POST'])
def api_open_location():
    """API endpoint to open a file location"""
    try:
        data = request.json
        if not data or 'path' not in data:
            return jsonify({"error": "No path provided"}), 400
        
        filepath = data['path']
        success = open_file_location(filepath)
        
        if success:
            return jsonify({"status": "success"})
        else:
            return jsonify({"error": "Failed to open location"}), 500
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/open-url', methods=['POST'])
def api_open_url():
    """API endpoint to open a URL in the browser"""
    try:
        data = request.json
        if not data or 'url' not in data:
            return jsonify({"error": "No URL provided"}), 400
        
        url = data['url']
        # Validate URL
        if not re.match(URL_PATTERN, url):
            return jsonify({"error": "Invalid URL"}), 400
            
        webbrowser.open(url)
        return jsonify({"status": "success"})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == "__main__":
    # Create the static directory if it doesn't exist
    os.makedirs('static', exist_ok=True)
    
    # Copy the HTML file to the static directory (in a real app, you'd have a build process)
    # For now, assuming the HTML file is in the same directory as this script
    try:
        with open('frontend.html', 'r') as src, open('static/index.html', 'w') as dst:
            dst.write(src.read())
    except FileNotFoundError:
        print("Warning: frontend.html not found. Please place the HTML file in the static directory.")
    
    # Start the Flask server
    print(f"Starting server on http://localhost:5000")
    app.run(debug=True, host='0.0.0.0', port=5000)

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/core/mirror.py
========================================

import os


def mirror(url, response):
    if response != 'dummy':
        clean_url = url.replace('http://', '').replace('https://', '').rstrip('/')
        parts = clean_url.split('?')[0].split('/')
        root = parts[0]
        webpage = parts[-1]
        parts.remove(root)
        try:
            parts.remove(webpage)
        except ValueError:
            pass
        prefix = root + '_mirror'
        try:
            os.mkdir(prefix)
        except OSError:
            pass
        suffix = ''
        if parts:
            for directory in parts:
                suffix += directory + '/'
                try:
                    os.mkdir(prefix + '/' + suffix)
                except OSError:
                    pass
        path = prefix + '/' + suffix
        trail = ''
        if '.' not in webpage:
            trail += '.html'
        if webpage == root:
            name = 'index.html'
        else:
            name = webpage
        if len(url.split('?')) > 1:
            trail += '?' + url.split('?')[1]
        with open(path + name + trail, 'w+') as out_file:
            out_file.write(response.encode('utf-8'))

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/core/config.py
========================================

"""Configuration options for Photon."""

VERBOSE = False

INTELS = [
    'facebook.com',
    'github.com',
    'instagram.com',
    'youtube.com',
]

BAD_TYPES = (
    'bmp',
    'css',
    'csv',
    'docx',
    'ico',
    'jpeg',
    'jpg',
    'js',
    'json',
    'pdf',
    'png',
    'svg',
    'xls',
    'xml',
)

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/core/colors.py
========================================

import sys


if sys.platform.lower().startswith(('os', 'win', 'darwin', 'ios')):
    # Colors shouldn't be displayed on Mac and Windows
    end = red = white = green = yellow = run = bad = good = info = que = ''
else:
    white = '\033[97m'
    green = '\033[92m'
    red = '\033[91m'
    yellow = '\033[93m'
    end = '\033[0m'
    back = '\033[7;91m'
    info = '\033[93m[!]\033[0m'
    que = '\033[94m[?]\033[0m'
    bad = '\033[91m[-]\033[0m'
    good = '\033[92m[+]\033[0m'
    run = '\033[97m[~]\033[0m'

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/core/flash.py
========================================

from __future__ import print_function
import concurrent.futures

from core.colors import info

def flash(function, links, thread_count):
    """Process the URLs and uses a threadpool to execute a function."""
    # Convert links (set) to list
    links = list(links)
    threadpool = concurrent.futures.ThreadPoolExecutor(
            max_workers=thread_count)
    futures = (threadpool.submit(function, link) for link in links)
    for i, _ in enumerate(concurrent.futures.as_completed(futures)):
        if i + 1 == len(links) or (i + 1) % thread_count == 0:
            print('%s Progress: %i/%i' % (info, i + 1, len(links)),
                    end='\r')
    print('')

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/core/prompt.py
========================================

"""Support for an input prompt."""
import os
import tempfile


def prompt(default=None):
    """Present the user a prompt."""
    editor = 'nano'
    with tempfile.NamedTemporaryFile(mode='r+') as tmpfile:
        if default:
            tmpfile.write(default)
            tmpfile.flush()

        child_pid = os.fork()
        is_child = child_pid == 0

        if is_child:
            os.execvp(editor, [editor, tmpfile.name])
        else:
            os.waitpid(child_pid, 0)
            tmpfile.seek(0)
            return tmpfile.read().strip()

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/core/requester.py
========================================

import random
import time

import requests
from requests.exceptions import TooManyRedirects


SESSION = requests.Session()
SESSION.max_redirects = 3

def requester(
        url,
        main_url=None,
        delay=0,
        cook=None,
        headers=None,
        timeout=10,
        host=None,
        proxies=[None],
        user_agents=[None],
        failed=None,
        processed=None
    ):
    """Handle the requests and return the response body."""
    cook = cook or set()
    headers = headers or set()
    user_agents = user_agents or ['Photon']
    failed = failed or set()
    processed = processed or set()
    # Mark the URL as crawled
    processed.add(url)
    # Pause/sleep the program for specified time
    time.sleep(delay)

    def make_request(url):
        """Default request"""
        final_headers = headers or {
            'Host': host,
            # Selecting a random user-agent
            'User-Agent': random.choice(user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip',
            'DNT': '1',
            'Connection': 'close',
        }
        try:
            response = SESSION.get(
                url,
                cookies=cook,
                headers=final_headers,
                verify=False,
                timeout=timeout,
                stream=True,
                proxies=random.choice(proxies)
            )
        except TooManyRedirects:
            return 'dummy'

        if 'text/html' in response.headers['content-type'] or \
           'text/plain' in response.headers['content-type']:
            if response.status_code != '404':
                return response.text
            else:
                response.close()
                failed.add(url)
                return 'dummy'
        else:
            response.close()
            return 'dummy'

    return make_request(url)

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/core/updater.py
========================================

import os
import re

from core.colors import run, que, good, green, end, info
from core.requester import requester


def updater():
    """Update the current installation.

    git clones the latest version and merges it with the current directory.
    """
    print('%s Checking for updates' % run)
    # Changes must be separated by ;
    changes = '''major bug fixes;removed ninja mode;dropped python < 3.2 support;fixed unicode output;proxy support;more intels'''
    latest_commit = requester('https://raw.githubusercontent.com/s0md3v/Photon/master/core/updater.py', host='raw.githubusercontent.com')
    # Just a hack to see if a new version is available
    if changes not in latest_commit:
        changelog = re.search(r"changes = '''(.*?)'''", latest_commit)
        # Splitting the changes to form a list
        changelog = changelog.group(1).split(';')
        print('%s A new version of Photon is available.' % good)
        print('%s Changes:' % info)
        for change in changelog: # print changes
            print('%s>%s %s' % (green, end, change))

        current_path = os.getcwd().split('/') # if you know it, you know it
        folder = current_path[-1] # current directory name
        path = '/'.join(current_path) # current directory path
        choice = input('%s Would you like to update? [Y/n] ' % que).lower()

        if choice != 'n':
            print('%s Updating Photon' % run)
            os.system('git clone --quiet https://github.com/s0md3v/Photon %s'
                      % (folder))
            os.system('cp -r %s/%s/* %s && rm -r %s/%s/ 2>/dev/null'
                      % (path, folder, path, path, folder))
            print('%s Update successful!' % good)
    else:
        print('%s Photon is up to date!' % good)

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/core/zap.py
========================================

import re
import requests
import random

from core.utils import verb, xml_parser
from core.colors import run, good
from plugins.wayback import time_machine


def zap(input_url, archive, domain, host, internal, robots, proxies):
    """Extract links from robots.txt and sitemap.xml."""
    if archive:
        print('%s Fetching URLs from archive.org' % run)
        if False:
            archived_urls = time_machine(domain, 'domain')
        else:
            archived_urls = time_machine(host, 'host')
        print('%s Retrieved %i URLs from archive.org' % (
            good, len(archived_urls) - 1))
        for url in archived_urls:
            verb('Internal page', url)
            internal.add(url)
    # Makes request to robots.txt
    response = requests.get(input_url + '/robots.txt',
                            proxies=random.choice(proxies)).text
    # Making sure robots.txt isn't some fancy 404 page
    if '<body' not in response:
        # If you know it, you know it
        matches = re.findall(r'Allow: (.*)|Disallow: (.*)', response)
        if matches:
            # Iterating over the matches, match is a tuple here
            for match in matches:
                # One item in match will always be empty so will combine both
                # items
                match = ''.join(match)
                # If the URL doesn't use a wildcard
                if '*' not in match:
                    url = input_url + match
                    # Add the URL to internal list for crawling
                    internal.add(url)
                    # Add the URL to robots list
                    robots.add(url)
            print('%s URLs retrieved from robots.txt: %s' % (good, len(robots)))
    # Makes request to sitemap.xml
    response = requests.get(input_url + '/sitemap.xml',
                            proxies=random.choice(proxies)).text
    # Making sure robots.txt isn't some fancy 404 page
    if '<body' not in response:
        matches = xml_parser(response)
        if matches: # if there are any matches
            print('%s URLs retrieved from sitemap.xml: %s' % (
                good, len(matches)))
            for match in matches:
                verb('Internal page', match)
                # Cleaning up the URL and adding it to the internal list for
                # crawling
                internal.add(match)

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/core/__init__.py
========================================

"""The Photon core."""

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/core/utils.py
========================================

import requests
import math
import os.path
import re
import argparse

import tld

from core.colors import info
from core.config import VERBOSE, BAD_TYPES

from urllib.parse import urlparse


def regxy(pattern, response, supress_regex, custom):
    """Extract a string based on regex pattern supplied by user."""
    try:
        matches = re.findall(r'%s' % pattern, response)
        for match in matches:
            verb('Custom regex', match)
            custom.add(match)
    except:
        supress_regex = True


def is_link(url, processed, files):
    """
    Determine whether or not a link should be crawled
    A url should not be crawled if it
        - Is a file
        - Has already been crawled

    Args:
        url: str Url to be processed
        processed: list[str] List of urls that have already been crawled

    Returns:
        bool If `url` should be crawled
    """
    if url not in processed:
        if url.startswith('#') or url.startswith('javascript:'):
            return False
        is_file = url.endswith(BAD_TYPES)
        if is_file:
            files.add(url)
            return False
        return True
    return False


def remove_regex(urls, regex):
    """
    Parse a list for non-matches to a regex.

    Args:
        urls: iterable of urls
        regex: string regex to be parsed for

    Returns:
        list of strings not matching regex
    """

    if not regex:
        return urls

    # To avoid iterating over the characters of a string
    if not isinstance(urls, (list, set, tuple)):
        urls = [urls]

    try:
        non_matching_urls = [url for url in urls if not re.search(regex, url)]
    except TypeError:
        return []

    return non_matching_urls


def writer(datasets, dataset_names, output_dir):
    """Write the results."""
    for dataset, dataset_name in zip(datasets, dataset_names):
        if dataset:
            filepath = output_dir + '/' + dataset_name + '.txt'
            with open(filepath, 'w+') as out_file:
                joined = '\n'.join(dataset)
                out_file.write(str(joined.encode('utf-8').decode('utf-8')))
                out_file.write('\n')


def timer(diff, processed):
    """Return the passed time."""
    # Changes seconds into minutes and seconds
    minutes, seconds = divmod(diff, 60)
    try:
        # Finds average time taken by requests
        time_per_request = diff / float(len(processed))
    except ZeroDivisionError:
        time_per_request = 0
    return minutes, seconds, time_per_request


def entropy(string):
    """Calculate the entropy of a string."""
    entropy = 0
    for number in range(256):
        result = float(string.encode('utf-8').count(
            chr(number))) / len(string.encode('utf-8'))
        if result != 0:
            entropy = entropy - result * math.log(result, 2)
    return entropy


def xml_parser(response):
    """Extract links from .xml files."""
    # Regex for extracting URLs
    return re.findall(r'<loc>(.*?)</loc>', response)


def verb(kind, string):
    """Enable verbose output."""
    if VERBOSE:
        print('%s %s: %s' % (info, kind, string))


def extract_headers(headers):
    """This function extracts valid headers from interactive input."""
    sorted_headers = {}
    matches = re.findall(r'(.*):\s(.*)', headers)
    for match in matches:
        header = match[0]
        value = match[1]
        try:
            if value[-1] == ',':
                value = value[:-1]
            sorted_headers[header] = value
        except IndexError:
            pass
    return sorted_headers


def top_level(url, fix_protocol=True):
    """Extract the top level domain from an URL."""
    ext = tld.get_tld(url, fix_protocol=fix_protocol)
    toplevel = '.'.join(urlparse(url).netloc.split('.')[-2:]).split(
        ext)[0] + ext
    return toplevel


def is_proxy_list(v, proxies):
    if os.path.isfile(v):
        with open(v, 'r') as _file:
            for line in _file:
                line = line.strip()
                if re.match(r"((http|socks5):\/\/.)?(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}):(\d{1,5})", line) or \
                   re.match(r"((http|socks5):\/\/.)?[-a-zA-Z0-9@:%._\+~#=]{2,256}\.[a-z]{2,6}:(\d{1,5})", line):
                    proxies.append({"http": line,
                                    "https": line})
                else:
                    print("%s ignored" % line)
        if proxies:
            return True
    return False


def proxy_type(v):
    """ Match IP:PORT or DOMAIN:PORT in a losse manner """
    proxies = []
    if re.match(r"((http|socks5):\/\/.)?(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}):(\d{1,5})", v):
        proxies.append({"http": v,
                        "https": v})
        return proxies
    elif re.match(r"((http|socks5):\/\/.)?[-a-zA-Z0-9@:%._\+~#=]{2,256}\.[a-z]{2,6}:(\d{1,5})", v):
        proxies.append({"http": v,
                        "https": v})
        return proxies
    elif is_proxy_list(v, proxies):
        return proxies
    else:
        raise argparse.ArgumentTypeError(
            "Proxy should follow IP:PORT or DOMAIN:PORT format")


def luhn(purported):

    # sum_of_digits (index * 2)
    LUHN_ODD_LOOKUP = (0, 2, 4, 6, 8, 1, 3, 5, 7, 9)

    if not isinstance(purported, str):
        purported = str(purported)
    try:
        evens = sum(int(p) for p in purported[-1::-2])
        odds = sum(LUHN_ODD_LOOKUP[int(p)] for p in purported[-2::-2])
        return (evens + odds) % 10 == 0
    except ValueError:  # Raised if an int conversion fails
        return False


def is_good_proxy(pip):
    try:
        requests.get('http://example.com', proxies=pip, timeout=3)
    except requests.exceptions.ConnectTimeout as e:
        return False
    except Exception as detail:
        return False

    return True


========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/core/regex.py
========================================

import re

# regex taken from https://github.com/InQuest/python-iocextract
# Reusable end punctuation regex.
END_PUNCTUATION = r"[\.\?>\"'\)!,}:;\u201d\u2019\uff1e\uff1c\]]*"

# Reusable regex for symbols commonly used to defang.
SEPARATOR_DEFANGS = r"[\(\)\[\]{}<>\\]"

# Split URLs on some characters that may be valid, but may also be garbage.
URL_SPLIT_STR = r"[>\"'\),};]"

# Get basic url format, including a few obfuscation techniques, main anchor is the uri scheme.
GENERIC_URL = re.compile(r"""
        (
            # Scheme.
            [fhstu]\S\S?[px]s?
            # One of these delimiters/defangs.
            (?:
                :\/\/|
                :\\\\|
                :?__
            )
            # Any number of defang characters.
            (?:
                \x20|
                """ + SEPARATOR_DEFANGS + r"""
            )*
            # Domain/path characters.
            \w
            \S+?
            # CISCO ESA style defangs followed by domain/path characters.
            (?:\x20[\/\.][^\.\/\s]\S*?)*
        )
    """ + END_PUNCTUATION + r"""
        (?=\s|$)
    """, re.IGNORECASE | re.VERBOSE | re.UNICODE)

# Get some obfuscated urls, main anchor is brackets around the period.
BRACKET_URL = re.compile(r"""
        \b
        (
            [\.\:\/\\\w\[\]\(\)-]+
            (?:
                \x20?
                [\(\[]
                \x20?
                \.
                \x20?
                [\]\)]
                \x20?
                \S*?
            )+
        )
    """ + END_PUNCTUATION + r"""
        (?=\s|$)
    """, re.VERBOSE | re.UNICODE)

# Get some obfuscated urls, main anchor is backslash before a period.
BACKSLASH_URL = re.compile(r"""
        \b
        (
            [\:\/\\\w\[\]\(\)-]+
            (?:
                \x20?
                \\?\.
                \x20?
                \S*?
            )*?
            (?:
                \x20?
                \\\.
                \x20?
                \S*?
            )
            (?:
                \x20?
                \\?\.
                \x20?
                \S*?
            )*
        )
    """ + END_PUNCTUATION + r"""
        (?=\s|$)
    """, re.VERBOSE | re.UNICODE)

# Get hex-encoded urls.
HEXENCODED_URL = re.compile(r"""
        (
            [46][86]
            (?:[57]4)?
            [57]4[57]0
            (?:[57]3)?
            3a2f2f
            (?:2[356def]|3[0-9adf]|[46][0-9a-f]|[57][0-9af])+
        )
        (?:[046]0|2[0-2489a-c]|3[bce]|[57][b-e]|[8-f][0-9a-f]|0a|0d|09|[
            \x5b-\x5d\x7b\x7d\x0a\x0d\x20
        ]|$)
    """, re.IGNORECASE | re.VERBOSE)

# Get urlencoded urls.
URLENCODED_URL = re.compile(r"""
        (s?[hf]t?tps?%3A%2F%2F\w[\w%-]*?)(?:[^\w%-]|$)
    """, re.IGNORECASE | re.VERBOSE)

# Get base64-encoded urls.
B64ENCODED_URL = re.compile(r"""
        (
            # b64re '([hH][tT][tT][pP][sS]|[hH][tT][tT][pP]|[fF][tT][pP])://'
            # Modified to ignore whitespace.
            (?:
                [\x2b\x2f-\x39A-Za-z]\s*[\x2b\x2f-\x39A-Za-z]\s*[\x31\x35\x39BFJNRVZdhlptx]\s*[Gm]\s*[Vd]\s*[FH]\s*[A]\s*\x36\s*L\s*y\s*[\x2b\x2f\x38-\x39]\s*|
                [\x2b\x2f-\x39A-Za-z]\s*[\x2b\x2f-\x39A-Za-z]\s*[\x31\x35\x39BFJNRVZdhlptx]\s*[Io]\s*[Vd]\s*[FH]\s*[R]\s*[Qw]\s*[O]\s*i\s*\x38\s*v\s*[\x2b\x2f-\x39A-Za-z]\s*|
                [\x2b\x2f-\x39A-Za-z]\s*[\x2b\x2f-\x39A-Za-z]\s*[\x31\x35\x39BFJNRVZdhlptx]\s*[Io]\s*[Vd]\s*[FH]\s*[R]\s*[Qw]\s*[Uc]\s*[z]\s*o\s*v\s*L\s*[\x2b\x2f-\x39w-z]\s*|
                [\x2b\x2f-\x39A-Za-z]\s*[\x30\x32EGUWkm]\s*[Z]\s*[\x30U]\s*[Uc]\s*[D]\s*o\s*v\s*L\s*[\x2b\x2f-\x39w-z]\s*|
                [\x2b\x2f-\x39A-Za-z]\s*[\x30\x32EGUWkm]\s*[h]\s*[\x30U]\s*[Vd]\s*[FH]\s*[A]\s*\x36\s*L\s*y\s*[\x2b\x2f\x38-\x39]\s*|
                [\x2b\x2f-\x39A-Za-z]\s*[\x30\x32EGUWkm]\s*[h]\s*[\x30U]\s*[Vd]\s*[FH]\s*[B]\s*[Tz]\s*[O]\s*i\s*\x38\s*v\s*[\x2b\x2f-\x39A-Za-z]\s*|
                [RZ]\s*[ln]\s*[R]\s*[Qw]\s*[O]\s*i\s*\x38\s*v\s*[\x2b\x2f-\x39A-Za-z]\s*|
                [Sa]\s*[FH]\s*[R]\s*[\x30U]\s*[Uc]\s*[D]\s*o\s*v\s*L\s*[\x2b\x2f-\x39w-z]\s*|
                [Sa]\s*[FH]\s*[R]\s*[\x30U]\s*[Uc]\s*[FH]\s*[M]\s*\x36\s*L\s*y\s*[\x2b\x2f\x38-\x39]\s*
            )
            # Up to 260 characters (pre-encoding, reasonable URL length).
            [A-Za-z0-9+/=\s]{1,357}
        )
        (?=[^A-Za-z0-9+/=\s]|$)
    """, re.VERBOSE)

# Get some valid obfuscated ip addresses.
IPV4 = re.compile(r"""
        (?:^|
            (?![^\d\.])
        )
        (?:
            (?:[1-9]?\d|1\d\d|2[0-4]\d|25[0-5])
            [\[\(\\]*?\.[\]\)]*?
        ){3}
        (?:[1-9]?\d|1\d\d|2[0-4]\d|25[0-5])
        (?:(?=[^\d\.])|$)
    """, re.VERBOSE)

# Experimental IPv6 regex, will not catch everything but should be sufficent for now.
IPV6 = re.compile(r"""
        \b(?:[a-f0-9]{1,4}:|:){2,7}(?:[a-f0-9]{1,4}|:)\b
    """, re.IGNORECASE | re.VERBOSE)

# Capture email addresses including common defangs.
EMAIL = re.compile(r"""
        (
            [a-z0-9_.+-]+
            [\(\[{\x20]*
            (?:@|\Wat\W)
            [\)\]}\x20]*
            [a-z0-9-]+
            (?:
                (?:
                    (?:
                        \x20*
                        """ + SEPARATOR_DEFANGS + r"""
                        \x20*
                    )*
                    \.
                    (?:
                        \x20*
                        """ + SEPARATOR_DEFANGS + r"""
                        \x20*
                    )*
                    |
                    \W+dot\W+
                )
                [a-z0-9-]+?
            )+
        )
    """ + END_PUNCTUATION + r"""
        (?=\s|$)
    """, re.IGNORECASE | re.VERBOSE | re.UNICODE)

MD5 = re.compile(r"(?:[^a-fA-F\d]|\b)([a-fA-F\d]{32})(?:[^a-fA-F\d]|\b)")
SHA1 = re.compile(r"(?:[^a-fA-F\d]|\b)([a-fA-F\d]{40})(?:[^a-fA-F\d]|\b)")
SHA256 = re.compile(r"(?:[^a-fA-F\d]|\b)([a-fA-F\d]{64})(?:[^a-fA-F\d]|\b)")
SHA512 = re.compile(
    r"(?:[^a-fA-F\d]|\b)([a-fA-F\d]{128})(?:[^a-fA-F\d]|\b)")

# YARA regex.
YARA_PARSE = re.compile(r"""
        (?:^|\s)
        (
            (?:
                \s*?import\s+?"[^\r\n]*?[\r\n]+|
                \s*?include\s+?"[^\r\n]*?[\r\n]+|
                \s*?//[^\r\n]*[\r\n]+|
                \s*?/\*.*?\*/\s*?
            )*
            (?:
                \s*?private\s+|
                \s*?global\s+
            )*
            rule\s*?
            \w+\s*?
            (?:
                :[\s\w]+
            )?
            \s+\{
            .*?
            condition\s*?:
            .*?
            \s*\}
        )
        (?:$|\s)
    """, re.MULTILINE | re.DOTALL | re.VERBOSE)

CREDIT_CARD = re.compile(r"[0-9]{4}[ ]?[-]?[0-9]{4}[ ]?[-]?[0-9]{4}[ ]?[-]?[0-9]{4}")

rintels = [(GENERIC_URL, "GENERIC_URL"),
           (BRACKET_URL, "BRACKET_URL"),
           (BACKSLASH_URL, "BACKSLASH_URL"),
           (HEXENCODED_URL, "HEXENCODED_URL"),
           (URLENCODED_URL, "URLENCODED_URL"),
           (B64ENCODED_URL, "B64ENCODED_URL"),
           (IPV4, "IPV4"),
           (IPV6, "IPV6"),
           (EMAIL, "EMAIL"),
           (MD5, "MD5"),
           (SHA1, "SHA1"),
           (SHA256, "SHA256"),
           (SHA512, "SHA512"),
           (YARA_PARSE, "YARA_PARSE"),
           (CREDIT_CARD, "CREDIT_CARD")]


rscript = re.compile(r'<(script|SCRIPT).*(src|SRC)=([^\s>]+)')
rhref = re.compile(r'<[aA].*(href|HREF)=([^\s>]+)')
rendpoint = re.compile(r'[\'"](/.*?)[\'"]|[\'"](http.*?)[\'"]')
rentropy = re.compile(r'[\w-]{16,45}')
