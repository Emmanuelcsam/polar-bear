
========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/textrankr.py
========================================

#!/usr/bin/env python3
"""
TextRank Implementation

A Python implementation of the TextRank algorithm for automatic text summarization.
This is used by the Deep Research Assistant for generating summaries without external APIs.
"""

import re
import math
import numpy as np
from collections import Counter

class TextRank:
    """TextRank implementation for extractive text summarization."""
    
    def __init__(self, damping=0.85, convergence_threshold=1e-5, max_iterations=50):
        """
        Initialize the TextRank algorithm.
        
        Args:
            damping: Damping factor for PageRank algorithm (typically 0.85)
            convergence_threshold: Threshold for PageRank convergence
            max_iterations: Maximum iterations for PageRank algorithm
        """
        self.damping = damping
        self.convergence_threshold = convergence_threshold
        self.max_iterations = max_iterations
        self.stop_words = self._load_stop_words()
    
    def _load_stop_words(self):
        """Load English stop words."""
        # Common English stop words
        return {
            'a', 'an', 'the', 'and', 'or', 'but', 'if', 'because', 'as', 'what',
            'which', 'this', 'that', 'these', 'those', 'then', 'just', 'so', 'than',
            'such', 'both', 'through', 'about', 'for', 'is', 'of', 'while', 'during',
            'to', 'from', 'in', 'out', 'on', 'off', 'at', 'by', 'with', 'into', 'over',
            'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when',
            'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most',
            'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',
            'than', 'too', 'very', 'can', 'will', 'should', 'now', 'i', 'me', 'my',
            'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',
            'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her',
            'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs',
            'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',
            'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have',
            'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'would', 'should',
            'could', 'ought', 'i\'m', 'you\'re', 'he\'s', 'she\'s', 'it\'s', 'we\'re',
            'they\'re', 'i\'ve', 'you\'ve', 'we\'ve', 'they\'ve', 'i\'d', 'you\'d',
            'he\'d', 'she\'d', 'we\'d', 'they\'d', 'i\'ll', 'you\'ll', 'he\'ll',
            'she\'ll', 'we\'ll', 'they\'ll', 'isn\'t', 'aren\'t', 'wasn\'t', 'weren\'t',
            'hasn\'t', 'haven\'t', 'hadn\'t', 'doesn\'t', 'don\'t', 'didn\'t', 'won\'t',
            'wouldn\'t', 'shan\'t', 'shouldn\'t', 'can\'t', 'cannot', 'couldn\'t',
            'mustn\'t', 'let\'s', 'that\'s', 'who\'s', 'what\'s', 'here\'s', 'there\'s',
            'when\'s', 'where\'s', 'why\'s', 'how\'s', 'click', 'site', 'page', 'also'
        }
    
    def _extract_sentences(self, text):
        """
        Extract sentences from text.
        
        Args:
            text: Input text
            
        Returns:
            List of sentences
        """
        # Handle common abbreviations to prevent incorrect sentence splits
        text = re.sub(r'(?i)(\s|^)(mr\.|mrs\.|ms\.|dr\.|prof\.|inc\.|ltd\.|sr\.|jr\.)', 
                      lambda m: m.group(1) + m.group(2).replace('.', '[DOT]'), text)
        
        # Simple sentence splitting based on common punctuation
        # This is a basic implementation; NLTK would be more accurate if available
        sentences = re.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?|\!)\s', text)
        
        # Restore the original periods in abbreviations
        sentences = [s.replace('[DOT]', '.') for s in sentences]
        
        # Filter out empty sentences and those that are too short
        filtered_sentences = []
        for s in sentences:
            s = s.strip()
            if s and len(s.split()) > 3:  # Skip sentences with 3 or fewer words
                filtered_sentences.append(s)
        
        return filtered_sentences
    
    def _tokenize(self, sentence):
        """
        Tokenize a sentence into words.
        
        Args:
            sentence: Input sentence
            
        Returns:
            List of words
        """
        # Convert to lowercase and remove punctuation
        sentence = sentence.lower()
        words = re.findall(r'\b\w+\b', sentence)
        
        # Filter out stop words and short words
        filtered_words = [word for word in words if word not in self.stop_words and len(word) > 2]
        
        return filtered_words
    
    def _calculate_similarity(self, sentence1, sentence2):
        """
        Calculate similarity between two sentences using cosine similarity.
        
        Args:
            sentence1: First sentence
            sentence2: Second sentence
            
        Returns:
            Similarity score between 0 and 1
        """
        # Tokenize sentences
        words1 = self._tokenize(sentence1)
        words2 = self._tokenize(sentence2)
        
        # Skip if either sentence has no content words
        if not words1 or not words2:
            return 0.0
        
        # Create word frequency counters
        counter1 = Counter(words1)
        counter2 = Counter(words2)
        
        # Find all unique words
        all_words = set(counter1.keys()).union(set(counter2.keys()))
        
        # Calculate dot product and magnitudes
        dot_product = sum(counter1.get(word, 0) * counter2.get(word, 0) for word in all_words)
        magnitude1 = math.sqrt(sum(counter1.get(word, 0) ** 2 for word in all_words))
        magnitude2 = math.sqrt(sum(counter2.get(word, 0) ** 2 for word in all_words))
        
        # Avoid division by zero
        if magnitude1 == 0 or magnitude2 == 0:
            return 0.0
        
        return dot_product / (magnitude1 * magnitude2)
    
    def _build_similarity_matrix(self, sentences):
        """
        Build a sentence similarity matrix.
        
        Args:
            sentences: List of sentences
            
        Returns:
            Similarity matrix
        """
        n = len(sentences)
        similarity_matrix = np.zeros((n, n))
        
        for i in range(n):
            for j in range(n):
                if i != j:
                    similarity_matrix[i][j] = self._calculate_similarity(sentences[i], sentences[j])
        
        return similarity_matrix
    
    def _page_rank(self, similarity_matrix):
        """
        Apply PageRank algorithm to the similarity matrix.
        
        Args:
            similarity_matrix: Sentence similarity matrix
            
        Returns:
            Dictionary of sentence indices to PageRank scores
        """
        n = len(similarity_matrix)
        
        # Initialize scores
        scores = np.ones(n) / n
        
        # Normalize the similarity matrix (column-stochastic)
        column_sums = similarity_matrix.sum(axis=0)
        # Avoid division by zero
        column_sums[column_sums == 0] = 1
        stochastic_matrix = similarity_matrix / column_sums
        
        # PageRank algorithm
        for _ in range(self.max_iterations):
            prev_scores = scores.copy()
            
            # Calculate new scores
            scores = (1 - self.damping) / n + self.damping * (stochastic_matrix.T @ scores)
            
            # Check for convergence
            if np.abs(scores - prev_scores).sum() < self.convergence_threshold:
                break
        
        # Convert to dictionary
        return {i: scores[i] for i in range(n)}
    
    def summarize(self, text, ratio=0.2, min_sentences=None, max_sentences=None):
        """
        Generate a summary of the input text.
        
        Args:
            text: Input text to summarize
            ratio: Proportion of sentences to include in the summary (0.0 to 1.0)
            min_sentences: Minimum number of sentences to include
            max_sentences: Maximum number of sentences to include
            
        Returns:
            Generated summary
        """
        # Extract sentences
        sentences = self._extract_sentences(text)
        
        # Handle very short texts
        if len(sentences) <= 3:
            return text
        
        # Build similarity matrix
        similarity_matrix = self._build_similarity_matrix(sentences)
        
        # Apply PageRank
        sentence_scores = self._page_rank(similarity_matrix)
        
        # Determine number of sentences for the summary
        target_count = max(1, int(len(sentences) * ratio))
        
        if min_sentences is not None:
            target_count = max(min_sentences, target_count)
        
        if max_sentences is not None:
            target_count = min(max_sentences, target_count)
        
        # Select top sentences
        ranked_sentences = sorted(sentence_scores.items(), key=lambda x: x[1], reverse=True)
        selected_indices = [idx for idx, _ in ranked_sentences[:target_count]]
        
        # Sort indices to maintain original sentence order
        selected_indices.sort()
        
        # Generate summary
        summary_sentences = [sentences[i] for i in selected_indices]
        summary = ' '.join(summary_sentences)
        
        return summary


# Simple example usage
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        # Read from file if provided
        with open(sys.argv[1], 'r', encoding='utf-8') as f:
            text = f.read()
    else:
        # Example text
        text = """
        TextRank is an algorithm based on Google's PageRank, adapted for text processing.
        It's used for automatic summarization and keyword extraction from documents.
        The algorithm works by building a graph representation of sentences and applying
        a graph-based ranking algorithm to select the most important sentences.
        TextRank is an extractive summarization method, meaning it selects existing
        sentences from the original text rather than generating new ones.
        This approach makes it language-independent and doesn't require training on a corpus.
        """
    
    # Create TextRank instance and generate summary
    textrank = TextRank()
    summary = textrank.summarize(text, ratio=0.5)
    
    print("Original text:")
    print(text)
    print("\nGenerated summary:")
    print(summary)

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/research_finder.py
========================================

#!/usr/bin/env python3
"""
Advanced Research System (ARS) - Ultimate Literature Review & Analysis Tool
===========================================================================

A comprehensive research platform that combines:
- Literature review management with YAML metadata
- Multi-format document processing (PDF, DOCX, TXT, MD, HTML, etc.)
- Advanced full-text and metadata search
- Web research and crawling capabilities
- AI-powered summarization and analysis
- Automated report generation
- Multiple export formats
- Both GUI and CLI interfaces

Version: 2.0.0
Author: Advanced Research System Team
"""

import os
import sys
import re
import csv
import json
import yaml
import time
import math
import hashlib
import logging
import argparse
import threading
import subprocess
import webbrowser
import urllib.parse
import urllib.request
from pathlib import Path
from datetime import datetime
from collections import defaultdict, Counter, OrderedDict
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Tuple, Optional, Any, Union
import string
import traceback
import shutil
import textwrap
import sqlite3
import pickle

# Document processing imports
try:
    import fitz  # PyMuPDF
    PYMUPDF_AVAILABLE = True
except ImportError:
    PYMUPDF_AVAILABLE = False

try:
    import PyPDF2
    PYPDF2_AVAILABLE = True
except ImportError:
    PYPDF2_AVAILABLE = False

try:
    from docx import Document as DocxDocument
    import docx2txt
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False

try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
except ImportError:
    PDFPLUMBER_AVAILABLE = False

try:
    import markdown
    MARKDOWN_AVAILABLE = True
except ImportError:
    MARKDOWN_AVAILABLE = False

try:
    from bs4 import BeautifulSoup, Comment
    BS4_AVAILABLE = True
except ImportError:
    BS4_AVAILABLE = False

try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False

try:
    import nltk
    from nltk.tokenize import sent_tokenize, word_tokenize
    from nltk.corpus import stopwords
    NLTK_AVAILABLE = True
except ImportError:
    NLTK_AVAILABLE = False

# GUI imports
try:
    import tkinter as tk
    from tkinter import ttk, filedialog, messagebox, scrolledtext
    from tkinter.font import Font
    import tkinter.font as tkfont
    GUI_AVAILABLE = True
except ImportError:
    GUI_AVAILABLE = False

# Web framework imports
try:
    from flask import Flask, request, render_template, send_file, jsonify
    FLASK_AVAILABLE = True
except ImportError:
    FLASK_AVAILABLE = False

# AI/ML imports
try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False

# Initialize logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('ARS')

# Global configuration
DEFAULT_CONFIG = {
    # System settings
    "version": "2.0.0",
    "database_path": "research_database.db",
    "output_dir": "research_output",
    "temp_dir": "temp",
    "cache_dir": "cache",
    
    # Document processing
    "supported_formats": [".pdf", ".docx", ".txt", ".md", ".html", ".htm", ".rtf"],
    "extract_metadata": True,
    "extract_citations": True,
    "extract_figures": False,
    "ocr_enabled": False,
    
    # Search settings
    "search_method": "advanced",  # basic, advanced, semantic
    "case_sensitive": False,
    "whole_words": False,
    "fuzzy_matching": True,
    "relevance_threshold": 0.5,
    
    # Web research
    "enable_web_search": True,
    "search_engine": "duckduckgo",
    "max_web_results": 30,
    "crawl_depth": 2,
    "respect_robots_txt": True,
    
    # AI/Summarization
    "summarization_enabled": True,
    "summary_method": "hybrid",  # textrank, ollama, openai, hybrid
    "summary_model": "gemma2:9b",
    "ollama_url": "http://localhost:11434/api/generate",
    "openai_api_key": "",
    
    # Analysis settings
    "enable_statistics": True,
    "enable_clustering": True,
    "enable_topic_modeling": False,
    "citation_analysis": True,
    
    # Export settings
    "export_formats": ["xlsx", "csv", "json", "markdown", "html", "bibtex"],
    "include_visualizations": True,
    "generate_bibliography": True,
    
    # Performance
    "max_threads": 8,
    "batch_size": 10,
    "cache_enabled": True,
    "memory_limit_mb": 2048,
}

# Database schema
DATABASE_SCHEMA = """
CREATE TABLE IF NOT EXISTS documents (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    filename TEXT NOT NULL,
    filepath TEXT UNIQUE NOT NULL,
    filetype TEXT,
    filesize INTEGER,
    date_added TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    date_modified TIMESTAMP,
    content_hash TEXT,
    full_text TEXT,
    metadata TEXT,
    processed BOOLEAN DEFAULT FALSE,
    error_msg TEXT
);

CREATE TABLE IF NOT EXISTS metadata (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    document_id INTEGER,
    key TEXT,
    value TEXT,
    FOREIGN KEY (document_id) REFERENCES documents(id)
);

CREATE TABLE IF NOT EXISTS paragraphs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    document_id INTEGER,
    page_num INTEGER,
    position INTEGER,
    content TEXT,
    FOREIGN KEY (document_id) REFERENCES documents(id)
);

CREATE TABLE IF NOT EXISTS search_results (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    search_query TEXT,
    document_id INTEGER,
    paragraph_id INTEGER,
    relevance_score REAL,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (document_id) REFERENCES documents(id),
    FOREIGN KEY (paragraph_id) REFERENCES paragraphs(id)
);

CREATE TABLE IF NOT EXISTS citations (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    document_id INTEGER,
    citation_text TEXT,
    citation_type TEXT,
    year INTEGER,
    authors TEXT,
    FOREIGN KEY (document_id) REFERENCES documents(id)
);

CREATE TABLE IF NOT EXISTS summaries (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    document_id INTEGER,
    summary_type TEXT,
    summary_text TEXT,
    method_used TEXT,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (document_id) REFERENCES documents(id)
);

CREATE TABLE IF NOT EXISTS web_sources (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    url TEXT UNIQUE,
    domain TEXT,
    title TEXT,
    content TEXT,
    crawl_depth INTEGER,
    parent_url TEXT,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX IF NOT EXISTS idx_metadata_key ON metadata(key);
CREATE INDEX IF NOT EXISTS idx_paragraphs_doc ON paragraphs(document_id);
CREATE INDEX IF NOT EXISTS idx_search_query ON search_results(search_query);
CREATE INDEX IF NOT EXISTS idx_citations_doc ON citations(document_id);
"""

class DatabaseManager:
    """Manages the research database"""
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.init_database()
        self.migrate_database()
        
    def init_database(self):
        """Initialize the database with schema"""
        with sqlite3.connect(self.db_path) as conn:
            conn.executescript(DATABASE_SCHEMA)
            conn.commit()
    
    def migrate_database(self):
        """Migrate database schema to handle updates"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # Check if date_modified column exists
            cursor.execute("PRAGMA table_info(documents)")
            columns = [col[1] for col in cursor.fetchall()]
            
            if 'date_modified' not in columns:
                logger.info("Migrating database: adding date_modified column")
                try:
                    cursor.execute("ALTER TABLE documents ADD COLUMN date_modified TIMESTAMP")
                    conn.commit()
                except sqlite3.OperationalError:
                    pass  # Column might already exist
            
            # Check other potentially missing columns
            if 'content_hash' not in columns:
                try:
                    cursor.execute("ALTER TABLE documents ADD COLUMN content_hash TEXT")
                    conn.commit()
                except sqlite3.OperationalError:
                    pass
            
            if 'processed' not in columns:
                try:
                    cursor.execute("ALTER TABLE documents ADD COLUMN processed BOOLEAN DEFAULT FALSE")
                    conn.commit()
                except sqlite3.OperationalError:
                    pass
            
            if 'error_msg' not in columns:
                try:
                    cursor.execute("ALTER TABLE documents ADD COLUMN error_msg TEXT")
                    conn.commit()
                except sqlite3.OperationalError:
                    pass
    
    def add_document(self, filepath: str, metadata: dict = None) -> int:
        """Add a document to the database"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # Check if document already exists
            cursor.execute("SELECT id FROM documents WHERE filepath = ?", (filepath,))
            existing = cursor.fetchone()
            if existing:
                return existing[0]
            
            # Get file stats
            try:
                file_stat = os.stat(filepath)
                filesize = file_stat.st_size
                date_modified = datetime.fromtimestamp(file_stat.st_mtime)
            except:
                filesize = 0
                date_modified = datetime.now()
            
            # Insert document
            cursor.execute("""
                INSERT INTO documents (filename, filepath, filetype, filesize, date_modified)
                VALUES (?, ?, ?, ?, ?)
            """, (
                os.path.basename(filepath),
                filepath,
                os.path.splitext(filepath)[1].lower(),
                filesize,
                date_modified
            ))
            
            doc_id = cursor.lastrowid
            
            # Add metadata
            if metadata:
                for key, value in metadata.items():
                    cursor.execute("""
                        INSERT INTO metadata (document_id, key, value)
                        VALUES (?, ?, ?)
                    """, (doc_id, key, str(value)))
            
            conn.commit()
            return doc_id
    
    def update_document_content(self, doc_id: int, full_text: str, content_hash: str):
        """Update document content"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("""
                UPDATE documents 
                SET full_text = ?, content_hash = ?, processed = TRUE
                WHERE id = ?
            """, (full_text, content_hash, doc_id))
            conn.commit()
    
    def add_paragraphs(self, doc_id: int, paragraphs: List[Tuple[int, int, str]]):
        """Add paragraphs for a document"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.executemany("""
                INSERT INTO paragraphs (document_id, page_num, position, content)
                VALUES (?, ?, ?, ?)
            """, [(doc_id, p[0], p[1], p[2]) for p in paragraphs])
            conn.commit()
    
    def search_documents(self, query: str, search_type: str = "full_text") -> List[Dict]:
        """Search documents in database"""
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            
            if search_type == "full_text":
                cursor.execute("""
                    SELECT d.*, COUNT(p.id) as match_count
                    FROM documents d
                    LEFT JOIN paragraphs p ON d.id = p.document_id
                    WHERE d.full_text LIKE ? OR p.content LIKE ?
                    GROUP BY d.id
                    ORDER BY match_count DESC
                """, (f"%{query}%", f"%{query}%"))
            elif search_type == "metadata":
                cursor.execute("""
                    SELECT DISTINCT d.*
                    FROM documents d
                    JOIN metadata m ON d.id = m.document_id
                    WHERE m.value LIKE ?
                """, (f"%{query}%",))
            
            return [dict(row) for row in cursor.fetchall()]
    
    def get_document_metadata(self, doc_id: int) -> Dict:
        """Get all metadata for a document"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT key, value FROM metadata WHERE document_id = ?
            """, (doc_id,))
            return {row[0]: row[1] for row in cursor.fetchall()}
    
    def add_search_result(self, query: str, doc_id: int, para_id: int, score: float):
        """Record a search result"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("""
                INSERT INTO search_results (search_query, document_id, paragraph_id, relevance_score)
                VALUES (?, ?, ?, ?)
            """, (query, doc_id, para_id, score))
            conn.commit()

class DocumentProcessor:
    """Advanced document processing engine"""
    
    def __init__(self, config: dict, db_manager: DatabaseManager):
        self.config = config
        self.db = db_manager
        self.processors = {
            '.pdf': self._process_pdf,
            '.docx': self._process_docx,
            '.txt': self._process_text,
            '.md': self._process_markdown,
            '.html': self._process_html,
            '.htm': self._process_html,
        }
    
    def process_document(self, filepath: str) -> Dict:
        """Process a document and extract all information"""
        logger.info(f"Processing document: {filepath}")
        
        # Get file extension
        ext = os.path.splitext(filepath)[1].lower()
        
        if ext not in self.processors:
            return {"error": f"Unsupported file type: {ext}"}
        
        try:
            # Add to database
            doc_id = self.db.add_document(filepath)
            
            # Process based on type
            result = self.processors[ext](filepath)
            
            # Calculate content hash
            content_hash = hashlib.sha256(result['full_text'].encode()).hexdigest()
            
            # Update database
            self.db.update_document_content(doc_id, result['full_text'], content_hash)
            
            # Add paragraphs
            if 'paragraphs' in result:
                self.db.add_paragraphs(doc_id, result['paragraphs'])
            
            # Extract and store metadata
            if result.get('metadata'):
                for key, value in result['metadata'].items():
                    self.db.add_metadata(doc_id, key, value)
            
            result['doc_id'] = doc_id
            return result
            
        except Exception as e:
            logger.error(f"Error processing {filepath}: {str(e)}")
            return {"error": str(e)}
    
    def _process_pdf(self, filepath: str) -> Dict:
        """Process PDF files with multiple extraction methods"""
        result = {
            'filepath': filepath,
            'filetype': 'pdf',
            'full_text': '',
            'paragraphs': [],
            'metadata': {},
            'pages': 0
        }
        
        # Try PyMuPDF first (most accurate)
        if PYMUPDF_AVAILABLE:
            try:
                doc = fitz.open(filepath)
                result['pages'] = len(doc)
                
                # Extract metadata
                metadata = doc.metadata
                if metadata:
                    result['metadata'] = {
                        'title': metadata.get('title', ''),
                        'author': metadata.get('author', ''),
                        'subject': metadata.get('subject', ''),
                        'keywords': metadata.get('keywords', ''),
                        'creator': metadata.get('creator', ''),
                        'producer': metadata.get('producer', ''),
                        'creation_date': str(metadata.get('creationDate', '')),
                        'modification_date': str(metadata.get('modDate', ''))
                    }
                
                # Extract text and paragraphs
                full_text = []
                for page_num, page in enumerate(doc, 1):
                    page_text = page.get_text()
                    full_text.append(page_text)
                    
                    # Extract paragraphs with positions
                    paragraphs = self._extract_paragraphs(page_text)
                    for para_text, position in paragraphs:
                        result['paragraphs'].append((page_num, position, para_text))
                
                result['full_text'] = '\n\n'.join(full_text)
                doc.close()
                
                # Extract YAML metadata if present
                yaml_meta = self._extract_yaml_metadata(result['full_text'])
                if yaml_meta:
                    result['metadata'].update(yaml_meta)
                
                return result
                
            except Exception as e:
                logger.warning(f"PyMuPDF failed for {filepath}: {str(e)}")
        
        # Fallback to pdfplumber
        if PDFPLUMBER_AVAILABLE:
            try:
                with pdfplumber.open(filepath) as pdf:
                    result['pages'] = len(pdf.pages)
                    
                    # Extract metadata
                    if pdf.metadata:
                        result['metadata'] = {k: str(v) for k, v in pdf.metadata.items()}
                    
                    # Extract text
                    full_text = []
                    for page_num, page in enumerate(pdf.pages, 1):
                        page_text = page.extract_text() or ""
                        full_text.append(page_text)
                        
                        # Extract paragraphs
                        paragraphs = self._extract_paragraphs(page_text)
                        for para_text, position in paragraphs:
                            result['paragraphs'].append((page_num, position, para_text))
                    
                    result['full_text'] = '\n\n'.join(full_text)
                    
                    # Extract YAML metadata
                    yaml_meta = self._extract_yaml_metadata(result['full_text'])
                    if yaml_meta:
                        result['metadata'].update(yaml_meta)
                    
                    return result
                    
            except Exception as e:
                logger.warning(f"pdfplumber failed for {filepath}: {str(e)}")
        
        # Final fallback to PyPDF2
        if PYPDF2_AVAILABLE:
            try:
                with open(filepath, 'rb') as file:
                    reader = PyPDF2.PdfReader(file)
                    result['pages'] = len(reader.pages)
                    
                    # Extract metadata
                    if reader.metadata:
                        result['metadata'] = {
                            'title': getattr(reader.metadata, 'title', ''),
                            'author': getattr(reader.metadata, 'author', ''),
                            'subject': getattr(reader.metadata, 'subject', ''),
                            'creator': getattr(reader.metadata, 'creator', ''),
                        }
                    
                    # Extract text
                    full_text = []
                    for page_num, page in enumerate(reader.pages, 1):
                        page_text = page.extract_text()
                        full_text.append(page_text)
                        
                        # Extract paragraphs
                        paragraphs = self._extract_paragraphs(page_text)
                        for para_text, position in paragraphs:
                            result['paragraphs'].append((page_num, position, para_text))
                    
                    result['full_text'] = '\n\n'.join(full_text)
                    
                    # Extract YAML metadata
                    yaml_meta = self._extract_yaml_metadata(result['full_text'])
                    if yaml_meta:
                        result['metadata'].update(yaml_meta)
                    
                    return result
                    
            except Exception as e:
                logger.error(f"All PDF processors failed for {filepath}: {str(e)}")
                raise
        
        raise Exception("No PDF processing library available")
    
    def _process_docx(self, filepath: str) -> Dict:
        """Process Word documents"""
        result = {
            'filepath': filepath,
            'filetype': 'docx',
            'full_text': '',
            'paragraphs': [],
            'metadata': {},
            'pages': 0
        }
        
        if not DOCX_AVAILABLE:
            raise Exception("python-docx not installed")
        
        try:
            # Extract with python-docx for structure
            doc = DocxDocument(filepath)
            
            # Extract metadata from core properties
            core_props = doc.core_properties
            result['metadata'] = {
                'title': core_props.title or '',
                'author': core_props.author or '',
                'subject': core_props.subject or '',
                'keywords': core_props.keywords or '',
                'created': str(core_props.created) if core_props.created else '',
                'modified': str(core_props.modified) if core_props.modified else '',
                'last_modified_by': core_props.last_modified_by or ''
            }
            
            # Extract paragraphs
            full_text = []
            position = 0
            for para_num, paragraph in enumerate(doc.paragraphs):
                para_text = paragraph.text.strip()
                if para_text:
                    full_text.append(para_text)
                    result['paragraphs'].append((1, position, para_text))
                    position += len(para_text)
            
            # Extract text from tables
            for table in doc.tables:
                for row in table.rows:
                    for cell in row.cells:
                        cell_text = cell.text.strip()
                        if cell_text:
                            full_text.append(cell_text)
            
            result['full_text'] = '\n\n'.join(full_text)
            
            # Extract YAML metadata
            yaml_meta = self._extract_yaml_metadata(result['full_text'])
            if yaml_meta:
                result['metadata'].update(yaml_meta)
            
            return result
            
        except Exception as e:
            # Fallback to docx2txt
            try:
                text = docx2txt.process(filepath)
                result['full_text'] = text
                
                # Extract paragraphs
                paragraphs = self._extract_paragraphs(text)
                for para_text, position in paragraphs:
                    result['paragraphs'].append((1, position, para_text))
                
                # Extract YAML metadata
                yaml_meta = self._extract_yaml_metadata(text)
                if yaml_meta:
                    result['metadata'] = yaml_meta
                
                return result
            except Exception as e2:
                logger.error(f"Error processing DOCX {filepath}: {str(e2)}")
                raise
    
    def _process_text(self, filepath: str) -> Dict:
        """Process plain text files"""
        result = {
            'filepath': filepath,
            'filetype': 'txt',
            'full_text': '',
            'paragraphs': [],
            'metadata': {},
            'pages': 1
        }
        
        try:
            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                text = f.read()
            
            result['full_text'] = text
            
            # Extract paragraphs
            paragraphs = self._extract_paragraphs(text)
            for para_text, position in paragraphs:
                result['paragraphs'].append((1, position, para_text))
            
            # Extract YAML metadata
            yaml_meta = self._extract_yaml_metadata(text)
            if yaml_meta:
                result['metadata'] = yaml_meta
            
            # Try to extract citation-style metadata
            citation_meta = self._extract_citation_metadata(text)
            if citation_meta:
                result['metadata'].update(citation_meta)
            
            return result
            
        except Exception as e:
            logger.error(f"Error processing text file {filepath}: {str(e)}")
            raise
    
    def _process_markdown(self, filepath: str) -> Dict:
        """Process Markdown files"""
        result = {
            'filepath': filepath,
            'filetype': 'md',
            'full_text': '',
            'paragraphs': [],
            'metadata': {},
            'pages': 1
        }
        
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                md_text = f.read()
            
            result['full_text'] = md_text
            
            # Extract YAML frontmatter
            yaml_meta = self._extract_yaml_metadata(md_text)
            if yaml_meta:
                result['metadata'] = yaml_meta
            
            # Convert to HTML for structure analysis
            if MARKDOWN_AVAILABLE:
                html = markdown.markdown(md_text, extensions=['meta', 'tables', 'fenced_code'])
                
                # Extract text from HTML
                if BS4_AVAILABLE:
                    soup = BeautifulSoup(html, 'html.parser')
                    text = soup.get_text()
                    
                    # Extract paragraphs from the text
                    paragraphs = self._extract_paragraphs(text)
                    for para_text, position in paragraphs:
                        result['paragraphs'].append((1, position, para_text))
            else:
                # Fallback: extract paragraphs from raw markdown
                paragraphs = self._extract_paragraphs(md_text)
                for para_text, position in paragraphs:
                    result['paragraphs'].append((1, position, para_text))
            
            return result
            
        except Exception as e:
            logger.error(f"Error processing Markdown file {filepath}: {str(e)}")
            raise
    
    def _process_html(self, filepath: str) -> Dict:
        """Process HTML files"""
        result = {
            'filepath': filepath,
            'filetype': 'html',
            'full_text': '',
            'paragraphs': [],
            'metadata': {},
            'pages': 1
        }
        
        if not BS4_AVAILABLE:
            raise Exception("BeautifulSoup4 not installed")
        
        try:
            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                html = f.read()
            
            soup = BeautifulSoup(html, 'html.parser')
            
            # Extract metadata from meta tags
            meta_tags = soup.find_all('meta')
            for tag in meta_tags:
                name = tag.get('name', tag.get('property', ''))
                content = tag.get('content', '')
                if name and content:
                    result['metadata'][name] = content
            
            # Extract title
            title_tag = soup.find('title')
            if title_tag:
                result['metadata']['title'] = title_tag.get_text(strip=True)
            
            # Remove script and style elements
            for script in soup(["script", "style"]):
                script.decompose()
            
            # Extract text
            text = soup.get_text()
            result['full_text'] = text
            
            # Extract paragraphs
            paragraphs = []
            for p in soup.find_all(['p', 'div', 'section', 'article']):
                para_text = p.get_text(strip=True)
                if len(para_text) > 50:  # Minimum paragraph length
                    paragraphs.append(para_text)
            
            # Add paragraphs with positions
            position = 0
            for para_text in paragraphs:
                result['paragraphs'].append((1, position, para_text))
                position += len(para_text)
            
            return result
            
        except Exception as e:
            logger.error(f"Error processing HTML file {filepath}: {str(e)}")
            raise
    
    def _extract_paragraphs(self, text: str, min_length: int = 50) -> List[Tuple[str, int]]:
        """Extract paragraphs from text with positions"""
        paragraphs = []
        
        # Split by double newlines or paragraph patterns
        splits = re.split(r'\n\s*\n|\r\n\s*\r\n', text)
        
        current_pos = 0
        for para in splits:
            para = para.strip()
            if len(para) >= min_length:
                # Find position in original text
                pos = text.find(para, current_pos)
                if pos != -1:
                    paragraphs.append((para, pos))
                    current_pos = pos + len(para)
        
        # If no paragraphs found with double newlines, try single newlines
        if not paragraphs:
            lines = text.split('\n')
            current_para = []
            current_pos = 0
            
            for line in lines:
                line = line.strip()
                if line:
                    current_para.append(line)
                elif current_para:
                    para_text = ' '.join(current_para)
                    if len(para_text) >= min_length:
                        pos = text.find(para_text, current_pos)
                        if pos != -1:
                            paragraphs.append((para_text, pos))
                            current_pos = pos + len(para_text)
                    current_para = []
            
            # Don't forget the last paragraph
            if current_para:
                para_text = ' '.join(current_para)
                if len(para_text) >= min_length:
                    pos = text.find(para_text, current_pos)
                    if pos != -1:
                        paragraphs.append((para_text, pos))
        
        return paragraphs
    
    def _extract_yaml_metadata(self, text: str) -> Dict:
        """Extract YAML metadata from text"""
        # Look for YAML front matter
        yaml_pattern = re.compile(r'^---\s*\n(.*?)\n---\s*\n', re.DOTALL | re.MULTILINE)
        match = yaml_pattern.search(text)
        
        if match:
            yaml_text = match.group(1)
            try:
                metadata = yaml.safe_load(yaml_text)
                if isinstance(metadata, dict):
                    # Normalize keys to lowercase with underscores
                    normalized = {}
                    for key, value in metadata.items():
                        norm_key = key.lower().replace(' ', '_').replace('-', '_')
                        normalized[norm_key] = value
                    return normalized
            except yaml.YAMLError as e:
                logger.warning(f"Error parsing YAML metadata: {str(e)}")
        
        return {}
    
    def _extract_citation_metadata(self, text: str) -> Dict:
        """Extract citation-style metadata from text"""
        metadata = {}
        
        # Common citation patterns
        patterns = {
            'author': r'(?:Author|Authors?|By):\s*(.+?)(?:\n|$)',
            'title': r'(?:Title):\s*(.+?)(?:\n|$)',
            'year': r'(?:Year|Date):\s*(\d{4})',
            'journal': r'(?:Journal|Published in):\s*(.+?)(?:\n|$)',
            'doi': r'(?:DOI|doi):\s*(.+?)(?:\n|$)',
            'abstract': r'(?:Abstract):\s*(.+?)(?:\n\n|$)',
        }
        
        for key, pattern in patterns.items():
            match = re.search(pattern, text, re.IGNORECASE | re.MULTILINE)
            if match:
                metadata[key] = match.group(1).strip()
        
        return metadata

class AdvancedSearchEngine:
    """Advanced search engine with multiple search methods"""
    
    def __init__(self, config: dict, db_manager: DatabaseManager):
        self.config = config
        self.db = db_manager
        self.search_methods = {
            'basic': self._basic_search,
            'advanced': self._advanced_search,
            'semantic': self._semantic_search
        }
    
    def search(self, query: str, method: str = None, filters: Dict = None) -> List[Dict]:
        """Perform search with specified method"""
        method = method or self.config.get('search_method', 'advanced')
        
        if method not in self.search_methods:
            method = 'advanced'
        
        logger.info(f"Performing {method} search for: {query}")
        
        # Execute search
        results = self.search_methods[method](query, filters)
        
        # Apply post-processing
        results = self._apply_relevance_ranking(results, query)
        
        # Filter by threshold
        threshold = self.config.get('relevance_threshold', 0.5)
        results = [r for r in results if r.get('relevance_score', 0) >= threshold]
        
        # Record search results in database
        for result in results:
            self.db.add_search_result(
                query, 
                result['doc_id'], 
                result.get('paragraph_id'),
                result['relevance_score']
            )
        
        return results
    
    def _basic_search(self, query: str, filters: Dict = None) -> List[Dict]:
        """Basic keyword search"""
        results = []
        
        # Search in database
        db_results = self.db.search_documents(query, 'full_text')
        
        for doc in db_results:
            result = {
                'doc_id': doc['id'],
                'filename': doc['filename'],
                'filepath': doc['filepath'],
                'match_count': doc.get('match_count', 0),
                'relevance_score': 0.5  # Basic score
            }
            results.append(result)
        
        return results
    
    def _advanced_search(self, query: str, filters: Dict = None) -> List[Dict]:
        """Advanced search with query parsing and ranking"""
        results = []
        
        # Parse query for special operators
        parsed_query = self._parse_advanced_query(query)
        
        # Build SQL query based on parsed query
        with sqlite3.connect(self.db.db_path) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            
            # Complex query with relevance scoring
            sql = """
                SELECT 
                    d.id, d.filename, d.filepath,
                    p.id as paragraph_id, p.content as paragraph_content,
                    p.page_num, p.position,
                    COUNT(DISTINCT p.id) as match_count,
                    SUM(
                        CASE 
                            WHEN p.content LIKE ? THEN 3
                            WHEN p.content LIKE ? THEN 2
                            WHEN d.full_text LIKE ? THEN 1
                            ELSE 0
                        END
                    ) as relevance_score
                FROM documents d
                LEFT JOIN paragraphs p ON d.id = p.document_id
                WHERE 1=1
            """
            
            params = []
            
            # Add search conditions
            for term in parsed_query['must_include']:
                sql += " AND (p.content LIKE ? OR d.full_text LIKE ?)"
                params.extend([f"%{term}%", f"%{term}%"])
            
            for term in parsed_query['must_exclude']:
                sql += " AND NOT (p.content LIKE ? OR d.full_text LIKE ?)"
                params.extend([f"%{term}%", f"%{term}%"])
            
            # Add relevance parameters
            params = [f"%{query}%", f"% {query} %", f"%{query}%"] + params
            
            sql += " GROUP BY d.id, p.id ORDER BY relevance_score DESC, match_count DESC"
            
            cursor.execute(sql, params)
            
            for row in cursor.fetchall():
                result = {
                    'doc_id': row['id'],
                    'filename': row['filename'],
                    'filepath': row['filepath'],
                    'paragraph_id': row['paragraph_id'],
                    'paragraph_content': row['paragraph_content'],
                    'page_num': row['page_num'],
                    'position': row['position'],
                    'match_count': row['match_count'],
                    'relevance_score': row['relevance_score'] / 10.0  # Normalize
                }
                results.append(result)
        
        return results
    
    def _semantic_search(self, query: str, filters: Dict = None) -> List[Dict]:
        """Semantic search using embeddings (placeholder for future implementation)"""
        # This would use sentence embeddings for semantic similarity
        # For now, fall back to advanced search
        return self._advanced_search(query, filters)
    
    def _parse_advanced_query(self, query: str) -> Dict:
        """Parse advanced query syntax"""
        parsed = {
            'must_include': [],
            'should_include': [],
            'must_exclude': [],
            'exact_phrases': []
        }
        
        # Extract exact phrases (in quotes)
        exact_pattern = r'"([^"]+)"'
        for match in re.finditer(exact_pattern, query):
            parsed['exact_phrases'].append(match.group(1))
            query = query.replace(match.group(0), '')
        
        # Extract must include (+term)
        must_pattern = r'\+(\S+)'
        for match in re.finditer(must_pattern, query):
            parsed['must_include'].append(match.group(1))
            query = query.replace(match.group(0), '')
        
        # Extract must exclude (-term)
        exclude_pattern = r'-(\S+)'
        for match in re.finditer(exclude_pattern, query):
            parsed['must_exclude'].append(match.group(1))
            query = query.replace(match.group(0), '')
        
        # Remaining terms are should include
        remaining_terms = query.strip().split()
        parsed['should_include'] = [t for t in remaining_terms if t]
        
        # If no special operators, treat all as must include
        if not any([parsed['must_include'], parsed['should_include'], 
                   parsed['exact_phrases'], parsed['must_exclude']]):
            parsed['must_include'] = query.strip().split()
        
        return parsed
    
    def _apply_relevance_ranking(self, results: List[Dict], query: str) -> List[Dict]:
        """Apply advanced relevance ranking to results"""
        query_terms = query.lower().split()
        
        for result in results:
            score = result.get('relevance_score', 0)
            
            # Boost for title matches
            if 'filename' in result:
                filename_lower = result['filename'].lower()
                for term in query_terms:
                    if term in filename_lower:
                        score += 0.3
            
            # Boost for metadata matches
            if 'metadata' in result:
                meta_text = ' '.join(str(v) for v in result['metadata'].values()).lower()
                for term in query_terms:
                    if term in meta_text:
                        score += 0.2
            
            # Boost for exact phrase matches
            if 'paragraph_content' in result:
                para_lower = result['paragraph_content'].lower()
                if query.lower() in para_lower:
                    score += 0.5
            
            result['relevance_score'] = min(score, 1.0)  # Cap at 1.0
        
        # Sort by relevance
        return sorted(results, key=lambda x: x['relevance_score'], reverse=True)

class WebResearchEngine:
    """Web research and crawling engine"""
    
    def __init__(self, config: dict, db_manager: DatabaseManager):
        self.config = config
        self.db = db_manager
        self.session = requests.Session() if REQUESTS_AVAILABLE else None
        self.visited_urls = set()
    
    def search_web(self, query: str, max_results: int = None) -> List[Dict]:
        """Search the web for information"""
        max_results = max_results or self.config.get('max_web_results', 30)
        results = []
        
        if not self.config.get('enable_web_search', True):
            return results
        
        logger.info(f"Searching web for: {query}")
        
        # Use DuckDuckGo HTML API
        try:
            search_url = f"https://html.duckduckgo.com/html/?q={urllib.parse.quote(query)}"
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(search_url, headers=headers, timeout=10)
            response.raise_for_status()
            
            if BS4_AVAILABLE:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Extract search results
                for result_div in soup.find_all('div', class_='result'):
                    title_elem = result_div.find('h2', class_='result__title')
                    snippet_elem = result_div.find('a', class_='result__snippet')
                    
                    if title_elem and snippet_elem:
                        link = title_elem.find('a')
                        if link and link.get('href'):
                            url = link['href']
                            # Extract actual URL from DuckDuckGo redirect
                            if 'uddg=' in url:
                                url = urllib.parse.unquote(url.split('uddg=')[1].split('&')[0])
                            
                            results.append({
                                'url': url,
                                'title': title_elem.get_text(strip=True),
                                'snippet': snippet_elem.get_text(strip=True),
                                'source': 'duckduckgo'
                            })
                            
                            if len(results) >= max_results:
                                break
            
        except Exception as e:
            logger.error(f"Web search error: {str(e)}")
        
        return results
    
    def crawl_url(self, url: str, depth: int = 1) -> Dict:
        """Crawl a URL and extract content"""
        if url in self.visited_urls:
            return {'url': url, 'already_visited': True}
        
        self.visited_urls.add(url)
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (compatible; ResearchBot/1.0)'
            }
            
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            
            # Parse content
            content_type = response.headers.get('content-type', '').lower()
            
            if 'text/html' in content_type and BS4_AVAILABLE:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Extract title
                title = ''
                title_tag = soup.find('title')
                if title_tag:
                    title = title_tag.get_text(strip=True)
                
                # Remove scripts and styles
                for tag in soup(['script', 'style', 'nav', 'footer', 'header']):
                    tag.decompose()
                
                # Extract main content
                main_content = None
                for selector in ['main', 'article', '#content', '.content', 'body']:
                    main_content = soup.find(selector)
                    if main_content:
                        break
                
                if not main_content:
                    main_content = soup.find('body')
                
                # Extract text
                text = main_content.get_text(separator='\n', strip=True) if main_content else ''
                
                # Extract links for further crawling
                links = []
                if depth > 0:
                    for link in soup.find_all('a', href=True):
                        abs_url = urllib.parse.urljoin(url, link['href'])
                        if self._is_valid_url(abs_url) and abs_url not in self.visited_urls:
                            links.append(abs_url)
                
                # Store in database
                with sqlite3.connect(self.db.db_path) as conn:
                    cursor = conn.cursor()
                    cursor.execute("""
                        INSERT OR REPLACE INTO web_sources 
                        (url, domain, title, content, crawl_depth)
                        VALUES (?, ?, ?, ?, ?)
                    """, (
                        url,
                        urllib.parse.urlparse(url).netloc,
                        title,
                        text,
                        depth
                    ))
                    conn.commit()
                
                return {
                    'url': url,
                    'title': title,
                    'content': text,
                    'links': links[:10],  # Limit links
                    'success': True
                }
            
        except Exception as e:
            logger.error(f"Error crawling {url}: {str(e)}")
            return {'url': url, 'error': str(e), 'success': False}
    
    def _is_valid_url(self, url: str) -> bool:
        """Check if URL is valid for crawling"""
        try:
            parsed = urllib.parse.urlparse(url)
            
            # Check scheme
            if parsed.scheme not in ['http', 'https']:
                return False
            
            # Check for common file extensions to skip
            skip_extensions = {'.pdf', '.jpg', '.jpeg', '.png', '.gif', '.zip', '.exe'}
            if any(url.lower().endswith(ext) for ext in skip_extensions):
                return False
            
            return True
            
        except:
            return False

class SummarizationEngine:
    """Advanced summarization engine with multiple methods"""
    
    def __init__(self, config: dict):
        self.config = config
        self.methods = {
            'textrank': self._summarize_textrank,
            'frequency': self._summarize_frequency,
            'ollama': self._summarize_ollama,
            'openai': self._summarize_openai,
            'hybrid': self._summarize_hybrid
        }
    
    def summarize(self, text: str, method: str = None, length: str = 'medium') -> str:
        """Summarize text using specified method"""
        if not text or len(text) < 100:
            return text
        
        method = method or self.config.get('summary_method', 'hybrid')
        
        if method not in self.methods:
            method = 'frequency'
        
        try:
            return self.methods[method](text, length)
        except Exception as e:
            logger.error(f"Summarization error with {method}: {str(e)}")
            # Fallback to frequency-based
            return self._summarize_frequency(text, length)
    
    def _summarize_textrank(self, text: str, length: str) -> str:
        """TextRank summarization"""
        if not NLTK_AVAILABLE:
            return self._summarize_frequency(text, length)
        
        try:
            # Tokenize into sentences
            sentences = sent_tokenize(text)
            
            if len(sentences) < 3:
                return text
            
            # Calculate sentence scores using TextRank algorithm
            # (Simplified version)
            word_freq = Counter()
            for sentence in sentences:
                words = word_tokenize(sentence.lower())
                words = [w for w in words if w.isalnum() and len(w) > 3]
                word_freq.update(words)
            
            # Score sentences
            sentence_scores = {}
            for sentence in sentences:
                words = word_tokenize(sentence.lower())
                words = [w for w in words if w.isalnum() and len(w) > 3]
                
                if len(words) > 0:
                    score = sum(word_freq[w] for w in words) / len(words)
                    sentence_scores[sentence] = score
            
            # Select top sentences
            num_sentences = {
                'short': max(1, len(sentences) // 5),
                'medium': max(2, len(sentences) // 3),
                'long': max(3, len(sentences) // 2)
            }.get(length, len(sentences) // 3)
            
            top_sentences = sorted(sentence_scores.items(), 
                                 key=lambda x: x[1], 
                                 reverse=True)[:num_sentences]
            
            # Reorder by original position
            summary_sentences = []
            for sentence in sentences:
                if any(sentence == s[0] for s in top_sentences):
                    summary_sentences.append(sentence)
            
            return ' '.join(summary_sentences)
            
        except Exception as e:
            logger.error(f"TextRank error: {str(e)}")
            return self._summarize_frequency(text, length)
    
    def _summarize_frequency(self, text: str, length: str) -> str:
        """Simple frequency-based summarization"""
        # Split into sentences
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 20]
        
        if len(sentences) < 3:
            return text
        
        # Calculate word frequencies
        words = re.findall(r'\b\w+\b', text.lower())
        word_freq = Counter(words)
        
        # Remove common words
        common_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 
                       'to', 'for', 'of', 'with', 'by', 'from', 'is', 'was', 'are', 'were'}
        for word in common_words:
            word_freq.pop(word, None)
        
        # Score sentences
        sentence_scores = {}
        for sentence in sentences:
            words = re.findall(r'\b\w+\b', sentence.lower())
            if words:
                score = sum(word_freq.get(w, 0) for w in words) / len(words)
                sentence_scores[sentence] = score
        
        # Select top sentences
        num_sentences = {
            'short': max(1, len(sentences) // 5),
            'medium': max(2, len(sentences) // 3),
            'long': max(3, len(sentences) // 2)
        }.get(length, len(sentences) // 3)
        
        top_sentences = sorted(sentence_scores.items(), 
                             key=lambda x: x[1], 
                             reverse=True)[:num_sentences]
        
        # Return sentences in original order
        summary = []
        for sentence in sentences:
            if any(sentence == s[0] for s in top_sentences):
                summary.append(sentence)
        
        return '. '.join(summary) + '.'
    
    def _summarize_ollama(self, text: str, length: str) -> str:
        """Summarize using Ollama"""
        if not REQUESTS_AVAILABLE:
            return self._summarize_frequency(text, length)
        
        try:
            prompt = f"""Summarize the following text in {length} detail:

{text[:8000]}  # Limit context

Summary:"""
            
            response = requests.post(
                self.config.get('ollama_url'),
                json={
                    'model': self.config.get('summary_model', 'mistral'),
                    'prompt': prompt,
                    'stream': False
                },
                timeout=30
            )
            
            if response.status_code == 200:
                return response.json().get('response', '').strip()
            else:
                raise Exception(f"Ollama API error: {response.status_code}")
                
        except Exception as e:
            logger.error(f"Ollama summarization error: {str(e)}")
            return self._summarize_frequency(text, length)
    
    def _summarize_openai(self, text: str, length: str) -> str:
        """Summarize using OpenAI"""
        api_key = self.config.get('openai_api_key')
        if not api_key or not REQUESTS_AVAILABLE:
            return self._summarize_frequency(text, length)
        
        try:
            headers = {
                'Authorization': f'Bearer {api_key}',
                'Content-Type': 'application/json'
            }
            
            messages = [
                {"role": "system", "content": f"You are a research assistant. Provide a {length} summary."},
                {"role": "user", "content": f"Summarize this text:\n\n{text[:8000]}"}
            ]
            
            response = requests.post(
                'https://api.openai.com/v1/chat/completions',
                headers=headers,
                json={
                    'model': 'gpt-3.5-turbo',
                    'messages': messages,
                    'temperature': 0.3,
                    'max_tokens': 500 if length == 'short' else 1000
                },
                timeout=30
            )
            
            if response.status_code == 200:
                return response.json()['choices'][0]['message']['content'].strip()
            else:
                raise Exception(f"OpenAI API error: {response.status_code}")
                
        except Exception as e:
            logger.error(f"OpenAI summarization error: {str(e)}")
            return self._summarize_frequency(text, length)
    
    def _summarize_hybrid(self, text: str, length: str) -> str:
        """Hybrid summarization combining multiple methods"""
        summaries = []
        
        # Try AI methods first
        if self.config.get('ollama_url'):
            summary = self._summarize_ollama(text, length)
            if summary and len(summary) > 50:
                summaries.append(summary)
        
        if self.config.get('openai_api_key'):
            summary = self._summarize_openai(text, length)
            if summary and len(summary) > 50:
                summaries.append(summary)
        
        # Always include extractive summary
        extractive = self._summarize_textrank(text, length)
        if extractive and len(extractive) > 50:
            summaries.append(extractive)
        
        if not summaries:
            return self._summarize_frequency(text, length)
        
        # Combine summaries intelligently
        if len(summaries) == 1:
            return summaries[0]
        
        # If multiple summaries, combine key points
        combined = "Summary:\n\n"
        for i, summary in enumerate(summaries, 1):
            if len(summaries) > 1:
                combined += f"[Method {i}] "
            combined += summary + "\n\n"
        
        return combined.strip()

class ReportGenerator:
    """Advanced report generation engine"""
    
    def __init__(self, config: dict):
        self.config = config
        self.formats = {
            'markdown': self._generate_markdown,
            'html': self._generate_html,
            'pdf': self._generate_pdf,
            'docx': self._generate_docx,
            'latex': self._generate_latex
        }
    
    def generate_report(self, research_data: Dict, format: str = 'markdown') -> str:
        """Generate comprehensive research report"""
        if format not in self.formats:
            format = 'markdown'
        
        return self.formats[format](research_data)
    
    def _generate_markdown(self, data: Dict) -> str:
        """Generate Markdown report"""
        report = []
        
        # Header
        report.append(f"# {data.get('title', 'Research Report')}")
        report.append(f"\n*Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n")
        
        # Executive Summary
        if data.get('summary'):
            report.append("## Executive Summary\n")
            report.append(data['summary'] + "\n")
        
        # Table of Contents
        report.append("## Table of Contents\n")
        sections = ['Introduction', 'Methodology', 'Literature Review', 
                   'Findings', 'Analysis', 'Conclusions', 'References']
        for i, section in enumerate(sections, 1):
            report.append(f"{i}. [{section}](#{section.lower().replace(' ', '-')})")
        report.append("")
        
        # Introduction
        report.append("## Introduction\n")
        report.append(data.get('introduction', 'This research report presents a comprehensive analysis of the collected literature and findings.'))
        report.append("")
        
        # Methodology
        report.append("## Methodology\n")
        report.append("### Data Collection\n")
        report.append(f"- **Documents Analyzed**: {data.get('doc_count', 0)}")
        report.append(f"- **Search Queries**: {', '.join(data.get('queries', []))}")
        report.append(f"- **Date Range**: {data.get('date_range', 'All available')}")
        report.append("")
        
        # Literature Review
        report.append("## Literature Review\n")
        
        if data.get('documents'):
            # Group by year or type
            docs_by_year = defaultdict(list)
            for doc in data['documents']:
                year = doc.get('metadata', {}).get('year', 'Unknown')
                docs_by_year[year].append(doc)
            
            for year in sorted(docs_by_year.keys(), reverse=True):
                if year != 'Unknown':
                    report.append(f"### {year}\n")
                    for doc in docs_by_year[year]:
                        report.append(f"**{doc.get('title', doc.get('filename', 'Untitled'))}**")
                        if doc.get('authors'):
                            report.append(f"*{doc['authors']}*")
                        if doc.get('summary'):
                            report.append(f"\n{doc['summary']}\n")
                        report.append("")
        
        # Findings
        report.append("## Findings\n")
        
        # Key themes
        if data.get('themes'):
            report.append("### Key Themes\n")
            for theme, count in data['themes'].items():
                report.append(f"- **{theme}**: {count} occurrences")
            report.append("")
        
        # Statistics
        if data.get('statistics'):
            report.append("### Statistical Analysis\n")
            stats = data['statistics']
            report.append(f"- Total documents: {stats.get('total_docs', 0)}")
            report.append(f"- Average document length: {stats.get('avg_length', 0):.0f} words")
            report.append(f"- Total citations found: {stats.get('total_citations', 0)}")
            report.append("")
        
        # Analysis
        report.append("## Analysis\n")
        report.append(data.get('analysis', 'Detailed analysis of the findings...'))
        report.append("")
        
        # Conclusions
        report.append("## Conclusions\n")
        report.append(data.get('conclusions', 'Based on the comprehensive analysis...'))
        report.append("")
        
        # References
        report.append("## References\n")
        if data.get('references'):
            for i, ref in enumerate(data['references'], 1):
                report.append(f"{i}. {ref}")
        report.append("")
        
        # Appendices
        if data.get('appendices'):
            report.append("## Appendices\n")
            for title, content in data['appendices'].items():
                report.append(f"### {title}\n")
                report.append(content)
                report.append("")
        
        return '\n'.join(report)
    
    def _generate_html(self, data: Dict) -> str:
        """Generate HTML report"""
        # Convert markdown to HTML
        md_report = self._generate_markdown(data)
        
        html = f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{data.get('title', 'Research Report')}</title>
    <style>
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background: #f5f5f5;
        }}
        .container {{
            background: white;
            padding: 40px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }}
        h1, h2, h3 {{
            color: #2c3e50;
        }}
        h1 {{
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
        }}
        h2 {{
            margin-top: 30px;
            border-bottom: 1px solid #ecf0f1;
            padding-bottom: 5px;
        }}
        code {{
            background: #f8f9fa;
            padding: 2px 5px;
            border-radius: 3px;
        }}
        blockquote {{
            border-left: 4px solid #3498db;
            padding-left: 20px;
            margin-left: 0;
            color: #666;
        }}
        table {{
            border-collapse: collapse;
            width: 100%;
            margin: 20px 0;
        }}
        th, td {{
            border: 1px solid #ddd;
            padding: 12px;
            text-align: left;
        }}
        th {{
            background: #3498db;
            color: white;
        }}
        tr:nth-child(even) {{
            background: #f8f9fa;
        }}
        .metadata {{
            background: #ecf0f1;
            padding: 15px;
            border-radius: 5px;
            margin: 10px 0;
        }}
        .summary {{
            background: #e8f4f8;
            padding: 20px;
            border-radius: 5px;
            margin: 20px 0;
            border-left: 5px solid #3498db;
        }}
        @media print {{
            body {{
                background: white;
            }}
            .container {{
                box-shadow: none;
            }}
        }}
    </style>
</head>
<body>
    <div class="container">
"""
        
        # Convert Markdown to HTML (simplified)
        if MARKDOWN_AVAILABLE:
            import markdown
            html_content = markdown.markdown(
                md_report, 
                extensions=['tables', 'fenced_code', 'nl2br', 'toc']
            )
        else:
            # Basic conversion
            html_content = md_report.replace('\n', '<br>\n')
            html_content = re.sub(r'# (.*)', r'<h1>\1</h1>', html_content)
            html_content = re.sub(r'## (.*)', r'<h2>\1</h2>', html_content)
            html_content = re.sub(r'### (.*)', r'<h3>\1</h3>', html_content)
            html_content = re.sub(r'\*\*(.*?)\*\*', r'<strong>\1</strong>', html_content)
            html_content = re.sub(r'\*(.*?)\*', r'<em>\1</em>', html_content)
        
        html += html_content
        html += """
    </div>
</body>
</html>"""
        
        return html
    
    def _generate_pdf(self, data: Dict) -> bytes:
        """Generate PDF report (requires additional libraries)"""
        # This would use reportlab or weasyprint
        # For now, generate HTML and note that PDF conversion is needed
        html = self._generate_html(data)
        
        # Note: Actual PDF generation would require:
        # from weasyprint import HTML
        # pdf = HTML(string=html).write_pdf()
        
        return html.encode('utf-8')
    
    def _generate_docx(self, data: Dict) -> bytes:
        """Generate DOCX report"""
        if not DOCX_AVAILABLE:
            raise Exception("python-docx not installed")
        
        doc = DocxDocument()
        
        # Title
        doc.add_heading(data.get('title', 'Research Report'), 0)
        doc.add_paragraph(f"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # Executive Summary
        if data.get('summary'):
            doc.add_heading('Executive Summary', level=1)
            doc.add_paragraph(data['summary'])
        
        # Add sections
        sections = {
            'Introduction': data.get('introduction', ''),
            'Methodology': data.get('methodology', ''),
            'Findings': data.get('findings', ''),
            'Conclusions': data.get('conclusions', '')
        }
        
        for section_title, content in sections.items():
            if content:
                doc.add_heading(section_title, level=1)
                doc.add_paragraph(content)
        
        # Save to bytes
        from io import BytesIO
        doc_bytes = BytesIO()
        doc.save(doc_bytes)
        doc_bytes.seek(0)
        
        return doc_bytes.read()
    
    def _generate_latex(self, data: Dict) -> str:
        """Generate LaTeX report"""
        latex = r"""\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{biblatex}

\title{""" + data.get('title', 'Research Report') + r"""}
\author{Advanced Research System}
\date{\today}

\begin{document}

\maketitle
\tableofcontents
\newpage

"""
        
        # Add sections
        if data.get('summary'):
            latex += r"\section{Executive Summary}" + "\n"
            latex += data['summary'] + "\n\n"
        
        # Introduction
        latex += r"\section{Introduction}" + "\n"
        latex += data.get('introduction', 'Introduction text...') + "\n\n"
        
        # Continue with other sections...
        
        latex += r"\end{document}"
        
        return latex

class AdvancedResearchGUI:
    """Advanced GUI for the research system"""
    
    def __init__(self):
        if not GUI_AVAILABLE:
            raise Exception("Tkinter not available")
        
        self.root = tk.Tk()
        self.root.title("Advanced Research System")
        self.root.geometry("1400x900")
        
        # Initialize components
        self.config = DEFAULT_CONFIG.copy()
        self.db = DatabaseManager(self.config['database_path'])
        self.processor = DocumentProcessor(self.config, self.db)
        self.search_engine = AdvancedSearchEngine(self.config, self.db)
        self.web_engine = WebResearchEngine(self.config, self.db)
        self.summarizer = SummarizationEngine(self.config)
        self.report_gen = ReportGenerator(self.config)
        
        # Current state
        self.current_documents = []
        self.search_results = []
        
        # Setup UI
        self.setup_ui()
        self.apply_theme()
    
    def setup_ui(self):
        """Setup the main UI components"""
        # Create menu bar
        self.create_menu()
        
        # Create main notebook
        self.notebook = ttk.Notebook(self.root)
        self.notebook.pack(fill='both', expand=True, padx=5, pady=5)
        
        # Create tabs
        self.create_library_tab()
        self.create_search_tab()
        self.create_analysis_tab()
        self.create_web_research_tab()
        self.create_report_tab()
        self.create_settings_tab()
        
        # Status bar
        self.status_bar = ttk.Label(self.root, text="Ready", relief=tk.SUNKEN)
        self.status_bar.pack(side='bottom', fill='x')
    
    def create_menu(self):
        """Create application menu"""
        menubar = tk.Menu(self.root)
        self.root.config(menu=menubar)
        
        # File menu
        file_menu = tk.Menu(menubar, tearoff=0)
        menubar.add_cascade(label="File", menu=file_menu)
        file_menu.add_command(label="Import Documents...", command=self.import_documents)
        file_menu.add_command(label="Import Folder...", command=self.import_folder)
        file_menu.add_separator()
        file_menu.add_command(label="Export Results...", command=self.export_results)
        file_menu.add_separator()
        file_menu.add_command(label="Exit", command=self.root.quit)
        
        # Edit menu
        edit_menu = tk.Menu(menubar, tearoff=0)
        menubar.add_cascade(label="Edit", menu=edit_menu)
        edit_menu.add_command(label="Preferences...", command=self.show_preferences)
        
        # Tools menu
        tools_menu = tk.Menu(menubar, tearoff=0)
        menubar.add_cascade(label="Tools", menu=tools_menu)
        tools_menu.add_command(label="Batch Process...", command=self.batch_process)
        tools_menu.add_command(label="Citation Manager...", command=self.citation_manager)
        tools_menu.add_command(label="Web Server...", command=self.start_web_server)
        
        # Help menu
        help_menu = tk.Menu(menubar, tearoff=0)
        menubar.add_cascade(label="Help", menu=help_menu)
        help_menu.add_command(label="Documentation", command=self.show_help)
        help_menu.add_command(label="About", command=self.show_about)
    
    def create_library_tab(self):
        """Create document library tab"""
        library_frame = ttk.Frame(self.notebook)
        self.notebook.add(library_frame, text="Document Library")
        
        # Toolbar
        toolbar = ttk.Frame(library_frame)
        toolbar.pack(fill='x', padx=5, pady=5)
        
        ttk.Button(toolbar, text="Add Documents", 
                  command=self.import_documents).pack(side='left', padx=2)
        ttk.Button(toolbar, text="Add Folder", 
                  command=self.import_folder).pack(side='left', padx=2)
        ttk.Button(toolbar, text="Remove Selected", 
                  command=self.remove_documents).pack(side='left', padx=2)
        ttk.Button(toolbar, text="Refresh", 
                  command=self.refresh_library).pack(side='left', padx=2)
        
        # Search bar
        search_frame = ttk.Frame(library_frame)
        search_frame.pack(fill='x', padx=5, pady=5)
        
        ttk.Label(search_frame, text="Filter:").pack(side='left')
        self.library_filter = ttk.Entry(search_frame, width=30)
        self.library_filter.pack(side='left', padx=5, fill='x', expand=True)
        self.library_filter.bind('<KeyRelease>', self.filter_library)
        
        # Document tree
        tree_frame = ttk.Frame(library_frame)
        tree_frame.pack(fill='both', expand=True, padx=5, pady=5)
        
        # Create treeview with columns
        columns = ('Type', 'Size', 'Modified', 'Status')
        self.doc_tree = ttk.Treeview(tree_frame, columns=columns, show='tree headings')
        
        # Configure columns
        self.doc_tree.column('#0', width=400, minwidth=200)
        self.doc_tree.column('Type', width=100)
        self.doc_tree.column('Size', width=100)
        self.doc_tree.column('Modified', width=150)
        self.doc_tree.column('Status', width=100)
        
        # Headings
        self.doc_tree.heading('#0', text='Document')
        self.doc_tree.heading('Type', text='Type')
        self.doc_tree.heading('Size', text='Size')
        self.doc_tree.heading('Modified', text='Modified')
        self.doc_tree.heading('Status', text='Status')
        
        # Scrollbars
        vsb = ttk.Scrollbar(tree_frame, orient='vertical', command=self.doc_tree.yview)
        hsb = ttk.Scrollbar(tree_frame, orient='horizontal', command=self.doc_tree.xview)
        self.doc_tree.configure(yscrollcommand=vsb.set, xscrollcommand=hsb.set)
        
        # Pack tree and scrollbars
        self.doc_tree.grid(row=0, column=0, sticky='nsew')
        vsb.grid(row=0, column=1, sticky='ns')
        hsb.grid(row=1, column=0, sticky='ew')
        
        tree_frame.grid_rowconfigure(0, weight=1)
        tree_frame.grid_columnconfigure(0, weight=1)
        
        # Document preview
        preview_frame = ttk.LabelFrame(library_frame, text="Preview", height=200)
        preview_frame.pack(fill='x', padx=5, pady=5)
        preview_frame.pack_propagate(False)
        
        self.preview_text = scrolledtext.ScrolledText(preview_frame, height=8, wrap='word')
        self.preview_text.pack(fill='both', expand=True, padx=5, pady=5)
        
        # Bind events
        self.doc_tree.bind('<<TreeviewSelect>>', self.on_document_select)
        self.doc_tree.bind('<Double-1>', self.open_document)
        
        # Load initial library
        self.refresh_library()
    
    def create_search_tab(self):
        """Create search tab"""
        search_frame = ttk.Frame(self.notebook)
        self.notebook.add(search_frame, text="Search")
        
        # Search input area
        input_frame = ttk.LabelFrame(search_frame, text="Search Query", padding=10)
        input_frame.pack(fill='x', padx=10, pady=5)
        
        # Search type
        type_frame = ttk.Frame(input_frame)
        type_frame.pack(fill='x', pady=5)
        
        ttk.Label(type_frame, text="Search Type:").pack(side='left')
        self.search_type = ttk.Combobox(type_frame, values=['Basic', 'Advanced', 'Semantic'], 
                                       state='readonly', width=15)
        self.search_type.set('Advanced')
        self.search_type.pack(side='left', padx=5)
        
        # Search query
        self.search_query = scrolledtext.ScrolledText(input_frame, height=3, wrap='word')
        self.search_query.pack(fill='x', pady=5)
        
        # Search options
        options_frame = ttk.Frame(input_frame)
        options_frame.pack(fill='x')
        
        self.case_sensitive = tk.BooleanVar()
        ttk.Checkbutton(options_frame, text="Case Sensitive", 
                       variable=self.case_sensitive).pack(side='left', padx=5)
        
        self.whole_words = tk.BooleanVar()
        ttk.Checkbutton(options_frame, text="Whole Words", 
                       variable=self.whole_words).pack(side='left', padx=5)
        
        self.search_metadata = tk.BooleanVar(value=True)
        ttk.Checkbutton(options_frame, text="Search Metadata", 
                       variable=self.search_metadata).pack(side='left', padx=5)
        
        # Search button
        ttk.Button(input_frame, text="Search", command=self.perform_search,
                  style='Accent.TButton').pack(pady=5)
        
        # Results area
        results_frame = ttk.LabelFrame(search_frame, text="Search Results", padding=10)
        results_frame.pack(fill='both', expand=True, padx=10, pady=5)
        
        # Results tree
        columns = ('Document', 'Page', 'Score', 'Preview')
        self.results_tree = ttk.Treeview(results_frame, columns=columns, show='headings')
        
        # Configure columns
        self.results_tree.column('Document', width=300)
        self.results_tree.column('Page', width=60)
        self.results_tree.column('Score', width=80)
        self.results_tree.column('Preview', width=400)
        
        # Headings
        for col in columns:
            self.results_tree.heading(col, text=col)
        
        # Scrollbar
        results_scroll = ttk.Scrollbar(results_frame, orient='vertical', 
                                     command=self.results_tree.yview)
        self.results_tree.configure(yscrollcommand=results_scroll.set)
        
        # Pack
        self.results_tree.pack(side='left', fill='both', expand=True)
        results_scroll.pack(side='right', fill='y')
        
        # Bind events
        self.results_tree.bind('<Double-1>', self.open_search_result)
    
    def create_analysis_tab(self):
        """Create analysis tab"""
        analysis_frame = ttk.Frame(self.notebook)
        self.notebook.add(analysis_frame, text="Analysis")
        
        # Analysis options
        options_frame = ttk.LabelFrame(analysis_frame, text="Analysis Options", padding=10)
        options_frame.pack(fill='x', padx=10, pady=5)
        
        # Analysis type selection
        self.analysis_types = {
            'statistics': tk.BooleanVar(value=True),
            'themes': tk.BooleanVar(value=True),
            'citations': tk.BooleanVar(value=True),
            'timeline': tk.BooleanVar(value=False),
            'network': tk.BooleanVar(value=False)
        }
        
        for name, var in self.analysis_types.items():
            ttk.Checkbutton(options_frame, text=name.title(), 
                           variable=var).pack(side='left', padx=5)
        
        # Run analysis button
        ttk.Button(options_frame, text="Run Analysis", 
                  command=self.run_analysis, style='Accent.TButton').pack(pady=5)
        
        # Results notebook
        analysis_notebook = ttk.Notebook(analysis_frame)
        analysis_notebook.pack(fill='both', expand=True, padx=10, pady=5)
        
        # Statistics tab
        stats_frame = ttk.Frame(analysis_notebook)
        analysis_notebook.add(stats_frame, text="Statistics")
        
        self.stats_text = scrolledtext.ScrolledText(stats_frame, wrap='word')
        self.stats_text.pack(fill='both', expand=True, padx=5, pady=5)
        
        # Themes tab
        themes_frame = ttk.Frame(analysis_notebook)
        analysis_notebook.add(themes_frame, text="Themes")
        
        self.themes_tree = ttk.Treeview(themes_frame, columns=('Count', 'Documents'), 
                                       show='tree headings')
        self.themes_tree.column('#0', width=300)
        self.themes_tree.column('Count', width=100)
        self.themes_tree.column('Documents', width=400)
        self.themes_tree.heading('#0', text='Theme')
        self.themes_tree.heading('Count', text='Count')
        self.themes_tree.heading('Documents', text='Documents')
        self.themes_tree.pack(fill='both', expand=True, padx=5, pady=5)
        
        # Citations tab
        citations_frame = ttk.Frame(analysis_notebook)
        analysis_notebook.add(citations_frame, text="Citations")
        
        self.citations_text = scrolledtext.ScrolledText(citations_frame, wrap='word')
        self.citations_text.pack(fill='both', expand=True, padx=5, pady=5)
    
    def create_web_research_tab(self):
        """Create web research tab"""
        web_frame = ttk.Frame(self.notebook)
        self.notebook.add(web_frame, text="Web Research")
        
        # Search area
        search_frame = ttk.LabelFrame(web_frame, text="Web Search", padding=10)
        search_frame.pack(fill='x', padx=10, pady=5)
        
        # Query input
        ttk.Label(search_frame, text="Search Query:").pack(anchor='w')
        self.web_query = ttk.Entry(search_frame, width=50)
        self.web_query.pack(fill='x', pady=5)
        
        # Options
        options_frame = ttk.Frame(search_frame)
        options_frame.pack(fill='x')
        
        ttk.Label(options_frame, text="Max Results:").pack(side='left')
        self.max_results = ttk.Spinbox(options_frame, from_=10, to=100, 
                                      increment=10, width=10)
        self.max_results.set(30)
        self.max_results.pack(side='left', padx=5)
        
        self.enable_crawl = tk.BooleanVar()
        ttk.Checkbutton(options_frame, text="Enable Deep Crawl", 
                       variable=self.enable_crawl).pack(side='left', padx=10)
        
        # Search button
        ttk.Button(search_frame, text="Search Web", 
                  command=self.search_web, style='Accent.TButton').pack(pady=5)
        
        # Results
        results_frame = ttk.LabelFrame(web_frame, text="Web Results", padding=10)
        results_frame.pack(fill='both', expand=True, padx=10, pady=5)
        
        # Results listbox
        self.web_results = tk.Listbox(results_frame, height=10)
        web_scroll = ttk.Scrollbar(results_frame, orient='vertical', 
                                  command=self.web_results.yview)
        self.web_results.configure(yscrollcommand=web_scroll.set)
        
        self.web_results.pack(side='left', fill='both', expand=True)
        web_scroll.pack(side='right', fill='y')
        
        # Preview
        preview_frame = ttk.LabelFrame(web_frame, text="Preview", padding=10)
        preview_frame.pack(fill='both', expand=True, padx=10, pady=5)
        
        self.web_preview = scrolledtext.ScrolledText(preview_frame, wrap='word', height=10)
        self.web_preview.pack(fill='both', expand=True)
        
        # Bind events
        self.web_results.bind('<<ListboxSelect>>', self.preview_web_result)
    
    def create_report_tab(self):
        """Create report generation tab"""
        report_frame = ttk.Frame(self.notebook)
        self.notebook.add(report_frame, text="Report")
        
        # Report options
        options_frame = ttk.LabelFrame(report_frame, text="Report Options", padding=10)
        options_frame.pack(fill='x', padx=10, pady=5)
        
        # Title
        ttk.Label(options_frame, text="Report Title:").grid(row=0, column=0, sticky='w', pady=5)
        self.report_title = ttk.Entry(options_frame, width=50)
        self.report_title.grid(row=0, column=1, pady=5, padx=5)
        self.report_title.insert(0, "Research Report")
        
        # Format
        ttk.Label(options_frame, text="Format:").grid(row=1, column=0, sticky='w', pady=5)
        self.report_format = ttk.Combobox(options_frame, 
                                         values=['Markdown', 'HTML', 'PDF', 'DOCX', 'LaTeX'],
                                         state='readonly', width=20)
        self.report_format.set('Markdown')
        self.report_format.grid(row=1, column=1, sticky='w', pady=5, padx=5)
        
        # Sections to include
        sections_frame = ttk.LabelFrame(options_frame, text="Include Sections", padding=5)
        sections_frame.grid(row=2, column=0, columnspan=2, pady=10, sticky='ew')
        
        self.report_sections = {
            'summary': tk.BooleanVar(value=True),
            'introduction': tk.BooleanVar(value=True),
            'methodology': tk.BooleanVar(value=True),
            'literature_review': tk.BooleanVar(value=True),
            'findings': tk.BooleanVar(value=True),
            'analysis': tk.BooleanVar(value=True),
            'conclusions': tk.BooleanVar(value=True),
            'references': tk.BooleanVar(value=True)
        }
        
        for i, (name, var) in enumerate(self.report_sections.items()):
            ttk.Checkbutton(sections_frame, text=name.replace('_', ' ').title(), 
                           variable=var).grid(row=i//4, column=i%4, padx=5, pady=2, sticky='w')
        
        # Generate button
        ttk.Button(options_frame, text="Generate Report", 
                  command=self.generate_report, style='Accent.TButton').grid(
                      row=3, column=0, columnspan=2, pady=10)
        
        # Preview area
        preview_frame = ttk.LabelFrame(report_frame, text="Report Preview", padding=10)
        preview_frame.pack(fill='both', expand=True, padx=10, pady=5)
        
        self.report_preview = scrolledtext.ScrolledText(preview_frame, wrap='word')
        self.report_preview.pack(fill='both', expand=True)
        
        # Export button
        ttk.Button(preview_frame, text="Export Report", 
                  command=self.export_report).pack(pady=5)
    
    def create_settings_tab(self):
        """Create settings tab"""
        settings_frame = ttk.Frame(self.notebook)
        self.notebook.add(settings_frame, text="Settings")
        
        # Create scrollable frame
        canvas = tk.Canvas(settings_frame)
        scrollbar = ttk.Scrollbar(settings_frame, orient="vertical", command=canvas.yview)
        scrollable_frame = ttk.Frame(canvas)
        
        scrollable_frame.bind(
            "<Configure>",
            lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
        )
        
        canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)
        
        # General settings
        general_frame = ttk.LabelFrame(scrollable_frame, text="General Settings", padding=10)
        general_frame.pack(fill='x', padx=10, pady=5)
        
        # Output directory
        ttk.Label(general_frame, text="Output Directory:").grid(row=0, column=0, sticky='w', pady=5)
        self.output_dir_var = tk.StringVar(value=self.config['output_dir'])
        ttk.Entry(general_frame, textvariable=self.output_dir_var, width=40).grid(
            row=0, column=1, pady=5, padx=5)
        ttk.Button(general_frame, text="Browse...", 
                  command=self.browse_output_dir).grid(row=0, column=2, pady=5)
        
        # Processing settings
        processing_frame = ttk.LabelFrame(scrollable_frame, text="Processing Settings", padding=10)
        processing_frame.pack(fill='x', padx=10, pady=5)
        
        # Max threads
        ttk.Label(processing_frame, text="Max Threads:").grid(row=0, column=0, sticky='w', pady=5)
        self.max_threads_var = tk.IntVar(value=self.config['max_threads'])
        ttk.Spinbox(processing_frame, from_=1, to=16, textvariable=self.max_threads_var, 
                   width=10).grid(row=0, column=1, sticky='w', pady=5, padx=5)
        
        # OCR
        self.ocr_enabled_var = tk.BooleanVar(value=self.config['ocr_enabled'])
        ttk.Checkbutton(processing_frame, text="Enable OCR for scanned PDFs", 
                       variable=self.ocr_enabled_var).grid(row=1, column=0, columnspan=2, 
                                                           sticky='w', pady=5)
        
        # AI settings
        ai_frame = ttk.LabelFrame(scrollable_frame, text="AI/Summarization Settings", padding=10)
        ai_frame.pack(fill='x', padx=10, pady=5)
        
        # Summarization method
        ttk.Label(ai_frame, text="Summarization Method:").grid(row=0, column=0, sticky='w', pady=5)
        self.summary_method_var = tk.StringVar(value=self.config['summary_method'])
        ttk.Combobox(ai_frame, textvariable=self.summary_method_var,
                    values=['textrank', 'frequency', 'ollama', 'openai', 'hybrid'],
                    state='readonly', width=20).grid(row=0, column=1, sticky='w', pady=5, padx=5)
        
        # Ollama URL
        ttk.Label(ai_frame, text="Ollama URL:").grid(row=1, column=0, sticky='w', pady=5)
        self.ollama_url_var = tk.StringVar(value=self.config['ollama_url'])
        ttk.Entry(ai_frame, textvariable=self.ollama_url_var, width=40).grid(
            row=1, column=1, pady=5, padx=5)
        
        # OpenAI API Key
        ttk.Label(ai_frame, text="OpenAI API Key:").grid(row=2, column=0, sticky='w', pady=5)
        self.openai_key_var = tk.StringVar(value=self.config['openai_api_key'])
        ttk.Entry(ai_frame, textvariable=self.openai_key_var, width=40, show='*').grid(
            row=2, column=1, pady=5, padx=5)
        
        # Save settings button
        ttk.Button(scrollable_frame, text="Save Settings", 
                  command=self.save_settings, style='Accent.TButton').pack(pady=10)
        
        # Pack canvas and scrollbar
        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")
    
    def apply_theme(self):
        """Apply modern theme to the application"""
        style = ttk.Style()
        
        # Configure styles
        style.configure('TNotebook', background='#f0f0f0')
        style.configure('TNotebook.Tab', padding=[20, 10], font=('Arial', 10))
        style.configure('TFrame', background='#ffffff')
        style.configure('TLabelframe', background='#ffffff')
        style.configure('TLabelframe.Label', font=('Arial', 10, 'bold'))
        
        # Accent button
        style.configure('Accent.TButton', font=('Arial', 10, 'bold'))
        style.map('Accent.TButton',
                 background=[('active', '#0056b3'), ('!active', '#007bff')],
                 foreground=[('active', 'white'), ('!active', 'white')])
    
    # Event handlers and methods
    def import_documents(self):
        """Import documents into the library"""
        files = filedialog.askopenfilenames(
            title="Select Documents",
            filetypes=[
                ("All Supported", "*.pdf;*.docx;*.txt;*.md;*.html;*.htm"),
                ("PDF Files", "*.pdf"),
                ("Word Documents", "*.docx"),
                ("Text Files", "*.txt"),
                ("Markdown Files", "*.md"),
                ("HTML Files", "*.html;*.htm"),
                ("All Files", "*.*")
            ]
        )
        
        if files:
            self.process_files(files)
    
    def import_folder(self):
        """Import all documents from a folder"""
        folder = filedialog.askdirectory(title="Select Folder")
        if not folder:
            return
        
        # Find all supported files
        files = []
        for ext in self.config['supported_formats']:
            files.extend(Path(folder).rglob(f"*{ext}"))
        
        if files:
            self.process_files([str(f) for f in files])
        else:
            messagebox.showinfo("No Files", "No supported files found in the selected folder")
    
    def process_files(self, files):
        """Process imported files"""
        progress = ttk.Progressbar(self.root, mode='determinate', maximum=len(files))
        progress.place(relx=0.5, rely=0.5, anchor='center')
        
        processed = 0
        errors = []
        
        for file in files:
            try:
                self.status_bar.config(text=f"Processing: {os.path.basename(file)}")
                self.root.update()
                
                result = self.processor.process_document(file)
                
                if result.get('error'):
                    errors.append(f"{os.path.basename(file)}: {result['error']}")
                else:
                    processed += 1
                
                progress['value'] = files.index(file) + 1
                self.root.update()
                
            except Exception as e:
                errors.append(f"{os.path.basename(file)}: {str(e)}")
        
        progress.destroy()
        
        # Show results
        if errors:
            error_msg = "\n".join(errors[:10])  # Show first 10 errors
            if len(errors) > 10:
                error_msg += f"\n... and {len(errors) - 10} more errors"
            messagebox.showwarning("Processing Errors", error_msg)
        
        messagebox.showinfo("Import Complete", 
                          f"Successfully processed {processed} out of {len(files)} files")
        
        self.refresh_library()
        self.status_bar.config(text="Ready")
    
    def refresh_library(self):
        """Refresh the document library display"""
        # Clear tree
        for item in self.doc_tree.get_children():
            self.doc_tree.delete(item)
        
        # Get documents from database
        with sqlite3.connect(self.db.db_path) as conn:
            cursor = conn.cursor()
            
            # First check what columns exist
            cursor.execute("PRAGMA table_info(documents)")
            columns = [col[1] for col in cursor.fetchall()]
            
            # Build query based on available columns
            select_columns = ['id', 'filename', 'filepath', 'filetype']
            
            # Add optional columns if they exist
            if 'filesize' in columns:
                select_columns.append('filesize')
            else:
                select_columns.append('0 as filesize')
                
            if 'date_modified' in columns:
                select_columns.append('date_modified')
            else:
                select_columns.append('NULL as date_modified')
                
            if 'processed' in columns:
                select_columns.append('processed')
            else:
                select_columns.append('1 as processed')
            
            query = f"""
                SELECT {', '.join(select_columns)}
                FROM documents
                ORDER BY filename
            """
            
            cursor.execute(query)
            
            for row in cursor.fetchall():
                doc_id, filename, filepath, filetype = row[:4]
                filesize = row[4] if len(row) > 4 else 0
                date_modified = row[5] if len(row) > 5 else None
                processed = row[6] if len(row) > 6 else True
                
                # Format values
                size_str = self.format_size(filesize) if filesize else "Unknown"
                date_str = date_modified[:10] if date_modified else "Unknown"
                status = "Ready" if processed else "Processing"
                
                # Insert into tree
                self.doc_tree.insert('', 'end', values=(filetype, size_str, date_str, status),
                                   text=filename, tags=(doc_id,))
    
    def format_size(self, bytes):
        """Format file size"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if bytes < 1024.0:
                return f"{bytes:.1f} {unit}"
            bytes /= 1024.0
        return f"{bytes:.1f} TB"
    
    def on_document_select(self, event):
        """Handle document selection"""
        selection = self.doc_tree.selection()
        if not selection:
            return
        
        # Get document ID from tags
        item = self.doc_tree.item(selection[0])
        doc_id = item['tags'][0] if item['tags'] else None
        
        if doc_id:
            # Load preview
            with sqlite3.connect(self.db.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT full_text FROM documents WHERE id = ?", (doc_id,))
                result = cursor.fetchone()
                
                if result and result[0]:
                    # Show first 1000 characters
                    preview = result[0][:1000]
                    if len(result[0]) > 1000:
                        preview += "\n\n... (truncated)"
                    
                    self.preview_text.delete(1.0, tk.END)
                    self.preview_text.insert(1.0, preview)
    
    def open_document(self, event):
        """Open selected document"""
        selection = self.doc_tree.selection()
        if not selection:
            return
        
        # Get filepath
        item = self.doc_tree.item(selection[0])
        doc_id = item['tags'][0] if item['tags'] else None
        
        if doc_id:
            with sqlite3.connect(self.db.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT filepath FROM documents WHERE id = ?", (doc_id,))
                result = cursor.fetchone()
                
                if result:
                    filepath = result[0]
                    if os.path.exists(filepath):
                        try:
                            if sys.platform.startswith('darwin'):
                                subprocess.call(['open', filepath])
                            elif sys.platform.startswith('win'):
                                os.startfile(filepath)
                            else:  # linux
                                subprocess.call(['xdg-open', filepath])
                        except Exception as e:
                            messagebox.showerror("Error", f"Could not open file: {str(e)}")
    
    def remove_documents(self):
        """Remove selected documents from library"""
        selection = self.doc_tree.selection()
        if not selection:
            messagebox.showinfo("No Selection", "Please select documents to remove")
            return
        
        if messagebox.askyesno("Confirm", f"Remove {len(selection)} document(s) from library?"):
            doc_ids = []
            for item in selection:
                tags = self.doc_tree.item(item)['tags']
                if tags:
                    doc_ids.append(tags[0])
            
            # Remove from database
            with sqlite3.connect(self.db.db_path) as conn:
                cursor = conn.cursor()
                for doc_id in doc_ids:
                    cursor.execute("DELETE FROM documents WHERE id = ?", (doc_id,))
                    cursor.execute("DELETE FROM metadata WHERE document_id = ?", (doc_id,))
                    cursor.execute("DELETE FROM paragraphs WHERE document_id = ?", (doc_id,))
                conn.commit()
            
            self.refresh_library()
    
    def filter_library(self, event):
        """Filter document library"""
        filter_text = self.library_filter.get().lower()
        
        # Clear and repopulate tree
        for item in self.doc_tree.get_children():
            self.doc_tree.delete(item)
        
        # Get filtered documents
        with sqlite3.connect(self.db.db_path) as conn:
            cursor = conn.cursor()
            
            # Check available columns
            cursor.execute("PRAGMA table_info(documents)")
            columns = [col[1] for col in cursor.fetchall()]
            
            # Build query with available columns
            select_columns = ['id', 'filename', 'filepath', 'filetype']
            
            if 'filesize' in columns:
                select_columns.append('filesize')
            else:
                select_columns.append('0 as filesize')
                
            if 'date_modified' in columns:
                select_columns.append('date_modified')
            else:
                select_columns.append('NULL as date_modified')
                
            if 'processed' in columns:
                select_columns.append('processed')
            else:
                select_columns.append('1 as processed')
            
            if filter_text:
                query = f"""
                    SELECT {', '.join(select_columns)}
                    FROM documents
                    WHERE LOWER(filename) LIKE ?
                    ORDER BY filename
                """
                cursor.execute(query, (f"%{filter_text}%",))
            else:
                query = f"""
                    SELECT {', '.join(select_columns)}
                    FROM documents
                    ORDER BY filename
                """
                cursor.execute(query)
            
            for row in cursor.fetchall():
                doc_id, filename, filepath, filetype = row[:4]
                filesize = row[4] if len(row) > 4 else 0
                date_modified = row[5] if len(row) > 5 else None
                processed = row[6] if len(row) > 6 else True
                
                size_str = self.format_size(filesize) if filesize else "Unknown"
                date_str = date_modified[:10] if date_modified else "Unknown"
                status = "Ready" if processed else "Processing"
                
                self.doc_tree.insert('', 'end', values=(filetype, size_str, date_str, status),
                                   text=filename, tags=(doc_id,))
    
    def perform_search(self):
        """Perform document search"""
        query = self.search_query.get(1.0, tk.END).strip()
        if not query:
            messagebox.showinfo("No Query", "Please enter a search query")
            return
        
        # Update config
        self.config['case_sensitive'] = self.case_sensitive.get()
        self.config['whole_words'] = self.whole_words.get()
        
        # Get search method
        method = self.search_type.get().lower()
        
        # Perform search
        self.status_bar.config(text="Searching...")
        self.root.update()
        
        try:
            results = self.search_engine.search(query, method)
            
            # Clear previous results
            for item in self.results_tree.get_children():
                self.results_tree.delete(item)
            
            # Display results
            for result in results:
                preview = ""
                if result.get('paragraph_content'):
                    preview = result['paragraph_content'][:200] + "..."
                
                self.results_tree.insert('', 'end', values=(
                    result['filename'],
                    result.get('page_num', 'N/A'),
                    f"{result['relevance_score']:.2f}",
                    preview
                ))
            
            self.status_bar.config(text=f"Found {len(results)} results")
            
        except Exception as e:
            messagebox.showerror("Search Error", str(e))
            self.status_bar.config(text="Search failed")
    
    def open_search_result(self, event):
        """Open selected search result"""
        selection = self.results_tree.selection()
        if selection:
            item = self.results_tree.item(selection[0])
            filename = item['values'][0]
            
            # Find and open the document
            with sqlite3.connect(self.db.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT filepath FROM documents WHERE filename = ?", (filename,))
                result = cursor.fetchone()
                
                if result:
                    filepath = result[0]
                    if os.path.exists(filepath):
                        try:
                            if sys.platform.startswith('darwin'):
                                subprocess.call(['open', filepath])
                            elif sys.platform.startswith('win'):
                                os.startfile(filepath)
                            else:
                                subprocess.call(['xdg-open', filepath])
                        except Exception as e:
                            messagebox.showerror("Error", f"Could not open file: {str(e)}")
    
    def run_analysis(self):
        """Run selected analyses"""
        self.status_bar.config(text="Running analysis...")
        self.root.update()
        
        try:
            # Get all documents
            with sqlite3.connect(self.db.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT COUNT(*) FROM documents WHERE processed = 1")
                doc_count = cursor.fetchone()[0]
            
            if doc_count == 0:
                messagebox.showinfo("No Documents", "No processed documents available for analysis")
                return
            
            # Run statistics
            if self.analysis_types['statistics'].get():
                stats = self.calculate_statistics()
                self.display_statistics(stats)
            
            # Extract themes
            if self.analysis_types['themes'].get():
                themes = self.extract_themes()
                self.display_themes(themes)
            
            # Analyze citations
            if self.analysis_types['citations'].get():
                citations = self.analyze_citations()
                self.display_citations(citations)
            
            self.status_bar.config(text="Analysis complete")
            
        except Exception as e:
            messagebox.showerror("Analysis Error", str(e))
            self.status_bar.config(text="Analysis failed")
    
    def calculate_statistics(self):
        """Calculate document statistics"""
        stats = {}
        
        with sqlite3.connect(self.db.db_path) as conn:
            cursor = conn.cursor()
            
            # Document count
            cursor.execute("SELECT COUNT(*) FROM documents WHERE processed = 1")
            stats['total_documents'] = cursor.fetchone()[0]
            
            # Total size
            cursor.execute("SELECT SUM(filesize) FROM documents")
            total_size = cursor.fetchone()[0] or 0
            stats['total_size'] = self.format_size(total_size)
            
            # Document types
            cursor.execute("""
                SELECT filetype, COUNT(*) 
                FROM documents 
                GROUP BY filetype
            """)
            stats['document_types'] = dict(cursor.fetchall())
            
            # Average document length
            cursor.execute("""
                SELECT AVG(LENGTH(full_text)) 
                FROM documents 
                WHERE full_text IS NOT NULL
            """)
            avg_length = cursor.fetchone()[0] or 0
            stats['avg_document_length'] = int(avg_length)
            
            # Total paragraphs
            cursor.execute("SELECT COUNT(*) FROM paragraphs")
            stats['total_paragraphs'] = cursor.fetchone()[0]
            
            # Metadata statistics
            cursor.execute("""
                SELECT key, COUNT(DISTINCT document_id) 
                FROM metadata 
                GROUP BY key 
                ORDER BY COUNT(DISTINCT document_id) DESC
            """)
            stats['metadata_fields'] = dict(cursor.fetchall())
        
        return stats
    
    def display_statistics(self, stats):
        """Display statistics in the UI"""
        self.stats_text.delete(1.0, tk.END)
        
        text = "DOCUMENT STATISTICS\n" + "="*50 + "\n\n"
        
        text += f"Total Documents: {stats['total_documents']}\n"
        text += f"Total Size: {stats['total_size']}\n"
        text += f"Average Document Length: {stats['avg_document_length']:,} characters\n"
        text += f"Total Paragraphs: {stats['total_paragraphs']:,}\n\n"
        
        text += "Document Types:\n"
        for filetype, count in stats['document_types'].items():
            text += f"  {filetype}: {count}\n"
        
        text += "\nMetadata Fields:\n"
        for field, count in list(stats['metadata_fields'].items())[:10]:
            text += f"  {field}: {count} documents\n"
        
        self.stats_text.insert(1.0, text)
    
    def extract_themes(self):
        """Extract themes from documents"""
        themes = Counter()
        
        # Simple keyword extraction (would be enhanced with NLP)
        with sqlite3.connect(self.db.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT full_text 
                FROM documents 
                WHERE full_text IS NOT NULL
            """)
            
            for row in cursor.fetchall():
                text = row[0].lower()
                
                # Extract keywords (simplified)
                words = re.findall(r'\b\w{4,}\b', text)
                
                # Filter common words
                stopwords = {'this', 'that', 'with', 'from', 'have', 'been', 
                           'were', 'their', 'would', 'could', 'should'}
                
                keywords = [w for w in words if w not in stopwords]
                
                # Count most common
                word_counts = Counter(keywords)
                themes.update(word_counts.most_common(20))
        
        return themes.most_common(50)
    
    def display_themes(self, themes):
        """Display themes in the UI"""
        # Clear tree
        for item in self.themes_tree.get_children():
            self.themes_tree.delete(item)
        
        # Add themes
        for theme, count in themes:
            self.themes_tree.insert('', 'end', text=theme, 
                                   values=(count, "Multiple documents"))
    
    def analyze_citations(self):
        """Analyze citations in documents"""
        citations = []
        
        # Simple citation extraction (would be enhanced with citation parsing)
        with sqlite3.connect(self.db.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT filename, full_text 
                FROM documents 
                WHERE full_text IS NOT NULL
            """)
            
            for filename, text in cursor.fetchall():
                # Look for common citation patterns
                # Year in parentheses
                year_matches = re.findall(r'\((\d{4})\)', text)
                
                # Author (Year) pattern
                author_year = re.findall(r'([A-Z][a-zA-Z]+(?:\s+(?:and|&)\s+[A-Z][a-zA-Z]+)*)\s*\((\d{4})\)', text)
                
                for match in author_year[:10]:  # Limit to first 10
                    citations.append({
                        'document': filename,
                        'authors': match[0],
                        'year': match[1],
                        'type': 'in-text'
                    })
        
        return citations
    
    def display_citations(self, citations):
        """Display citations in the UI"""
        self.citations_text.delete(1.0, tk.END)
        
        text = "CITATION ANALYSIS\n" + "="*50 + "\n\n"
        
        if not citations:
            text += "No citations found.\n"
        else:
            # Group by document
            by_doc = defaultdict(list)
            for cit in citations:
                by_doc[cit['document']].append(cit)
            
            for doc, cits in by_doc.items():
                text += f"\n{doc}:\n"
                for cit in cits:
                    text += f"  - {cit['authors']} ({cit['year']})\n"
        
        self.citations_text.insert(1.0, text)
    
    def search_web(self):
        """Perform web search"""
        query = self.web_query.get().strip()
        if not query:
            messagebox.showinfo("No Query", "Please enter a search query")
            return
        
        self.status_bar.config(text="Searching web...")
        self.root.update()
        
        try:
            # Clear previous results
            self.web_results.delete(0, tk.END)
            self.web_preview.delete(1.0, tk.END)
            
            # Search
            max_results = int(self.max_results.get())
            results = self.web_engine.search_web(query, max_results)
            
            # Display results
            self.web_search_results = results  # Store for preview
            for i, result in enumerate(results):
                display_text = f"{i+1}. {result['title']}"
                self.web_results.insert(tk.END, display_text)
            
            self.status_bar.config(text=f"Found {len(results)} web results")
            
            # Crawl if enabled
            if self.enable_crawl.get() and results:
                self.status_bar.config(text="Crawling web pages...")
                self.root.update()
                
                # Crawl first few results
                for result in results[:5]:
                    self.web_engine.crawl_url(result['url'], depth=1)
                
                self.status_bar.config(text="Web crawl complete")
            
        except Exception as e:
            messagebox.showerror("Web Search Error", str(e))
            self.status_bar.config(text="Web search failed")
    
    def preview_web_result(self, event):
        """Preview selected web result"""
        selection = self.web_results.curselection()
        if not selection:
            return
        
        index = selection[0]
        if index < len(self.web_search_results):
            result = self.web_search_results[index]
            
            preview = f"Title: {result['title']}\n"
            preview += f"URL: {result['url']}\n\n"
            preview += f"Snippet: {result['snippet']}\n"
            
            self.web_preview.delete(1.0, tk.END)
            self.web_preview.insert(1.0, preview)
    
    def generate_report(self):
        """Generate research report"""
        self.status_bar.config(text="Generating report...")
        self.root.update()
        
        try:
            # Gather data for report
            report_data = {
                'title': self.report_title.get(),
                'timestamp': datetime.now(),
                'queries': []  # Would be populated from search history
            }
            
            # Get document data
            with sqlite3.connect(self.db.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT d.id, d.filename, d.filepath, d.full_text
                    FROM documents d
                    WHERE d.processed = 1
                """)
                
                documents = []
                for doc_id, filename, filepath, full_text in cursor.fetchall():
                    # Get metadata
                    metadata = self.db.get_document_metadata(doc_id)
                    
                    # Generate summary
                    summary = ""
                    if self.config.get('summarization_enabled') and full_text:
                        summary = self.summarizer.summarize(full_text[:5000], length='medium')
                    
                    documents.append({
                        'id': doc_id,
                        'filename': filename,
                        'title': metadata.get('title', filename),
                        'authors': metadata.get('author', metadata.get('authors', '')),
                        'year': metadata.get('year', ''),
                        'metadata': metadata,
                        'summary': summary
                    })
                
                report_data['documents'] = documents
                report_data['doc_count'] = len(documents)
            
            # Add sections based on checkboxes
            if self.report_sections['summary'].get():
                report_data['summary'] = self.generate_executive_summary(documents)
            
            if self.report_sections['introduction'].get():
                report_data['introduction'] = "This comprehensive research report presents an analysis of the collected literature..."
            
            if self.report_sections['methodology'].get():
                report_data['methodology'] = "The research methodology involved systematic collection and analysis of documents..."
            
            # Get statistics
            if self.report_sections['analysis'].get():
                stats = self.calculate_statistics()
                report_data['statistics'] = stats
                
                # Get themes
                themes = self.extract_themes()
                report_data['themes'] = dict(themes[:20])
            
            # Generate report
            format_map = {
                'Markdown': 'markdown',
                'HTML': 'html',
                'PDF': 'pdf',
                'DOCX': 'docx',
                'LaTeX': 'latex'
            }
            
            format = format_map.get(self.report_format.get(), 'markdown')
            report_content = self.report_gen.generate_report(report_data, format)
            
            # Display preview
            self.report_preview.delete(1.0, tk.END)
            if format in ['markdown', 'latex']:
                self.report_preview.insert(1.0, report_content)
            elif format == 'html':
                # Show raw HTML for now
                self.report_preview.insert(1.0, report_content[:2000] + "\n\n... (HTML preview truncated)")
            else:
                self.report_preview.insert(1.0, f"Report generated in {format} format. Click 'Export Report' to save.")
            
            self.current_report = report_content
            self.current_report_format = format
            
            self.status_bar.config(text="Report generated successfully")
            
        except Exception as e:
            messagebox.showerror("Report Generation Error", str(e))
            self.status_bar.config(text="Report generation failed")
    
    def generate_executive_summary(self, documents):
        """Generate executive summary for report"""
        if not documents:
            return "No documents available for summary."
        
        # Combine key information
        summary_parts = []
        
        # Document overview
        summary_parts.append(f"This report analyzes {len(documents)} documents")
        
        # Year range
        years = [int(doc['year']) for doc in documents if doc['year'] and doc['year'].isdigit()]
        if years:
            summary_parts.append(f"spanning from {min(years)} to {max(years)}")
        
        # Key topics (simplified)
        all_titles = ' '.join(doc['title'] for doc in documents if doc['title'])
        common_words = Counter(re.findall(r'\b\w{5,}\b', all_titles.lower())).most_common(5)
        if common_words:
            topics = ', '.join(word[0] for word in common_words)
            summary_parts.append(f"Key topics include: {topics}")
        
        return '. '.join(summary_parts) + '.'
    
    def export_report(self):
        """Export the generated report"""
        if not hasattr(self, 'current_report'):
            messagebox.showinfo("No Report", "Please generate a report first")
            return
        
        # Get file extension
        ext_map = {
            'markdown': '.md',
            'html': '.html',
            'pdf': '.pdf',
            'docx': '.docx',
            'latex': '.tex'
        }
        
        ext = ext_map.get(self.current_report_format, '.txt')
        
        # Ask for save location
        filename = filedialog.asksaveasfilename(
            defaultextension=ext,
            filetypes=[
                (f"{self.current_report_format.upper()} files", f"*{ext}"),
                ("All files", "*.*")
            ]
        )
        
        if filename:
            try:
                if self.current_report_format in ['markdown', 'html', 'latex']:
                    with open(filename, 'w', encoding='utf-8') as f:
                        f.write(self.current_report)
                else:
                    # Binary formats
                    with open(filename, 'wb') as f:
                        f.write(self.current_report)
                
                messagebox.showinfo("Export Complete", f"Report saved to:\n{filename}")
                
            except Exception as e:
                messagebox.showerror("Export Error", str(e))
    
    def export_results(self):
        """Export search/analysis results"""
        export_window = tk.Toplevel(self.root)
        export_window.title("Export Results")
        export_window.geometry("400x300")
        
        # Export options
        ttk.Label(export_window, text="Select export format:", 
                 font=('Arial', 10, 'bold')).pack(pady=10)
        
        format_var = tk.StringVar(value="xlsx")
        formats = [
            ("Excel Spreadsheet (.xlsx)", "xlsx"),
            ("CSV File (.csv)", "csv"),
            ("JSON File (.json)", "json"),
            ("BibTeX File (.bib)", "bibtex")
        ]
        
        for text, value in formats:
            ttk.Radiobutton(export_window, text=text, variable=format_var, 
                           value=value).pack(anchor='w', padx=20, pady=5)
        
        # Export button
        def do_export():
            format = format_var.get()
            
            # Get save location
            ext_map = {'xlsx': '.xlsx', 'csv': '.csv', 'json': '.json', 'bibtex': '.bib'}
            filename = filedialog.asksaveasfilename(
                defaultextension=ext_map[format],
                filetypes=[(f"{format.upper()} files", f"*{ext_map[format]}")]
            )
            
            if filename:
                try:
                    self.export_data(filename, format)
                    messagebox.showinfo("Export Complete", f"Data exported to:\n{filename}")
                    export_window.destroy()
                except Exception as e:
                    messagebox.showerror("Export Error", str(e))
        
        ttk.Button(export_window, text="Export", command=do_export,
                  style='Accent.TButton').pack(pady=20)
    
    def export_data(self, filename, format):
        """Export data in specified format"""
        # Get all document data
        with sqlite3.connect(self.db.db_path) as conn:
            if format == 'xlsx':
                # Use pandas for Excel export
                if not PANDAS_AVAILABLE:
                    raise Exception("pandas not installed")
                
                # Documents sheet
                df_docs = pd.read_sql_query("""
                    SELECT filename, filepath, filetype, filesize, 
                           date_added, date_modified, processed
                    FROM documents
                """, conn)
                
                # Metadata sheet
                df_meta = pd.read_sql_query("""
                    SELECT d.filename, m.key, m.value
                    FROM metadata m
                    JOIN documents d ON m.document_id = d.id
                """, conn)
                
                # Write to Excel with multiple sheets
                with pd.ExcelWriter(filename, engine='openpyxl') as writer:
                    df_docs.to_excel(writer, sheet_name='Documents', index=False)
                    df_meta.to_excel(writer, sheet_name='Metadata', index=False)
                    
                    # Add search results if any
                    if hasattr(self, 'search_results') and self.search_results:
                        results_data = []
                        for r in self.search_results:
                            results_data.append({
                                'filename': r.get('filename'),
                                'page': r.get('page_num'),
                                'score': r.get('relevance_score'),
                                'preview': r.get('paragraph_content', '')[:200]
                            })
                        df_results = pd.DataFrame(results_data)
                        df_results.to_excel(writer, sheet_name='Search Results', index=False)
            
            elif format == 'csv':
                # Export documents to CSV
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT filename, filepath, filetype, filesize, 
                           date_added, date_modified, processed
                    FROM documents
                """)
                
                with open(filename, 'w', newline='', encoding='utf-8') as f:
                    writer = csv.writer(f)
                    writer.writerow(['Filename', 'Path', 'Type', 'Size', 
                                   'Date Added', 'Date Modified', 'Processed'])
                    writer.writerows(cursor.fetchall())
            
            elif format == 'json':
                # Export all data as JSON
                data = {
                    'export_date': datetime.now().isoformat(),
                    'documents': []
                }
                
                cursor = conn.cursor()
                cursor.execute("SELECT * FROM documents")
                columns = [desc[0] for desc in cursor.description]
                
                for row in cursor.fetchall():
                    doc = dict(zip(columns, row))
                    doc['metadata'] = self.db.get_document_metadata(doc['id'])
                    data['documents'].append(doc)
                
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(data, f, indent=2, default=str)
            
            elif format == 'bibtex':
                # Export as BibTeX
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT d.id, d.filename 
                    FROM documents d
                """)
                
                with open(filename, 'w', encoding='utf-8') as f:
                    for doc_id, filename in cursor.fetchall():
                        meta = self.db.get_document_metadata(doc_id)
                        
                        # Generate BibTeX entry
                        entry_type = '@article'
                        if 'book' in filename.lower():
                            entry_type = '@book'
                        elif 'thesis' in filename.lower():
                            entry_type = '@phdthesis'
                        
                        cite_key = re.sub(r'[^\w]', '', filename)[:20]
                        
                        f.write(f"{entry_type}{{{cite_key},\n")
                        if meta.get('title'):
                            f.write(f"  title = {{{meta['title']}}},\n")
                        if meta.get('author'):
                            f.write(f"  author = {{{meta['author']}}},\n")
                        if meta.get('year'):
                            f.write(f"  year = {{{meta['year']}}},\n")
                        if meta.get('journal'):
                            f.write(f"  journal = {{{meta['journal']}}},\n")
                        f.write("}\n\n")
    
    def browse_output_dir(self):
        """Browse for output directory"""
        folder = filedialog.askdirectory()
        if folder:
            self.output_dir_var.set(folder)
    
    def save_settings(self):
        """Save application settings"""
        # Update config
        self.config['output_dir'] = self.output_dir_var.get()
        self.config['max_threads'] = self.max_threads_var.get()
        self.config['ocr_enabled'] = self.ocr_enabled_var.get()
        self.config['summary_method'] = self.summary_method_var.get()
        self.config['ollama_url'] = self.ollama_url_var.get()
        self.config['openai_api_key'] = self.openai_key_var.get()
        
        # Save to file
        try:
            with open('config.json', 'w') as f:
                json.dump(self.config, f, indent=2)
            
            messagebox.showinfo("Settings Saved", "Settings have been saved successfully")
        except Exception as e:
            messagebox.showerror("Save Error", f"Could not save settings: {str(e)}")
    
    def show_preferences(self):
        """Show preferences dialog"""
        # Switch to settings tab
        self.notebook.select(5)  # Settings is the 6th tab (0-indexed)
    
    def batch_process(self):
        """Batch process documents"""
        batch_window = tk.Toplevel(self.root)
        batch_window.title("Batch Processing")
        batch_window.geometry("600x400")
        
        ttk.Label(batch_window, text="Batch Document Processing", 
                 font=('Arial', 12, 'bold')).pack(pady=10)
        
        # Options
        options_frame = ttk.Frame(batch_window)
        options_frame.pack(fill='x', padx=20, pady=10)
        
        process_options = {
            'reprocess_all': tk.BooleanVar(value=False),
            'extract_citations': tk.BooleanVar(value=True),
            'generate_summaries': tk.BooleanVar(value=True),
            'update_metadata': tk.BooleanVar(value=True)
        }
        
        for i, (name, var) in enumerate(process_options.items()):
            ttk.Checkbutton(options_frame, text=name.replace('_', ' ').title(), 
                           variable=var).grid(row=i//2, column=i%2, sticky='w', padx=10, pady=5)
        
        # Progress
        progress_frame = ttk.Frame(batch_window)
        progress_frame.pack(fill='x', padx=20, pady=20)
        
        progress_label = ttk.Label(progress_frame, text="Ready to process")
        progress_label.pack()
        
        progress_bar = ttk.Progressbar(progress_frame, mode='determinate')
        progress_bar.pack(fill='x', pady=10)
        
        # Log
        log_frame = ttk.Frame(batch_window)
        log_frame.pack(fill='both', expand=True, padx=20, pady=10)
        
        log_text = scrolledtext.ScrolledText(log_frame, height=10)
        log_text.pack(fill='both', expand=True)
        
        def run_batch():
            # Get documents to process
            with sqlite3.connect(self.db.db_path) as conn:
                cursor = conn.cursor()
                
                if process_options['reprocess_all'].get():
                    cursor.execute("SELECT id, filepath FROM documents")
                else:
                    cursor.execute("SELECT id, filepath FROM documents WHERE processed = 0")
                
                docs = cursor.fetchall()
            
            if not docs:
                log_text.insert(tk.END, "No documents to process.\n")
                return
            
            progress_bar['maximum'] = len(docs)
            
            for i, (doc_id, filepath) in enumerate(docs):
                progress_label.config(text=f"Processing: {os.path.basename(filepath)}")
                log_text.insert(tk.END, f"Processing {filepath}...\n")
                log_text.see(tk.END)
                batch_window.update()
                
                try:
                    # Reprocess document
                    result = self.processor.process_document(filepath)
                    
                    if result.get('error'):
                        log_text.insert(tk.END, f"  Error: {result['error']}\n")
                    else:
                        log_text.insert(tk.END, "  Success\n")
                        
                        # Generate summary if requested
                        if process_options['generate_summaries'].get():
                            if result.get('full_text'):
                                summary = self.summarizer.summarize(
                                    result['full_text'][:5000], 
                                    length='medium'
                                )
                                # Store summary in database
                                with sqlite3.connect(self.db.db_path) as conn:
                                    cursor = conn.cursor()
                                    cursor.execute("""
                                        INSERT INTO summaries 
                                        (document_id, summary_type, summary_text, method_used)
                                        VALUES (?, ?, ?, ?)
                                    """, (doc_id, 'auto', summary, self.config['summary_method']))
                                    conn.commit()
                                
                                log_text.insert(tk.END, "  Generated summary\n")
                    
                except Exception as e:
                    log_text.insert(tk.END, f"  Error: {str(e)}\n")
                
                progress_bar['value'] = i + 1
                batch_window.update()
            
            progress_label.config(text="Batch processing complete")
            log_text.insert(tk.END, "\nBatch processing complete!\n")
            messagebox.showinfo("Complete", "Batch processing finished")
        
        # Buttons
        button_frame = ttk.Frame(batch_window)
        button_frame.pack(pady=10)
        
        ttk.Button(button_frame, text="Start Processing", 
                  command=run_batch, style='Accent.TButton').pack(side='left', padx=5)
        ttk.Button(button_frame, text="Close", 
                  command=batch_window.destroy).pack(side='left', padx=5)
    
    def citation_manager(self):
        """Open citation manager"""
        cite_window = tk.Toplevel(self.root)
        cite_window.title("Citation Manager")
        cite_window.geometry("800x600")
        
        ttk.Label(cite_window, text="Citation Manager", 
                 font=('Arial', 12, 'bold')).pack(pady=10)
        
        # Citation list
        columns = ('Document', 'Citation', 'Type', 'Year')
        cite_tree = ttk.Treeview(cite_window, columns=columns, show='headings')
        
        for col in columns:
            cite_tree.heading(col, text=col)
            cite_tree.column(col, width=150)
        
        cite_tree.pack(fill='both', expand=True, padx=10, pady=10)
        
        # Load citations
        with sqlite3.connect(self.db.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT d.filename, c.citation_text, c.citation_type, c.year
                FROM citations c
                JOIN documents d ON c.document_id = d.id
                ORDER BY c.year DESC
            """)
            
            for row in cursor.fetchall():
                cite_tree.insert('', 'end', values=row)
        
        # Export button
        ttk.Button(cite_window, text="Export Citations", 
                  command=lambda: self.export_citations(cite_window)).pack(pady=10)
    
    def export_citations(self, parent_window):
        """Export citations"""
        filename = filedialog.asksaveasfilename(
            parent=parent_window,
            defaultextension=".bib",
            filetypes=[("BibTeX", "*.bib"), ("Text", "*.txt")]
        )
        
        if filename:
            with sqlite3.connect(self.db.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT citation_text, authors, year
                    FROM citations
                    ORDER BY year DESC
                """)
                
                with open(filename, 'w', encoding='utf-8') as f:
                    for citation, authors, year in cursor.fetchall():
                        f.write(f"{citation}\n")
            
            messagebox.showinfo("Export Complete", f"Citations exported to:\n{filename}")
    
    def start_web_server(self):
        """Start web interface server"""
        if not FLASK_AVAILABLE:
            messagebox.showerror("Flask Not Available", 
                               "Flask is not installed. Install with: pip install flask")
            return
        
        # Create and start Flask app in a thread
        def run_server():
            app = create_flask_app(self.config, self.db)
            app.run(host='127.0.0.1', port=5000, debug=False)
        
        server_thread = threading.Thread(target=run_server, daemon=True)
        server_thread.start()
        
        # Open browser
        time.sleep(1)
        webbrowser.open('http://127.0.0.1:5000')
        
        messagebox.showinfo("Web Server Started", 
                          "Web interface started at http://127.0.0.1:5000")
    
    def show_help(self):
        """Show help documentation"""
        help_window = tk.Toplevel(self.root)
        help_window.title("Help - Advanced Research System")
        help_window.geometry("800x600")
        
        help_text = scrolledtext.ScrolledText(help_window, wrap='word')
        help_text.pack(fill='both', expand=True, padx=10, pady=10)
        
        help_content = """ADVANCED RESEARCH SYSTEM - HELP

TABLE OF CONTENTS
1. Getting Started
2. Document Library
3. Search Functions
4. Analysis Tools
5. Web Research
6. Report Generation
7. Keyboard Shortcuts

1. GETTING STARTED
==================
The Advanced Research System (ARS) is a comprehensive tool for managing, searching, and analyzing research documents.

Quick Start:
- Click "Add Documents" or "Add Folder" to import your research files
- Use the Search tab to find specific content
- Run analyses to extract insights
- Generate professional reports

Supported File Types:
- PDF (.pdf)
- Word Documents (.docx)
- Text Files (.txt)
- Markdown (.md)
- HTML (.html, .htm)

2. DOCUMENT LIBRARY
==================
The Document Library is your central repository for all research materials.

Features:
- Import individual files or entire folders
- Automatic metadata extraction
- Full-text indexing
- Preview documents
- Filter by filename

Tips:
- Double-click a document to open it
- Use the filter box for quick searches
- Right-click for context menu options

3. SEARCH FUNCTIONS
==================
Three search modes are available:

Basic Search:
- Simple keyword matching
- Fast and straightforward

Advanced Search:
- Query operators: +must -exclude "exact phrase"
- Relevance ranking
- Metadata search

Semantic Search:
- Meaning-based matching
- Finds related concepts

Search Tips:
- Use quotes for exact phrases
- + prefix for required terms
- - prefix to exclude terms

4. ANALYSIS TOOLS
=================
Available analyses:

Statistics:
- Document counts
- Average lengths
- Type distribution

Theme Extraction:
- Common topics
- Keyword frequency
- Concept clustering

Citation Analysis:
- Extract citations
- Build bibliography
- Track references

5. WEB RESEARCH
===============
Extend your research with web sources:

Features:
- Web search integration
- Page crawling
- Content extraction
- Source tracking

Usage:
- Enter search terms
- Enable deep crawl for thorough results
- Preview results before saving

6. REPORT GENERATION
===================
Create professional research reports:

Formats:
- Markdown
- HTML
- PDF
- Word (DOCX)
- LaTeX

Sections:
- Executive Summary
- Introduction
- Methodology
- Literature Review
- Findings
- Analysis
- Conclusions
- References

7. KEYBOARD SHORTCUTS
====================
Ctrl+O - Open/Import documents
Ctrl+S - Save current work
Ctrl+F - Focus search box
Ctrl+R - Generate report
Ctrl+E - Export results
F5 - Refresh library
Esc - Close dialogs

For more information, visit the project documentation.
"""
        
        help_text.insert(1.0, help_content)
        help_text.config(state='disabled')
        
        ttk.Button(help_window, text="Close", 
                  command=help_window.destroy).pack(pady=10)
    
    def show_about(self):
        """Show about dialog"""
        about_text = f"""Advanced Research System (ARS)
Version {DEFAULT_CONFIG['version']}

A comprehensive research platform for:
 Literature review management
 Document analysis
 Web research
 Report generation

Created with Python and modern NLP technologies.

Libraries used:
 Document Processing: PyMuPDF, python-docx, pdfplumber
 Analysis: NLTK, pandas, BeautifulSoup4
 Interface: tkinter, Flask
 AI: Ollama, OpenAI (optional)

 2024 Advanced Research System Team
"""
        
        messagebox.showinfo("About ARS", about_text)
    
    def run(self):
        """Start the GUI application"""
        self.root.mainloop()

def create_flask_app(config, db_manager):
    """Create Flask web application"""
    app = Flask(__name__)
    app.config['SECRET_KEY'] = 'research-system-secret-key'
    
    @app.route('/')
    def index():
        return '''
        <!DOCTYPE html>
        <html>
        <head>
            <title>Advanced Research System - Web Interface</title>
            <style>
                body {
                    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
                    max-width: 1200px;
                    margin: 0 auto;
                    padding: 20px;
                    background: #f5f7fa;
                }
                .container {
                    background: white;
                    padding: 30px;
                    border-radius: 10px;
                    box-shadow: 0 2px 10px rgba(0,0,0,0.1);
                }
                h1 {
                    color: #2c3e50;
                    border-bottom: 3px solid #3498db;
                    padding-bottom: 10px;
                }
                .section {
                    margin: 20px 0;
                    padding: 20px;
                    background: #f8f9fa;
                    border-radius: 5px;
                }
                .button {
                    display: inline-block;
                    padding: 10px 20px;
                    background: #3498db;
                    color: white;
                    text-decoration: none;
                    border-radius: 5px;
                    margin: 5px;
                }
                .button:hover {
                    background: #2980b9;
                }
            </style>
        </head>
        <body>
            <div class="container">
                <h1>Advanced Research System - Web Interface</h1>
                
                <div class="section">
                    <h2>Document Library</h2>
                    <p>Access and manage your research documents.</p>
                    <a href="/documents" class="button">View Documents</a>
                    <a href="/upload" class="button">Upload Documents</a>
                </div>
                
                <div class="section">
                    <h2>Search</h2>
                    <p>Search across all your documents.</p>
                    <a href="/search" class="button">Search Documents</a>
                </div>
                
                <div class="section">
                    <h2>Analysis</h2>
                    <p>Analyze your research collection.</p>
                    <a href="/analysis" class="button">Run Analysis</a>
                </div>
                
                <div class="section">
                    <h2>Reports</h2>
                    <p>Generate comprehensive research reports.</p>
                    <a href="/reports" class="button">Create Report</a>
                </div>
            </div>
        </body>
        </html>
        '''
    
    @app.route('/documents')
    def documents():
        # Get documents from database
        with sqlite3.connect(db_manager.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT id, filename, filetype, filesize, date_added
                FROM documents
                ORDER BY date_added DESC
            """)
            docs = cursor.fetchall()
        
        html = '''
        <!DOCTYPE html>
        <html>
        <head>
            <title>Document Library</title>
            <style>
                body { font-family: Arial, sans-serif; margin: 20px; }
                table { border-collapse: collapse; width: 100%; }
                th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
                th { background-color: #4CAF50; color: white; }
                tr:nth-child(even) { background-color: #f2f2f2; }
            </style>
        </head>
        <body>
            <h1>Document Library</h1>
            <table>
                <tr>
                    <th>ID</th>
                    <th>Filename</th>
                    <th>Type</th>
                    <th>Size</th>
                    <th>Date Added</th>
                </tr>
        '''
        
        for doc in docs:
            html += f'''
                <tr>
                    <td>{doc[0]}</td>
                    <td>{doc[1]}</td>
                    <td>{doc[2]}</td>
                    <td>{doc[3]}</td>
                    <td>{doc[4]}</td>
                </tr>
            '''
        
        html += '''
            </table>
            <p><a href="/">Back to Home</a></p>
        </body>
        </html>
        '''
        
        return html
    
    @app.route('/search', methods=['GET', 'POST'])
    def search():
        if request.method == 'POST':
            query = request.form.get('query', '')
            # Perform search using the search engine
            # Return results
            return f"<h1>Search Results for: {query}</h1><p>Search functionality to be implemented</p>"
        
        return '''
        <!DOCTYPE html>
        <html>
        <head>
            <title>Search Documents</title>
        </head>
        <body>
            <h1>Search Documents</h1>
            <form method="post">
                <input type="text" name="query" placeholder="Enter search query" size="50">
                <button type="submit">Search</button>
            </form>
            <p><a href="/">Back to Home</a></p>
        </body>
        </html>
        '''
    
    return app

def main():
    """Main entry point for the application"""
    # Create necessary directories
    for dir_name in ['research_output', 'temp', 'cache']:
        os.makedirs(dir_name, exist_ok=True)
    
    parser = argparse.ArgumentParser(
        description="Advanced Research System - Ultimate Literature Review & Analysis Tool",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Launch GUI
  python ars.py

  # Process documents from command line
  python ars.py --process-folder /path/to/documents --output-dir ./results

  # Run web search
  python ars.py --web-search "machine learning healthcare" --max-results 50

  # Generate report
  python ars.py --generate-report --format pdf --output report.pdf

  # Start web server
  python ars.py --web-server --port 5000
        """
    )
    
    # Mode selection
    parser.add_argument('--gui', action='store_true', default=True,
                       help='Launch GUI interface (default)')
    parser.add_argument('--cli', action='store_true',
                       help='Use command-line interface')
    parser.add_argument('--web-server', action='store_true',
                       help='Start web server interface')
    
    # Processing options
    parser.add_argument('--process-file', type=str,
                       help='Process a single document')
    parser.add_argument('--process-folder', type=str,
                       help='Process all documents in a folder')
    parser.add_argument('--recursive', action='store_true',
                       help='Process folders recursively')
    
    # Search options
    parser.add_argument('--search', type=str,
                       help='Search query')
    parser.add_argument('--search-type', choices=['basic', 'advanced', 'semantic'],
                       default='advanced', help='Search type')
    parser.add_argument('--web-search', type=str,
                       help='Web search query')
    parser.add_argument('--max-results', type=int, default=30,
                       help='Maximum search results')
    
    # Analysis options
    parser.add_argument('--analyze', action='store_true',
                       help='Run analysis on documents')
    parser.add_argument('--extract-themes', action='store_true',
                       help='Extract themes from documents')
    parser.add_argument('--citation-analysis', action='store_true',
                       help='Analyze citations')
    
    # Report options
    parser.add_argument('--generate-report', action='store_true',
                       help='Generate research report')
    parser.add_argument('--report-title', type=str, default='Research Report',
                       help='Report title')
    parser.add_argument('--format', choices=['markdown', 'html', 'pdf', 'docx', 'latex'],
                       default='markdown', help='Output format')
    
    # Output options
    parser.add_argument('--output', type=str,
                       help='Output file path')
    parser.add_argument('--output-dir', type=str, default='research_output',
                       help='Output directory')
    
    # Configuration
    parser.add_argument('--config', type=str,
                       help='Configuration file path')
    parser.add_argument('--db-path', type=str, default='research_database.db',
                       help='Database file path')
    
    # Server options
    parser.add_argument('--port', type=int, default=5000,
                       help='Web server port')
    parser.add_argument('--host', type=str, default='127.0.0.1',
                       help='Web server host')
    
    args = parser.parse_args()
    
    # Load configuration
    config = DEFAULT_CONFIG.copy()
    if args.config and os.path.exists(args.config):
        with open(args.config, 'r') as f:
            config.update(json.load(f))
    
    # Update config with command line arguments
    config['database_path'] = args.db_path
    config['output_dir'] = args.output_dir
    
    # Create output directory
    os.makedirs(config['output_dir'], exist_ok=True)
    
    # Initialize database
    db_manager = DatabaseManager(config['database_path'])
    
    # Check if any CLI operations requested
    cli_mode = any([
        args.cli, args.process_file, args.process_folder,
        args.search, args.web_search, args.analyze,
        args.generate_report, args.web_server
    ])
    
    if args.web_server:
        # Start web server
        if not FLASK_AVAILABLE:
            print("Error: Flask not installed. Install with: pip install flask")
            sys.exit(1)
        
        app = create_flask_app(config, db_manager)
        print(f"Starting web server at http://{args.host}:{args.port}")
        app.run(host=args.host, port=args.port, debug=False)
    
    elif cli_mode and not args.gui:
        # CLI mode
        print("Advanced Research System - CLI Mode")
        print("="*50)
        
        # Initialize components
        processor = DocumentProcessor(config, db_manager)
        search_engine = AdvancedSearchEngine(config, db_manager)
        web_engine = WebResearchEngine(config, db_manager)
        summarizer = SummarizationEngine(config)
        report_gen = ReportGenerator(config)
        
        # Process files
        if args.process_file:
            print(f"Processing file: {args.process_file}")
            result = processor.process_document(args.process_file)
            if result.get('error'):
                print(f"Error: {result['error']}")
            else:
                print(f"Successfully processed: {args.process_file}")
        
        elif args.process_folder:
            print(f"Processing folder: {args.process_folder}")
            
            # Find all supported files
            files = []
            if args.recursive:
                for ext in config['supported_formats']:
                    files.extend(Path(args.process_folder).rglob(f"*{ext}"))
            else:
                for ext in config['supported_formats']:
                    files.extend(Path(args.process_folder).glob(f"*{ext}"))
            
            print(f"Found {len(files)} files to process")
            
            # Process each file
            for i, file in enumerate(files, 1):
                print(f"Processing ({i}/{len(files)}): {file.name}")
                try:
                    result = processor.process_document(str(file))
                    if result.get('error'):
                        print(f"  Error: {result['error']}")
                except Exception as e:
                    print(f"  Error: {str(e)}")
            
            print("Processing complete")
        
        # Search
        if args.search:
            print(f"Searching for: {args.search}")
            results = search_engine.search(args.search, args.search_type)
            
            print(f"Found {len(results)} results:")
            for i, result in enumerate(results[:10], 1):
                print(f"\n{i}. {result['filename']} (Score: {result['relevance_score']:.2f})")
                if result.get('paragraph_content'):
                    preview = result['paragraph_content'][:200] + "..."
                    print(f"   {preview}")
        
        # Web search
        if args.web_search:
            print(f"Searching web for: {args.web_search}")
            results = web_engine.search_web(args.web_search, args.max_results)
            
            print(f"Found {len(results)} web results:")
            for i, result in enumerate(results[:10], 1):
                print(f"\n{i}. {result['title']}")
                print(f"   URL: {result['url']}")
                print(f"   {result['snippet']}")
        
        # Analysis
        if args.analyze or args.extract_themes or args.citation_analysis:
            print("Running analysis...")
            
            # Get document count
            with sqlite3.connect(db_manager.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT COUNT(*) FROM documents WHERE processed = 1")
                doc_count = cursor.fetchone()[0]
            
            print(f"Analyzing {doc_count} documents")
            
            # Run requested analyses
            # (Implementation would go here)
            print("Analysis complete")
        
        # Generate report
        if args.generate_report:
            print("Generating report...")
            
            # Gather report data
            report_data = {
                'title': args.report_title,
                'documents': []
                # (Additional data gathering would go here)
            }
            
            # Generate report
            report_content = report_gen.generate_report(report_data, args.format)
            
            # Save report
            output_file = args.output or f"report.{args.format}"
            
            if args.format in ['markdown', 'html', 'latex']:
                with open(output_file, 'w', encoding='utf-8') as f:
                    f.write(report_content)
            else:
                with open(output_file, 'wb') as f:
                    f.write(report_content)
            
            print(f"Report saved to: {output_file}")
    
    else:
        # GUI mode (default)
        if not GUI_AVAILABLE:
            print("Error: Tkinter not available. Use --cli flag for command-line mode.")
            sys.exit(1)
        
        print("Launching Advanced Research System GUI...")
        app = AdvancedResearchGUI()
        app.run()

if __name__ == "__main__":
    main()
========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/proton.py
========================================

#!/usr/bin/env python3
import argparse
import subprocess
import sys
import logging
import shutil
import traceback
import json
import os
import re
import time
import textwrap
from collections import OrderedDict

__version__ = "1.8"

# Path to your virtual environment activation script and Photon script
VENV_PATH = os.path.expanduser("~/newvenv/bin/activate")
PHOTON_SCRIPT = os.path.join(os.path.dirname(os.path.realpath(__file__)), "photon.py")

def check_command(command):
    """Verify if a command is available in the system PATH."""
    if shutil.which(command) is None:
        logging.error(f"'{command}' is not installed. Please install it to proceed.")
        sys.exit(1)
    logging.debug(f"'{command}' is available.")

def pretty_print_json(output):
    """Attempt to parse and pretty-print JSON output; if it fails, return raw output."""
    try:
        data = json.loads(output)
        return json.dumps(data, indent=2)
    except json.JSONDecodeError:
        return output

def run_command(command, label, debug=False, use_venv=False, ignore_errors=False):
    """
    Execute a command, optionally within a virtual environment.
    Returns the command output as a string.
    """
    try:
        if use_venv:
            # For commands like Photon that must run inside your virtualenv,
            # we assume python3 is available.
            check_command("python3")
        else:
            check_command(command[0])
        logging.debug(f"Executing command: {' '.join(command)}")
        
        if use_venv:
            # Combine the activation command with the target command
            full_command = f"source {VENV_PATH} && " + " ".join(command)
            result = subprocess.run(full_command, shell=True, capture_output=True, text=True, executable="/bin/bash")
        else:
            result = subprocess.run(command, capture_output=True, text=True)

        if result.returncode != 0:
            logging.error(f"Error running {label}: {result.stderr.strip()}")
            if debug:
                logging.debug("Full stderr: " + result.stderr)
            if ignore_errors:
                # Return empty result but don't fail completely
                return ""
            return None
        logging.debug(f"{label} output: {result.stdout.strip()}")
        return result.stdout.strip()
    except Exception as exc:
        logging.error(f"Exception running {label}: {exc}")
        if debug:
            traceback.print_exc()
        return None

def calculate_resource_values(depth):
    """
    Calculate appropriate resource values based on the depth parameter (1-100).
    Returns a dictionary with calibrated values for different tools.
    
    Depth scale:
    1-20: Light/Quick search
    21-50: Medium search
    51-80: Thorough search
    81-100: Deep/Extensive search
    """
    # Ensure depth is within valid range (now 1-100)
    depth = max(1, min(100, depth))
    
    # Calculate ddgr results - maintain a reasonable page size
    # We'll use constant page size of 10 for better UX, but adjust total results
    ddgr_page_size = 10
    
    # Calculate ddgr total results based on depth
    if depth <= 20:
        ddgr_results = 10  # Light search
    elif depth <= 50:
        ddgr_results = 15  # Medium search
    elif depth <= 80:
        ddgr_results = 20  # Thorough search
    else:
        ddgr_results = 25  # Deep search (max for ddgr)
    
    # Calculate Photon crawl parameters
    # Level: 1 to 5 (Photon's internal crawl depth)
    photon_level = 1 + int((depth - 1) / 20)  # Ranges from 1-5
    
    # Threads: 2 to 50 (threading for parallel processing)
    photon_threads = 2 + int((depth - 1) * 0.48)  # Linear scaling
    
    # Timeout: 5s to 30s (longer for deeper crawls)
    photon_timeout = 5 + int((depth - 1) * 0.25)  # Linear scaling
    
    # URLs to crawl per page: proportional to depth
    if depth <= 20:
        urls_percent = 0.3  # Crawl 30% of results for light search
    elif depth <= 50:
        urls_percent = 0.5  # Crawl 50% of results for medium search
    elif depth <= 80:
        urls_percent = 0.7  # Crawl 70% of results for thorough search
    else:
        urls_percent = 1.0  # Crawl 100% of results for deep search
    
    urls_to_crawl = max(1, min(ddgr_page_size, round(ddgr_page_size * urls_percent)))
    
    return {
        "ddgr_results": ddgr_results,
        "ddgr_page_size": ddgr_page_size,
        "photon_threads": photon_threads,
        "photon_level": photon_level,
        "photon_timeout": photon_timeout,
        "urls_to_crawl": urls_to_crawl
    }

def build_ddgr_command(query, ddgr_args, page_size=10):
    """Build the ddgr command with all user-supplied arguments."""
    command = ['ddgr', '--json']
    
    # Ensure we're using a proper page size for ddgr
    command.extend(['--num', str(page_size)])
    
    # Add all ddgr-specific arguments
    if ddgr_args.get('region'):
        command.extend(['--reg', ddgr_args.get('region')])
    if ddgr_args.get('colorize'):
        command.extend(['--colorize', ddgr_args.get('colorize')])
    if ddgr_args.get('nocolor'):
        command.append('--nocolor')
    if ddgr_args.get('colors'):
        command.extend(['--colors', ddgr_args.get('colors')])
    if ddgr_args.get('time'):
        command.extend(['--time', ddgr_args.get('time')])
    if ddgr_args.get('site'):
        for site in ddgr_args.get('site'):
            command.extend(['--site', site])
    if ddgr_args.get('expand'):
        command.append('--expand')
    if ddgr_args.get('proxy'):
        command.extend(['--proxy', ddgr_args.get('proxy')])
    if ddgr_args.get('unsafe'):
        command.append('--unsafe')
    if ddgr_args.get('noua'):
        command.append('--noua')
    
    # Add the query
    command.append(query)
    
    return command

def run_ddgr_with_pagination(query, ddgr_args, depth=30, debug=False, page=1):
    """
    Run DuckDuckGo search using ddgr with proper pagination support.
    This function uses ddgr's interactive mode and simulates pagination commands.
    
    Parameters:
    - query: The search query string
    - ddgr_args: Dictionary of ddgr-specific arguments
    - depth: Depth parameter (1-100)
    - debug: Whether to enable debug output
    - page: Page number (1-based for user display)
    
    Returns:
    - List of URLs from search results
    - Boolean indicating if there might be more results
    """
    resources = calculate_resource_values(depth)
    page_size = resources["ddgr_page_size"]
    
    # For first page, search normally
    if page == 1:
        print(f"\n[+] DuckDuckGo (ddgr) search results for: {query}")
        print(f"[+] Search depth: {depth}/100 (Page {page})\n")
        command = build_ddgr_command(query, ddgr_args, page_size)
    else:
        print(f"\n[+] DuckDuckGo (ddgr) search results - Page {page} for: {query}")
        print(f"[+] Search depth: {depth}/100\n")
        
        # For subsequent pages, we need to simulate pagination:
        # 1. Run ddgr in non-interactive mode first to get initial results
        # 2. Then run multiple "next page" commands to get to the desired page
        
        # First, run the initial query with more results to move through pages faster
        # Use the max limit of 25 to reduce the number of pagination steps needed
        initial_results = 25
        command = ['ddgr', '--json', '--num', str(initial_results), query]
        # Add other args
        if ddgr_args.get('region'):
            command.extend(['--reg', ddgr_args.get('region')])
        if ddgr_args.get('time'):
            command.extend(['--time', ddgr_args.get('time')])
        if ddgr_args.get('site'):
            for site in ddgr_args.get('site'):
                command.extend(['--site', site])
        if ddgr_args.get('unsafe'):
            command.append('--unsafe')
    
    output = run_command(command, "ddgr", debug)
    if not output:
        print("[-] No results from ddgr search")
        return [], False
    
    urls = []
    try:
        results = json.loads(output)
        
        # If we're on a later page, we need to extract the correct subset of results
        result_offset = 0
        if page > 1:
            # For page 2, we want results 10-19 (assuming page_size=10)
            # For page 3, we want results 20-29, etc.
            result_offset = (page - 1) * page_size
            
            # If offset is beyond available results, no more results
            if result_offset >= len(results):
                print("[-] No more results available.")
                return [], False
            
            # Get the slice of results for this page
            end_offset = min(result_offset + page_size, len(results))
            page_results = results[result_offset:end_offset]
        else:
            # First page, just take the first page_size results
            page_results = results[:page_size]
        
        # Process and display the results
        for i, result in enumerate(page_results, 1):
            title = result.get("title", "No Title")
            url = result.get("url", "No URL")
            abstract = result.get("abstract", "")
            
            # Add URLs to the list for Photon crawling
            urls.append(url)
            
            # Display result with correct global index
            global_index = result_offset + i
            print(f"{global_index}. {title}")
            print(f"   {url}")
            if abstract:
                wrapped_abstract = textwrap.fill(abstract, width=80, initial_indent="   ", subsequent_indent="   ")
                print(f"{wrapped_abstract}\n")
            else:
                print()  # Empty line for spacing
        
        # Determine if there might be more results
        # We consider there are more if:
        # 1. We got a full page of results, or
        # 2. We know there are more results in our fetched batch
        has_more = (len(page_results) == page_size) or (result_offset + len(page_results) < len(results))
        
        return urls, has_more
    
    except json.JSONDecodeError:
        logging.error("Failed to parse ddgr JSON output.")
        print(output)
        return [], False

def build_photon_command(target, photon_args, output_dir):
    """Build the Photon command with all user-supplied arguments."""
    command = [
        'python3', PHOTON_SCRIPT,
        '-u', target,
        '-o', output_dir
    ]
    
    # Add all photon-specific arguments
    if photon_args.get('level') is not None:
        command.extend(['-l', str(photon_args.get('level'))])
    if photon_args.get('threads') is not None:
        command.extend(['-t', str(photon_args.get('threads'))])
    if photon_args.get('delay') is not None:
        command.extend(['-d', str(photon_args.get('delay'))])
    if photon_args.get('timeout') is not None:
        command.extend(['--timeout', str(photon_args.get('timeout'))])
    if photon_args.get('cookie'):
        command.extend(['-c', photon_args.get('cookie')])
    if photon_args.get('regex'):
        command.extend(['-r', photon_args.get('regex')])
    if photon_args.get('export'):
        command.extend(['-e', photon_args.get('export')])
    if photon_args.get('seeds'):
        command.extend(['-s'] + photon_args.get('seeds'))
    if photon_args.get('user_agent'):
        command.extend(['--user-agent', photon_args.get('user_agent')])
    if photon_args.get('exclude'):
        command.extend(['--exclude', photon_args.get('exclude')])
    if photon_args.get('proxy'):
        command.extend(['-p', photon_args.get('proxy')])
    
    # Add boolean flags
    if photon_args.get('verbose'):
        command.append('-v')
    if photon_args.get('headers'):
        command.append('--headers')
    if photon_args.get('dns'):
        command.append('--dns')
    if photon_args.get('keys'):
        command.append('--keys')
    if photon_args.get('only_urls'):
        command.append('--only-urls')
    if photon_args.get('wayback'):
        command.append('--wayback')
    
    return command

def run_photon_on_single_target(target, photon_args, depth=30, debug=False, index=None, total=None):
    """Run Photon OSINT on a single target URL with user-specified arguments."""
    resources = calculate_resource_values(depth)
    
    # Use calculated resources if not provided in photon_args
    level = photon_args.get('level') or resources["photon_level"]
    threads = photon_args.get('threads') or resources["photon_threads"]
    timeout = photon_args.get('timeout') or resources["photon_timeout"]
    
    # Create a progress indicator if we're processing multiple URLs
    progress_str = ""
    if index is not None and total is not None:
        progress_str = f"[{index}/{total}] "
    
    print(f"\n[+] {progress_str}Photon crawling target: {target}")
    print(f"[+] Crawl depth: {depth}/100 (level: {level}, threads: {threads}, timeout: {timeout}s)")
    
    if not os.path.isfile(PHOTON_SCRIPT):
        logging.error(f"Photon script not found at {PHOTON_SCRIPT}. Please ensure photon.py is available.")
        return None
    
    # Create organized output folder structure
    main_output_dir = "photon_results"
    if not os.path.exists(main_output_dir):
        os.makedirs(main_output_dir)
    
    # Get domain name for subfolder
    domain = target.replace("https://", "").replace("http://", "").split("/")[0]
    domain_safe = re.sub(r'[^\w\-_]', '_', domain)  # Make domain name safe for filesystem
    
    # Create a unique subfolder for this target
    timestamp = int(time.time())
    target_dir = f"{domain_safe}_{timestamp}"
    output_dir = os.path.join(main_output_dir, target_dir)
    
    # Build the Photon command
    command = build_photon_command(target, photon_args, output_dir)
    
    # Run the command with the proper settings and handle failures gracefully
    output = run_command(command, f"Photon OSINT on {target}", debug, use_venv=True, ignore_errors=True)
    
    # Create the output directory even if the command failed
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        
    if output is not None:
        print(f"[+] Completed scanning {target}")
        print(f"[+] Results saved to {output_dir}/")
        return output_dir
    
    # If the output is None, it means the command completely failed
    print(f"[!] Issues encountered while scanning {target}, but continuing...")
    print(f"[+] Partial results may be available in {output_dir}/")
    return output_dir

def run_photon_on_multiple_targets(targets, photon_args, depth=30, debug=False):
    """Run Photon OSINT on multiple target URLs with user-specified arguments."""
    if not targets:
        print("[-] No targets to crawl with Photon.")
        return
    
    resources = calculate_resource_values(depth)
    max_targets = photon_args.get('max_targets') or resources["urls_to_crawl"]
    
    # Deduplicate targets while preserving order
    unique_targets = list(OrderedDict.fromkeys(targets))
    
    if len(unique_targets) > max_targets:
        print(f"[*] Limiting Photon crawl to top {max_targets} targets based on depth setting {depth}/100")
        targets_to_crawl = unique_targets[:max_targets]
    else:
        targets_to_crawl = unique_targets
    
    print(f"\n[+] Starting Photon crawler on {len(targets_to_crawl)} targets")
    
    results = []
    for i, target in enumerate(targets_to_crawl, 1):
        result_dir = run_photon_on_single_target(
            target, photon_args, depth, debug, i, len(targets_to_crawl)
        )
        if result_dir:
            results.append((target, result_dir))
    
    if results:
        print("\n[+] Photon crawling complete. Summary:")
        print(f"[+] All results saved to the 'photon_results/' directory")
        for target, output_dir in results:
            print(f"  - {target} -> {os.path.basename(output_dir)}/")
    else:
        print("\n[-] No successful Photon crawls.")

def ask_for_more():
    """Ask the user if they want more results."""
    while True:
        answer = input("\nMore results? (y/n): ").lower().strip()
        if answer in ['y', 'yes']:
            return True
        elif answer in ['n', 'no']:
            return False
        else:
            print("Please answer 'y' or 'n'.")

def is_url(text):
    """Check if the given text is a URL."""
    url_pattern = re.compile(r'^https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+')
    return bool(url_pattern.match(text))

def main():
    # Create argument parser with extensive options for both ddgr and photon
    parser = argparse.ArgumentParser(
        description="Enhanced OSINT tool using Photon OSINT and ddgr with pagination support.",
        epilog=("Examples:\n"
                "  python3 proton.py \"search keywords\" 25                          # Search and crawl results with medium depth\n"
                "  python3 proton.py \"search keywords\" \"https://example.com\" 45    # Search, crawl specific site and search results\n"
                "  python3 proton.py --query 'osint tools' --depth 75 --no-crawl   # Search only, no crawling\n"
                "  python3 proton.py --target 'https://example.com' --depth 100     # Crawl only a specific site deeply\n"
                "\nDepth Values (1-100):\n"
                "  1-20:   Quick/light crawl (fewer results, shallow depth, faster)\n"
                "  21-50:  Medium crawl (moderate results and depth)\n"
                "  51-80:  Thorough crawl (more results, deeper level)\n"
                "  81-100: Deep crawl (maximum results and depth level, longer runtime)"),
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    # Basic options
    basic_group = parser.add_argument_group('Basic Options')
    basic_group.add_argument('--query', type=str, 
                    help="Search query for DuckDuckGo (ddgr)")
    basic_group.add_argument('--target', type=str, 
                    help="Target URL for Photon OSINT scraping")
    basic_group.add_argument('--depth', type=int, default=30, 
                    help="Search depth (1-100): 1=quickest, 100=deepest. Controls number of results, crawl depth, and threads")
    basic_group.add_argument('--debug', action='store_true', 
                    help="Enable debug mode for detailed output and error messages")
    basic_group.add_argument('--no-crawl', action='store_true',
                    help="Disable automatic crawling of search results (search only)")
    basic_group.add_argument('--no-pagination', action='store_true',
                    help="Disable pagination ('more' prompt at the end)")
    basic_group.add_argument('--version', action='version', 
                    version=f"%(prog)s {__version__}")
    
    # ddgr-specific options
    ddgr_group = parser.add_argument_group('DuckDuckGo (ddgr) Options')
    ddgr_group.add_argument('--ddgr-region', type=str, metavar='REG', default='us-en',
                    help="region-specific search e.g. 'us-en' for US (default)")
    ddgr_group.add_argument('--ddgr-colorize', type=str, choices=['auto', 'always', 'never'], default='auto',
                    help="whether to colorize output")
    ddgr_group.add_argument('--ddgr-nocolor', action='store_true',
                    help="equivalent to --ddgr-colorize=never")
    ddgr_group.add_argument('--ddgr-colors', type=str, metavar='COLORS',
                    help="set output colors")
    ddgr_group.add_argument('--ddgr-time', type=str, metavar='SPAN', choices=('d', 'w', 'm', 'y'),
                    help="time limit search [d (1 day), w (1 wk), m (1 month), y (1 year)]")
    ddgr_group.add_argument('--ddgr-site', type=str, metavar='SITE', action='append',
                    help="search sites using DuckDuckGo")
    ddgr_group.add_argument('--ddgr-expand', action='store_true',
                    help="Show complete url in search results")
    ddgr_group.add_argument('--ddgr-proxy', type=str, metavar='URI',
                    help="tunnel traffic through an HTTPS proxy; URI format: [http[s]://][user:pwd@]host[:port]")
    ddgr_group.add_argument('--ddgr-unsafe', action='store_true',
                    help="disable safe search")
    ddgr_group.add_argument('--ddgr-noua', action='store_true',
                    help="disable user agent")
    
    # Photon-specific options
    photon_group = parser.add_argument_group('Photon OSINT Options')
    photon_group.add_argument('--photon-level', type=int, metavar='LEVEL', 
                    help="levels to crawl (1-5)")
    photon_group.add_argument('--photon-threads', type=int, metavar='THREADS',
                    help="number of threads")
    photon_group.add_argument('--photon-delay', type=float, metavar='DELAY',
                    help="delay between requests")
    photon_group.add_argument('--photon-timeout', type=float, metavar='TIMEOUT',
                    help="http request timeout")
    photon_group.add_argument('--photon-cookie', type=str, metavar='COOKIE',
                    help="cookie")
    photon_group.add_argument('--photon-regex', type=str, metavar='REGEX',
                    help="regex pattern")
    photon_group.add_argument('--photon-export', type=str, metavar='FORMAT', choices=['csv', 'json'],
                    help="export format (csv, json)")
    photon_group.add_argument('--photon-seeds', type=str, metavar='SEEDS', action='append',
                    help="additional seed URLs")
    photon_group.add_argument('--photon-user-agent', type=str, metavar='UA',
                    help="custom user agent(s)")
    photon_group.add_argument('--photon-exclude', type=str, metavar='REGEX',
                    help="exclude URLs matching this regex")
    photon_group.add_argument('--photon-proxy', type=str, metavar='PROXY',
                    help="Proxy server IP:PORT or DOMAIN:PORT")
    photon_group.add_argument('--photon-verbose', action='store_true',
                    help="verbose output")
    photon_group.add_argument('--photon-headers', action='store_true',
                    help="add headers")
    photon_group.add_argument('--photon-dns', action='store_true',
                    help="enumerate subdomains and DNS data")
    photon_group.add_argument('--photon-keys', action='store_true',
                    help="find secret keys")
    photon_group.add_argument('--photon-only-urls', action='store_true',
                    help="only extract URLs")
    photon_group.add_argument('--photon-wayback', action='store_true',
                    help="fetch URLs from archive.org as seeds")
    photon_group.add_argument('--photon-max-targets', type=int, metavar='N',
                    help="maximum number of targets to crawl (overrides automatic scaling)")
    
    # Positional arguments (for simpler command line usage)
    parser.add_argument('keywords', type=str, nargs='?', 
                    help="Search keywords (e.g., \"George Washington\")")
    parser.add_argument('url_or_depth', type=str, nargs='?',
                    help="Either a URL to crawl (e.g., \"https://example.com\") or depth value (e.g., \"45\")")
    parser.add_argument('positional_depth', type=str, nargs='?',
                    help="Depth value when URL is provided (e.g., \"45\" when using format: \"keywords URL depth\")")
    
    args = parser.parse_args()

    # Configure logging based on the debug flag
    logging_level = logging.DEBUG if args.debug else logging.INFO
    logging.basicConfig(level=logging_level, format='[%(levelname)s] %(message)s')
    
    if args.debug:
        logging.debug("Debug mode enabled.")

    # Process positional arguments if provided
    if args.keywords:
        # We have at least the keywords argument
        query = args.keywords
        target = None
        depth = args.depth  # Default to named argument depth
        
        if args.url_or_depth:
            # Second argument could be either a URL or depth
            if is_url(args.url_or_depth):
                # It's a URL
                target = args.url_or_depth
                # If we have a third positional argument, it's the depth
                if args.positional_depth and args.positional_depth.isdigit():
                    depth = int(args.positional_depth)
            elif args.url_or_depth.isdigit():
                # It's a depth value
                depth = int(args.url_or_depth)
    else:
        # Use named arguments
        query = args.query
        target = args.target
        depth = args.depth
    
    # Ensure depth is within valid range
    if depth is not None:
        depth = max(1, min(100, depth))
    
    # Ensure we have at least one action to perform
    if not query and not target:
        parser.error("You must provide either keywords, a URL, or use --query/--target arguments.")
    
    # Initialize list of URLs to crawl with Photon
    urls_to_crawl = []
    
    # Add specifically provided target URL if any
    if target:
        urls_to_crawl.append(target)
    
    # Prepare ddgr arguments
    ddgr_args = {
        'region': args.ddgr_region,
        'colorize': args.ddgr_colorize,
        'nocolor': args.ddgr_nocolor,
        'colors': args.ddgr_colors,
        'time': args.ddgr_time,
        'site': args.ddgr_site,
        'expand': args.ddgr_expand,
        'proxy': args.ddgr_proxy,
        'unsafe': args.ddgr_unsafe,
        'noua': args.ddgr_noua
    }
    
    # Prepare photon arguments
    photon_args = {
        'level': args.photon_level,
        'threads': args.photon_threads,
        'delay': args.photon_delay,
        'timeout': args.photon_timeout,
        'cookie': args.photon_cookie,
        'regex': args.photon_regex,
        'export': args.photon_export,
        'seeds': args.photon_seeds,
        'user_agent': args.photon_user_agent,
        'exclude': args.photon_exclude,
        'proxy': args.photon_proxy,
        'verbose': args.photon_verbose,
        'headers': args.photon_headers,
        'dns': args.photon_dns,
        'keys': args.photon_keys,
        'only_urls': args.photon_only_urls,
        'wayback': args.photon_wayback,
        'max_targets': args.photon_max_targets
    }
    
    # Run ddgr search if query is provided
    if query:
        page = 1  # Start at page 1 (1-based for user display)
        has_more = True
        
        while has_more:
            # Run search for current page with proper pagination
            search_urls, has_more = run_ddgr_with_pagination(query, ddgr_args, depth, args.debug, page)
            
            # Add search results to crawl list if auto-crawl is enabled
            if not args.no_crawl and search_urls:
                # Run Photon on collected URLs for this page
                run_photon_on_multiple_targets(search_urls, photon_args, depth, args.debug)
            
            # Check if pagination is disabled or if we've reached the end
            if args.no_pagination or not has_more:
                break
                
            # Ask for more results
            if not ask_for_more():
                break
                
            # Move to next page
            page += 1
            
    elif target and not args.no_crawl:
        # If no query but a target was provided, crawl that target
        run_photon_on_single_target(target, photon_args, depth, args.debug)

if __name__ == '__main__':
    main()

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/photon.py
========================================

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""The Photon main part."""
from __future__ import print_function

import argparse
import os
import re
import requests
import sys
import time
import warnings
import random

from core.colors import good, info, run, green, red, white, end, bad

# Just a fancy ass banner
print('''%s      ____  __          __
     / %s__%s \/ /_  ____  / /_____  ____
    / %s/_/%s / __ \/ %s__%s \/ __/ %s__%s \/ __ \\
   / ____/ / / / %s/_/%s / /_/ %s/_/%s / / / /
  /_/   /_/ /_/\____/\__/\____/_/ /_/ %sv1.3.2%s\n''' %
      (red, white, red, white, red, white, red, white, red, white, red, white,
       red, white, end))

try:
    from urllib.parse import urlparse  # For Python 3
except ImportError:
    print('%s Photon runs only on Python 3.2 and above.' % info)
    quit()

import core.config
from core.config import INTELS
from core.flash import flash
from core.mirror import mirror
from core.prompt import prompt
from core.requester import requester
from core.updater import updater
from core.utils import (luhn,
                        proxy_type,
                        is_good_proxy,
                        top_level,
                        extract_headers,
                        verb, is_link,
                        entropy, regxy,
                        remove_regex,
                        timer,
                        writer)
from core.regex import rintels, rendpoint, rhref, rscript, rentropy

from core.zap import zap

# Disable SSL related warnings
warnings.filterwarnings('ignore')

# Processing command line arguments
parser = argparse.ArgumentParser()
# Options
parser.add_argument('-u', '--url', help='root url', dest='root')
parser.add_argument('-c', '--cookie', help='cookie', dest='cook')
parser.add_argument('-r', '--regex', help='regex pattern', dest='regex')
parser.add_argument('-e', '--export', help='export format', dest='export', choices=['csv', 'json'])
parser.add_argument('-o', '--output', help='output directory', dest='output')
parser.add_argument('-l', '--level', help='levels to crawl', dest='level',
                    type=int)
parser.add_argument('-t', '--threads', help='number of threads', dest='threads',
                    type=int)
parser.add_argument('-d', '--delay', help='delay between requests',
                    dest='delay', type=float)
parser.add_argument('-v', '--verbose', help='verbose output', dest='verbose',
                    action='store_true')
parser.add_argument('-s', '--seeds', help='additional seed URLs', dest='seeds',
                    nargs="+", default=[])
parser.add_argument('--stdout', help='send variables to stdout', dest='std')
parser.add_argument('--user-agent', help='custom user agent(s)',
                    dest='user_agent')
parser.add_argument('--exclude', help='exclude URLs matching this regex',
                    dest='exclude')
parser.add_argument('--timeout', help='http request timeout', dest='timeout',
                    type=float)
parser.add_argument('-p', '--proxy', help='Proxy server IP:PORT or DOMAIN:PORT', dest='proxies',
                    type=proxy_type)

# Switches
parser.add_argument('--clone', help='clone the website locally', dest='clone',
                    action='store_true')
parser.add_argument('--headers', help='add headers', dest='headers',
                    action='store_true')
parser.add_argument('--dns', help='enumerate subdomains and DNS data',
                    dest='dns', action='store_true')
parser.add_argument('--keys', help='find secret keys', dest='api',
                    action='store_true')
parser.add_argument('--update', help='update photon', dest='update',
                    action='store_true')
parser.add_argument('--only-urls', help='only extract URLs', dest='only_urls',
                    action='store_true')
parser.add_argument('--wayback', help='fetch URLs from archive.org as seeds',
                    dest='archive', action='store_true')
args = parser.parse_args()


# If the user has supplied --update argument
if args.update:
    updater()
    quit()

# If the user has supplied a URL
if args.root:
    main_inp = args.root
    if main_inp.endswith('/'):
        # We will remove it as it can cause problems later in the code
        main_inp = main_inp[:-1]
# If the user hasn't supplied an URL
else:
    print('\n' + parser.format_help().lower())
    quit()

clone = args.clone
headers = args.headers  # prompt for headers
verbose = args.verbose  # verbose output
delay = args.delay or 0  # Delay between requests
timeout = args.timeout or 6  # HTTP request timeout
cook = args.cook or None  # Cookie
api = bool(args.api)  # Extract high entropy strings i.e. API keys and stuff

proxies = []
if args.proxies:
    print("%s Testing proxies, can take a while..." % info)
    for proxy in args.proxies:
        if is_good_proxy(proxy):
            proxies.append(proxy)
        else:
            print("%s Proxy %s doesn't seem to work or timedout" %
                  (bad, proxy['http']))
    print("%s Done" % info)
    if not proxies:
        print("%s no working proxies, quitting!" % bad)
        exit()
else:
    proxies.append(None)

crawl_level = args.level or 2  # Crawling level
thread_count = args.threads or 2  # Number of threads
only_urls = bool(args.only_urls)  # Only URLs mode is off by default

# Variables we are gonna use later to store stuff
keys = set()  # High entropy strings, prolly secret keys
files = set()  # The pdf, css, png, etc files.
intel = set()  # The email addresses, website accounts, AWS buckets etc.
robots = set()  # The entries of robots.txt
custom = set()  # Strings extracted by custom regex pattern
failed = set()  # URLs that photon failed to crawl
scripts = set()  # THe Javascript files
external = set()  # URLs that don't belong to the target i.e. out-of-scope
# URLs that have get params in them e.g. example.com/page.php?id=2
fuzzable = set()
endpoints = set()  # URLs found from javascript files
processed = set(['dummy'])  # URLs that have been crawled
# URLs that belong to the target i.e. in-scope
internal = set(args.seeds)

everything = []
bad_scripts = set()  # Unclean javascript file urls
bad_intel = set() # needed for intel filtering

core.config.verbose = verbose

if headers:
    try:
        prompt = prompt()
    except FileNotFoundError as e:
        print('Could not load headers prompt: {}'.format(e))
        quit()
    headers = extract_headers(prompt)

# If the user hasn't supplied the root URL with http(s), we will handle it
if main_inp.startswith('http'):
    main_url = main_inp
else:
    try:
        requests.get('https://' + main_inp, proxies=random.choice(proxies))
        main_url = 'https://' + main_inp
    except:
        main_url = 'http://' + main_inp

schema = main_url.split('//')[0] # https: or http:?
# Adding the root URL to internal for crawling
internal.add(main_url)
# Extracts host out of the URL
host = urlparse(main_url).netloc

output_dir = args.output or host

try:
    domain = top_level(main_url)
except:
    domain = host

if args.user_agent:
    user_agents = args.user_agent.split(',')
else:
    with open(sys.path[0] + '/core/user-agents.txt', 'r') as uas:
        user_agents = [agent.strip('\n') for agent in uas]


supress_regex = False

def intel_extractor(url, response):
    """Extract intel from the response body."""
    for rintel in rintels:
        res = re.sub(r'<(script).*?</\1>(?s)', '', response)
        res = re.sub(r'<[^<]+?>', '', res)
        matches = rintel[0].findall(res)
        if matches:
            for match in matches:
                verb('Intel', match)
                bad_intel.add((match, rintel[1], url))


def js_extractor(response):
    """Extract js files from the response body"""
    # Extract .js files
    matches = rscript.findall(response)
    for match in matches:
        match = match[2].replace('\'', '').replace('"', '')
        verb('JS file', match)
        bad_scripts.add(match)

def remove_file(url):
    if url.count('/') > 2:
        replacable = re.search(r'/[^/]*?$', url).group()
        if replacable != '/':
            return url.replace(replacable, '')
        else:
            return url
    else:
        return url

def extractor(url):
    """Extract details from the response body."""
    response = requester(url, main_url, delay, cook, headers, timeout, host, proxies, user_agents, failed, processed)
    if clone:
        mirror(url, response)
    matches = rhref.findall(response)
    for link in matches:
        # Remove everything after a "#" to deal with in-page anchors
        link = link[1].replace('\'', '').replace('"', '').split('#')[0]
        # Checks if the URLs should be crawled
        if is_link(link, processed, files):
            if link[:4] == 'http':
                if link.startswith(main_url):
                    verb('Internal page', link)
                    internal.add(link)
                else:
                    verb('External page', link)
                    external.add(link)
            elif link[:2] == '//':
                if link.split('/')[2].startswith(host):
                    verb('Internal page', link)
                    internal.add(schema + '://' + link)
                else:
                    verb('External page', link)
                    external.add(link)
            elif link[:1] == '/':
                verb('Internal page', link)
                internal.add(remove_file(url) + link)
            else:
                verb('Internal page', link)
                usable_url = remove_file(url)
                if usable_url.endswith('/'):
                    internal.add(usable_url + link)
                elif link.startswith('/'):
                    internal.add(usable_url + link)
                else:
                    internal.add(usable_url + '/' + link)

    if not only_urls:
        intel_extractor(url, response)
        js_extractor(response)
    if args.regex and not supress_regex:
        regxy(args.regex, response, supress_regex, custom)
    if api:
        matches = rentropy.findall(response)
        for match in matches:
            if entropy(match) >= 4:
                verb('Key', match)
                keys.add(url + ': ' + match)


def jscanner(url):
    """Extract endpoints from JavaScript code."""
    response = requester(url, main_url, delay, cook, headers, timeout, host, proxies, user_agents, failed, processed)
    # Extract URLs/endpoints
    matches = rendpoint.findall(response)
    # Iterate over the matches, match is a tuple
    for match in matches:
        # Combining the items because one of them is always empty
        match = match[0] + match[1]
        # Making sure it's not some JavaScript code
        if not re.search(r'[}{><"\']', match) and not match == '/':
            verb('JS endpoint', match)
            endpoints.add(match)


# Records the time at which crawling started
then = time.time()

# Step 1. Extract urls from robots.txt & sitemap.xml
zap(main_url, args.archive, domain, host, internal, robots, proxies)

# This is so the level 1 emails are parsed as well
internal = set(remove_regex(internal, args.exclude))

# Step 2. Crawl recursively to the limit specified in "crawl_level"
for level in range(crawl_level):
    # Links to crawl = (all links - already crawled links) - links not to crawl
    links = remove_regex(internal - processed, args.exclude)
    # If links to crawl are 0 i.e. all links have been crawled
    if not links:
        break
    # if crawled links are somehow more than all links. Possible? ;/
    elif len(internal) <= len(processed):
        if len(internal) > 2 + len(args.seeds):
            break
    print('%s Level %i: %i URLs' % (run, level + 1, len(links)))
    try:
        flash(extractor, links, thread_count)
    except KeyboardInterrupt:
        print('')
        break

if not only_urls:
    for match in bad_scripts:
        if match.startswith(main_url):
            scripts.add(match)
        elif match.startswith('/') and not match.startswith('//'):
            scripts.add(main_url + match)
        elif not match.startswith('http') and not match.startswith('//'):
            scripts.add(main_url + '/' + match)
    # Step 3. Scan the JavaScript files for endpoints
    print('%s Crawling %i JavaScript files' % (run, len(scripts)))
    flash(jscanner, scripts, thread_count)

    for url in internal:
        if '=' in url:
            fuzzable.add(url)

    for match, intel_name, url in bad_intel:
        if isinstance(match, tuple):
            for x in match:  # Because "match" is a tuple
                if x != '':  # If the value isn't empty
                    if intel_name == "CREDIT_CARD":
                        if not luhn(match):
                            # garbage number
                            continue
                    intel.add("%s:%s" % (intel_name, x))
        else:
            if intel_name == "CREDIT_CARD":
                if not luhn(match):
                    # garbage number
                    continue
            intel.add("%s:%s:%s" % (url, intel_name, match))
        for url in external:
            try:
                if top_level(url, fix_protocol=True) in INTELS:
                    intel.add(url)
            except:
                pass

# Records the time at which crawling stopped
now = time.time()
# Finds total time taken
diff = (now - then)
minutes, seconds, time_per_request = timer(diff, processed)

# Step 4. Save the results
if not os.path.exists(output_dir): # if the directory doesn't exist
    os.mkdir(output_dir) # create a new directory

datasets = [files, intel, robots, custom, failed, internal, scripts,
            external, fuzzable, endpoints, keys]
dataset_names = ['files', 'intel', 'robots', 'custom', 'failed', 'internal',
                 'scripts', 'external', 'fuzzable', 'endpoints', 'keys']

writer(datasets, dataset_names, output_dir)
# Printing out results
print(('%s-%s' % (red, end)) * 50)
for dataset, dataset_name in zip(datasets, dataset_names):
    if dataset:
        print('%s %s: %s' % (good, dataset_name.capitalize(), len(dataset)))
print(('%s-%s' % (red, end)) * 50)

print('%s Total requests made: %i' % (info, len(processed)))
print('%s Total time taken: %i minutes %i seconds' % (info, minutes, seconds))
print('%s Requests per second: %i' % (info, int(len(processed) / diff)))

datasets = {
    'files': list(files), 'intel': list(intel), 'robots': list(robots),
    'custom': list(custom), 'failed': list(failed), 'internal': list(internal),
    'scripts': list(scripts), 'external': list(external),
    'fuzzable': list(fuzzable), 'endpoints': list(endpoints),
    'keys': list(keys)
}

if args.dns:
    print('%s Enumerating subdomains' % run)
    from plugins.find_subdomains import find_subdomains
    subdomains = find_subdomains(domain)
    print('%s %i subdomains found' % (info, len(subdomains)))
    writer([subdomains], ['subdomains'], output_dir)
    datasets['subdomains'] = subdomains
    from plugins.dnsdumpster import dnsdumpster
    print('%s Generating DNS map' % run)
    dnsdumpster(domain, output_dir)

if args.export:
    from plugins.exporter import exporter
    # exporter(directory, format, datasets)
    exporter(output_dir, args.export, datasets)

print('%s Results saved in %s%s%s directory' % (good, green, output_dir, end))

if args.std:
    for string in datasets[args.std]:
        sys.stdout.write(string + '\n')

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/summary_collector.py
========================================

#!/usr/bin/env python3

import os
import glob

# Define the path to search
search_path = "/home/jarvis/photon_results"
# Define the output file
output_file = "summary_collection.txt"

# Find all summary.txt files
summary_files = glob.glob(os.path.join(search_path, "**", "summary.txt"), recursive=True)

# Open the output file for writing
with open(output_file, "w") as outfile:
    for file_path in summary_files:
        outfile.write(f"## Contents from: {file_path}\n")
        
        # Read and write the contents of each summary file
        try:
            with open(file_path, "r") as infile:
                outfile.write(infile.read())
        except UnicodeDecodeError:
            outfile.write("Error: Could not read file (possibly binary data)\n")
        
        outfile.write("\n\n")

print(f"Summary collection has been created at {output_file}")

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/scope.py
========================================

#!/usr/bin/env python3
"""
PDF Text Scanner - Enhanced Version
Scans PDF documents for specified text and extracts paragraphs containing that text.

Features:
- Interactive setup with user-friendly prompts
- Recursive subdirectory scanning at any depth
- Smart paragraph extraction with context
- Multiple output formats (TXT, CSV, JSON, Summary)
- Progress tracking with detailed logging
- Error recovery and robust file handling
- Support for regex and case-sensitive searches

Usage:
    python pdf_scanner.py

Requirements:
    pip install pdfplumber tqdm colorama
"""

import os
import re
import csv
import sys
import json
import time
import traceback
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Set
from collections import defaultdict
import logging

# Try to import required libraries
try:
    import pdfplumber
    from tqdm import tqdm
    from colorama import init, Fore, Style
    init(autoreset=True)  # Initialize colorama
except ImportError as e:
    print("Required libraries not found. Please install them using:")
    print("pip install pdfplumber tqdm colorama")
    print(f"\nError details: {e}")
    sys.exit(1)


class PDFTextScanner:
    """Enhanced PDF text scanner with improved features and error handling."""
    
    def __init__(self, search_text: str, pdf_directory: str, output_directory: str = "results"):
        """Initialize the PDF scanner with configuration."""
        self.search_text = search_text
        self.pdf_directory = Path(pdf_directory).resolve()
        self.output_directory = Path(output_directory).resolve()
        self.output_directory.mkdir(parents=True, exist_ok=True)
        
        # Configuration flags
        self.include_subdirs = False
        self.case_sensitive = False
        self.use_regex = False
        
        # Results storage
        self.results = []
        self.failed_files = []
        self.processed_files = 0
        self.total_files = 0
        
        # Setup logging
        self.setup_logging()
        
    def setup_logging(self):
        """Configure comprehensive logging system."""
        log_filename = f"scan_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        log_file = self.output_directory / log_filename
        
        # Create a custom logger
        self.logger = logging.getLogger('PDFScanner')
        self.logger.setLevel(logging.DEBUG)
        
        # File handler for detailed logs
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.DEBUG)
        file_formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(file_formatter)
        
        # Console handler for important messages only
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(logging.INFO)
        console_formatter = logging.Formatter('%(levelname)s: %(message)s')
        console_handler.setFormatter(console_formatter)
        
        # Add handlers to logger
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)
        
        self.logger.info(f"Logging initialized. Log file: {log_file}")
        
    def get_pdf_files(self, include_subdirs: bool = False) -> List[Path]:
        """
        Get all PDF files from the specified directory.
        
        Args:
            include_subdirs: Whether to recursively search subdirectories
            
        Returns:
            List of PDF file paths
        """
        try:
            if include_subdirs:
                # Recursive search
                pdf_files = list(self.pdf_directory.rglob("*.pdf"))
                pdf_files.extend(self.pdf_directory.rglob("*.PDF"))  # Case variations
                self.logger.info(f"Searching recursively in {self.pdf_directory}")
            else:
                # Non-recursive search
                pdf_files = list(self.pdf_directory.glob("*.pdf"))
                pdf_files.extend(self.pdf_directory.glob("*.PDF"))
                self.logger.info(f"Searching in {self.pdf_directory} (top-level only)")
            
            # Remove duplicates and sort
            pdf_files = sorted(list(set(pdf_files)))
            
            if pdf_files:
                # Log directory structure information
                dirs = set(f.parent for f in pdf_files)
                self.logger.info(f"Found {len(pdf_files)} PDF files in {len(dirs)} directories")
            else:
                self.logger.warning(f"No PDF files found in {self.pdf_directory}")
                
            return pdf_files
            
        except Exception as e:
            self.logger.error(f"Error scanning for PDF files: {e}")
            return []
            
    def extract_paragraph(self, text: str, position: int, 
                         context_chars: int = 300) -> Tuple[str, int, int]:
        """
        Extract a paragraph around the found text with improved logic.
        
        Args:
            text: Full page text
            position: Position of found text
            context_chars: Number of characters to include as context
            
        Returns:
            Tuple of (paragraph, start_pos, end_pos)
        """
        if not text:
            return "", 0, 0
            
        # Define paragraph boundaries
        paragraph_patterns = [
            r'\n\s*\n',           # Double newline
            r'\n(?=[A-Z])',       # Newline followed by capital letter
            r'\.\s+(?=[A-Z])',    # Period followed by capital letter
            r'[.!?]\s*\n',        # Sentence ending followed by newline
        ]
        
        # Find all potential paragraph breaks
        breaks = set([0, len(text)])
        for pattern in paragraph_patterns:
            for match in re.finditer(pattern, text):
                breaks.add(match.start())
                breaks.add(match.end())
        
        breaks = sorted(list(breaks))
        
        # Find the paragraph containing our position
        para_start = 0
        para_end = len(text)
        
        for i in range(len(breaks) - 1):
            if breaks[i] <= position < breaks[i + 1]:
                para_start = breaks[i]
                para_end = breaks[i + 1]
                
                # Expand to include more context if paragraph is too small
                while para_end - para_start < context_chars and (
                    para_start > 0 or para_end < len(text)
                ):
                    if para_start > 0:
                        # Find previous break
                        idx = breaks.index(para_start)
                        if idx > 0:
                            para_start = breaks[idx - 1]
                    if para_end < len(text) and para_end - para_start < context_chars:
                        # Find next break
                        idx = breaks.index(para_end)
                        if idx < len(breaks) - 1:
                            para_end = breaks[idx + 1]
                            
                break
                
        # Clean up the paragraph
        paragraph = text[para_start:para_end].strip()
        
        # Remove excessive whitespace
        paragraph = re.sub(r'\s+', ' ', paragraph)
        
        return paragraph, para_start, para_end
        
    def search_in_pdf(self, pdf_path: Path) -> List[Dict]:
        """
        Search for text in a single PDF file with improved error handling.
        
        Args:
            pdf_path: Path to PDF file
            
        Returns:
            List of results found in this PDF
        """
        file_results = []
        
        try:
            with pdfplumber.open(pdf_path) as pdf:
                total_pages = len(pdf.pages)
                
                for page_num, page in enumerate(pdf.pages, 1):
                    try:
                        # Extract text with fallback options
                        text = page.extract_text()
                        
                        if not text:
                            # Try alternative extraction methods
                            text = page.extract_text(layout=True)
                            
                        if not text:
                            self.logger.debug(f"No text found on page {page_num} of {pdf_path.name}")
                            continue
                            
                        # Prepare search pattern
                        if self.use_regex:
                            pattern = self.search_text
                        else:
                            pattern = re.escape(self.search_text)
                            
                        flags = 0 if self.case_sensitive else re.IGNORECASE
                        
                        # Find all occurrences
                        matches = list(re.finditer(pattern, text, flags))
                        
                        for match_num, match in enumerate(matches, 1):
                            position = match.start()
                            paragraph, para_start, para_end = self.extract_paragraph(
                                text, position
                            )
                            
                            # Calculate relative path for better organization
                            try:
                                relative_path = pdf_path.relative_to(self.pdf_directory)
                            except ValueError:
                                relative_path = pdf_path.name
                                
                            result = {
                                'filename': pdf_path.name,
                                'filepath': str(relative_path),
                                'full_path': str(pdf_path),
                                'page': page_num,
                                'total_pages': total_pages,
                                'search_term': self.search_text,
                                'found_text': match.group(),
                                'paragraph': paragraph,
                                'position': position,
                                'match_number': match_num,
                                'timestamp': datetime.now().isoformat()
                            }
                            
                            file_results.append(result)
                            
                    except Exception as e:
                        self.logger.error(f"Error processing page {page_num} of {pdf_path.name}: {e}")
                        continue
                        
        except Exception as e:
            self.logger.error(f"Error opening PDF {pdf_path}: {e}")
            self.failed_files.append({
                'file': str(pdf_path),
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            })
            
        return file_results
        
    def scan_all_pdfs(self, case_sensitive: bool = False, 
                     use_regex: bool = False, 
                     include_subdirs: bool = False):
        """
        Scan all PDF files with improved progress tracking and error recovery.
        """
        self.case_sensitive = case_sensitive
        self.use_regex = use_regex
        self.include_subdirs = include_subdirs
        
        pdf_files = self.get_pdf_files(include_subdirs)
        self.total_files = len(pdf_files)
        
        if not pdf_files:
            print(f"{Fore.YELLOW}No PDF files found to scan.{Style.RESET_ALL}")
            return
            
        print(f"\n{Fore.GREEN}Found {self.total_files} PDF files to scan{Style.RESET_ALL}")
        print(f"Searching for: '{Fore.CYAN}{self.search_text}{Style.RESET_ALL}'")
        
        # Process each PDF with progress bar
        with tqdm(total=self.total_files, desc="Scanning PDFs", unit="file") as pbar:
            for pdf_file in pdf_files:
                pbar.set_description(f"Scanning: {pdf_file.name[:30]}...")
                
                results = self.search_in_pdf(pdf_file)
                self.results.extend(results)
                self.processed_files += 1
                
                if results:
                    tqdm.write(
                        f"{Fore.GREEN} Found {len(results)} occurrences in "
                        f"{pdf_file.name}{Style.RESET_ALL}"
                    )
                    
                pbar.update(1)
                
        # Summary
        print(f"\n{Fore.CYAN}Scan Complete!{Style.RESET_ALL}")
        print(f"Files processed: {self.processed_files}/{self.total_files}")
        print(f"Total occurrences found: {len(self.results)}")
        if self.failed_files:
            print(f"{Fore.YELLOW}Failed to process {len(self.failed_files)} files{Style.RESET_ALL}")
            
    def save_txt_results(self) -> Path:
        """Save results to a formatted text file."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        txt_file = self.output_directory / f"results_{timestamp}.txt"
        
        with open(txt_file, 'w', encoding='utf-8') as f:
            # Header
            f.write("PDF TEXT SEARCH RESULTS\n")
            f.write("=" * 80 + "\n")
            f.write(f"Search Term: '{self.search_text}'\n")
            f.write(f"Search Type: {'Regex' if self.use_regex else 'Plain Text'}\n")
            f.write(f"Case Sensitive: {'Yes' if self.case_sensitive else 'No'}\n")
            f.write(f"Include Subdirectories: {'Yes' if self.include_subdirs else 'No'}\n")
            f.write(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Total Files Scanned: {self.total_files}\n")
            f.write(f"Total Occurrences: {len(self.results)}\n")
            f.write("=" * 80 + "\n\n")
            
            # Group results by file
            file_groups = defaultdict(list)
            for result in self.results:
                file_groups[result['filepath']].append(result)
                
            # Write results
            for filepath, file_results in sorted(file_groups.items()):
                f.write(f"\nFILE: {filepath}\n")
                f.write(f"Occurrences in this file: {len(file_results)}\n")
                f.write("-" * 80 + "\n")
                
                for i, result in enumerate(file_results, 1):
                    f.write(f"\nOccurrence {i}:\n")
                    f.write(f"  Page: {result['page']} of {result['total_pages']}\n")
                    f.write(f"  Found: '{result['found_text']}'\n")
                    f.write(f"  Position: character {result['position']}\n")
                    f.write(f"\n  Context:\n")
                    
                    # Wrap paragraph text
                    paragraph_lines = self._wrap_text(result['paragraph'], 76)
                    for line in paragraph_lines:
                        f.write(f"    {line}\n")
                        
                f.write("\n" + "=" * 80 + "\n")
                
            # Add failed files section if any
            if self.failed_files:
                f.write("\n\nFAILED FILES\n")
                f.write("=" * 80 + "\n")
                for failed in self.failed_files:
                    f.write(f"\nFile: {failed['file']}\n")
                    f.write(f"Error: {failed['error']}\n")
                    
        self.logger.info(f"Text results saved to: {txt_file}")
        return txt_file
        
    def save_csv_results(self) -> Path:
        """Save results to CSV with improved formatting."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        csv_file = self.output_directory / f"results_{timestamp}.csv"
        
        if not self.results:
            self.logger.warning("No results to save to CSV")
            return csv_file
            
        fieldnames = [
            'filename', 'filepath', 'page', 'total_pages', 
            'search_term', 'found_text', 'position', 
            'paragraph_preview', 'full_paragraph'
        ]
        
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            
            for result in self.results:
                row = {
                    'filename': result['filename'],
                    'filepath': result['filepath'],
                    'page': result['page'],
                    'total_pages': result['total_pages'],
                    'search_term': result['search_term'],
                    'found_text': result['found_text'],
                    'position': result['position'],
                    'paragraph_preview': result['paragraph'][:200] + '...' 
                        if len(result['paragraph']) > 200 else result['paragraph'],
                    'full_paragraph': result['paragraph']
                }
                writer.writerow(row)
                
        self.logger.info(f"CSV results saved to: {csv_file}")
        return csv_file
        
    def save_json_results(self) -> Path:
        """Save results to JSON for programmatic access."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_file = self.output_directory / f"results_{timestamp}.json"
        
        data = {
            'metadata': {
                'search_term': self.search_text,
                'search_type': 'regex' if self.use_regex else 'plain_text',
                'case_sensitive': self.case_sensitive,
                'include_subdirs': self.include_subdirs,
                'scan_date': datetime.now().isoformat(),
                'total_files': self.total_files,
                'processed_files': self.processed_files,
                'total_occurrences': len(self.results)
            },
            'results': self.results,
            'failed_files': self.failed_files
        }
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
            
        self.logger.info(f"JSON results saved to: {json_file}")
        return json_file
        
    def generate_summary(self) -> Path:
        """Generate an enhanced summary report."""
        summary_file = self.output_directory / f"summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        
        # Analyze results
        file_groups = defaultdict(list)
        page_distribution = defaultdict(int)
        
        for result in self.results:
            file_groups[result['filepath']].append(result)
            page_distribution[result['page']] += 1
            
        with open(summary_file, 'w', encoding='utf-8') as f:
            # Header
            f.write("SEARCH SUMMARY REPORT\n")
            f.write("=" * 80 + "\n")
            f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Search Term: '{self.search_text}'\n")
            f.write("\n")
            
            # Statistics
            f.write("STATISTICS\n")
            f.write("-" * 40 + "\n")
            f.write(f"Total PDF files scanned: {self.total_files}\n")
            f.write(f"Files successfully processed: {self.processed_files}\n")
            f.write(f"Files with matches: {len(file_groups)}\n")
            f.write(f"Total occurrences found: {len(self.results)}\n")
            if self.results:
                avg_per_file = len(self.results) / len(file_groups)
                f.write(f"Average occurrences per file: {avg_per_file:.1f}\n")
            f.write("\n")
            
            # Files with most occurrences
            if file_groups:
                f.write("TOP FILES BY OCCURRENCES\n")
                f.write("-" * 40 + "\n")
                sorted_files = sorted(
                    file_groups.items(), 
                    key=lambda x: len(x[1]), 
                    reverse=True
                )[:10]
                
                for filepath, occurrences in sorted_files:
                    f.write(f"{filepath}: {len(occurrences)} occurrences\n")
                f.write("\n")
                
            # Directory breakdown if using subdirectories
            if self.include_subdirs and file_groups:
                f.write("DIRECTORY BREAKDOWN\n")
                f.write("-" * 40 + "\n")
                
                dir_stats = defaultdict(lambda: {'files': 0, 'occurrences': 0})
                for filepath, occurrences in file_groups.items():
                    dir_path = str(Path(filepath).parent)
                    if dir_path == '.':
                        dir_path = '[Main Directory]'
                    dir_stats[dir_path]['files'] += 1
                    dir_stats[dir_path]['occurrences'] += len(occurrences)
                    
                for dir_path, stats in sorted(dir_stats.items()):
                    f.write(f"{dir_path}:\n")
                    f.write(f"  Files with matches: {stats['files']}\n")
                    f.write(f"  Total occurrences: {stats['occurrences']}\n")
                f.write("\n")
                
            # Failed files
            if self.failed_files:
                f.write("FAILED FILES\n")
                f.write("-" * 40 + "\n")
                for failed in self.failed_files:
                    f.write(f"{failed['file']}: {failed['error']}\n")
                    
        self.logger.info(f"Summary saved to: {summary_file}")
        return summary_file
        
    def _wrap_text(self, text: str, width: int) -> List[str]:
        """Wrap text to specified width."""
        words = text.split()
        lines = []
        current_line = []
        current_length = 0
        
        for word in words:
            if current_length + len(word) + len(current_line) <= width:
                current_line.append(word)
                current_length += len(word)
            else:
                if current_line:
                    lines.append(' '.join(current_line))
                current_line = [word]
                current_length = len(word)
                
        if current_line:
            lines.append(' '.join(current_line))
            
        return lines


class InteractiveSetup:
    """Handle interactive setup with improved user experience."""
    
    @staticmethod
    def get_user_input(prompt: str, default: str = None, 
                      valid_options: List[str] = None, 
                      allow_empty: bool = False) -> str:
        """
        Get user input with validation and default handling.
        """
        if default:
            prompt += f" [{Fore.GREEN}{default}{Style.RESET_ALL}]"
        prompt += ": "
        
        while True:
            try:
                user_input = input(prompt).strip()
                
                # Remove quotes if present
                if user_input and len(user_input) >= 2:
                    if (user_input[0] == user_input[-1]) and user_input[0] in ['"', "'"]:
                        user_input = user_input[1:-1]
                        
                # Use default if empty
                if not user_input and default:
                    return default
                    
                # Validate against options
                if valid_options:
                    if user_input.lower() in [opt.lower() for opt in valid_options]:
                        return user_input.lower()
                    else:
                        print(f"{Fore.RED}Please choose one of: {', '.join(valid_options)}{Style.RESET_ALL}")
                        continue
                        
                # Check if empty input is allowed
                if user_input or allow_empty:
                    return user_input
                else:
                    print(f"{Fore.RED}This field is required.{Style.RESET_ALL}")
                    
            except KeyboardInterrupt:
                print(f"\n{Fore.YELLOW}Setup cancelled by user.{Style.RESET_ALL}")
                sys.exit(0)
                
    @staticmethod
    def validate_directory(path_str: str) -> Tuple[bool, Path, str]:
        """
        Validate directory path and provide helpful feedback.
        
        Returns:
            Tuple of (is_valid, resolved_path, message)
        """
        # Expand user home directory
        path_str = os.path.expanduser(path_str)
        
        # Handle Windows paths
        path_str = path_str.replace('\\', '/')
        
        try:
            path = Path(path_str).resolve()
            
            if not path.exists():
                return False, path, f"Directory does not exist: {path}"
                
            if not path.is_dir():
                return False, path, f"Path is not a directory: {path}"
                
            # Check for PDF files
            pdf_files = list(path.glob("*.pdf")) + list(path.glob("*.PDF"))
            pdf_files_recursive = list(path.rglob("*.pdf")) + list(path.rglob("*.PDF"))
            
            if not pdf_files_recursive:
                return False, path, "No PDF files found in directory or subdirectories"
                
            message = f"Found {len(pdf_files)} PDFs in main directory"
            if len(pdf_files_recursive) > len(pdf_files):
                message += f", {len(pdf_files_recursive) - len(pdf_files)} in subdirectories"
                
            return True, path, message
            
        except Exception as e:
            return False, None, f"Error processing path: {e}"
            
    @staticmethod
    def display_banner():
        """Display welcome banner."""
        print(f"\n{Fore.CYAN}{'=' * 70}{Style.RESET_ALL}")
        print(f"{Fore.CYAN}PDF TEXT SCANNER - Enhanced Version{Style.RESET_ALL}")
        print(f"{Fore.CYAN}{'=' * 70}{Style.RESET_ALL}")
        print("\nThis tool searches through PDF files for specified text")
        print("and extracts the paragraphs where that text appears.\n")
        
    @staticmethod
    def run_setup() -> Dict:
        """Run the complete interactive setup process."""
        InteractiveSetup.display_banner()
        
        # Get search text
        print(f"{Fore.YELLOW}STEP 1: Search Configuration{Style.RESET_ALL}")
        search_text = InteractiveSetup.get_user_input(
            "Enter the text to search for"
        )
        
        # Get PDF directory
        print(f"\n{Fore.YELLOW}STEP 2: Select PDF Directory{Style.RESET_ALL}")
        print("Enter the full path to your PDF directory.")
        print("Examples:")
        print("   /home/user/Documents/PDFs")
        print("   C:\\Users\\Name\\Documents")
        print("   ./pdfs")
        print("   ~/Downloads/Articles\n")
        
        while True:
            pdf_dir = InteractiveSetup.get_user_input(
                "Enter PDF directory path", "."
            )
            
            is_valid, path, message = InteractiveSetup.validate_directory(pdf_dir)
            
            if is_valid:
                print(f"{Fore.GREEN} {message}{Style.RESET_ALL}")
                pdf_dir = str(path)
                break
            else:
                print(f"{Fore.RED} {message}{Style.RESET_ALL}")
                
                # Offer to create directory if it doesn't exist
                if path and not path.exists():
                    create = InteractiveSetup.get_user_input(
                        "Create this directory?", "n", ["y", "n"]
                    )
                    if create == "y":
                        try:
                            path.mkdir(parents=True, exist_ok=True)
                            print(f"{Fore.GREEN} Directory created{Style.RESET_ALL}")
                            continue
                        except Exception as e:
                            print(f"{Fore.RED}Failed to create directory: {e}{Style.RESET_ALL}")
                            
                retry = InteractiveSetup.get_user_input(
                    "Try another directory?", "y", ["y", "n"]
                )
                if retry == "n":
                    print("Setup cancelled.")
                    sys.exit(0)
                    
        # Get output directory
        print(f"\n{Fore.YELLOW}STEP 3: Output Configuration{Style.RESET_ALL}")
        output_dir = InteractiveSetup.get_user_input(
            "Enter output directory for results", "results"
        )
        
        # Search options
        print(f"\n{Fore.YELLOW}STEP 4: Search Options{Style.RESET_ALL}")
        case_sensitive = InteractiveSetup.get_user_input(
            "Case-sensitive search?", "n", ["y", "n"]
        ) == "y"
        
        use_regex = InteractiveSetup.get_user_input(
            "Use regex pattern matching?", "n", ["y", "n"]
        ) == "y"
        
        # Check for subdirectories
        path = Path(pdf_dir)
        pdf_files_main = list(path.glob("*.pdf")) + list(path.glob("*.PDF"))
        pdf_files_all = list(path.rglob("*.pdf")) + list(path.rglob("*.PDF"))
        
        include_subdirs = False
        if len(pdf_files_all) > len(pdf_files_main):
            subdirs_count = len(set(f.parent for f in pdf_files_all)) - 1
            print(f"\n{Fore.CYAN} Subdirectory Detection{Style.RESET_ALL}")
            print(f"Main directory: {len(pdf_files_main)} PDFs")
            print(f"Subdirectories: {len(pdf_files_all) - len(pdf_files_main)} PDFs in {subdirs_count} folders")
            print(f"Total available: {len(pdf_files_all)} PDFs")
            
            include_subdirs = InteractiveSetup.get_user_input(
                "Include all subdirectories?", "y", ["y", "n"]
            ) == "y"
            
        # Output format options
        print(f"\n{Fore.YELLOW}STEP 5: Output Formats{Style.RESET_ALL}")
        print("Select which output formats to generate:")
        
        generate_txt = InteractiveSetup.get_user_input(
            "Generate detailed TXT report?", "y", ["y", "n"]
        ) == "y"
        
        generate_csv = InteractiveSetup.get_user_input(
            "Generate CSV for data analysis?", "y", ["y", "n"]
        ) == "y"
        
        generate_json = InteractiveSetup.get_user_input(
            "Generate JSON for programmatic access?", "n", ["y", "n"]
        ) == "y"
        
        generate_summary = InteractiveSetup.get_user_input(
            "Generate summary report?", "y", ["y", "n"]
        ) == "y"
        
        # Ensure at least one output format
        if not any([generate_txt, generate_csv, generate_json, generate_summary]):
            print(f"{Fore.YELLOW}No output formats selected. Defaulting to TXT.{Style.RESET_ALL}")
            generate_txt = True
            
        return {
            'search_text': search_text,
            'pdf_directory': pdf_dir,
            'output_directory': output_dir,
            'case_sensitive': case_sensitive,
            'use_regex': use_regex,
            'include_subdirs': include_subdirs,
            'generate_txt': generate_txt,
            'generate_csv': generate_csv,
            'generate_json': generate_json,
            'generate_summary': generate_summary
        }


def display_results_preview(scanner: PDFTextScanner):
    """Display a preview of results after scanning."""
    if not scanner.results:
        return
        
    print(f"\n{Fore.CYAN}RESULTS PREVIEW{Style.RESET_ALL}")
    print("=" * 70)
    
    # Group by file
    file_groups = defaultdict(list)
    for result in scanner.results:
        file_groups[result['filepath']].append(result)
        
    # Show up to 5 files
    for i, (filepath, occurrences) in enumerate(sorted(file_groups.items())[:5]):
        print(f"\n{Fore.GREEN}{filepath}{Style.RESET_ALL}")
        print(f"  Occurrences: {len(occurrences)}")
        print(f"  Pages: {', '.join(str(r['page']) for r in occurrences[:10])}")
        if len(occurrences) > 10:
            print(f"  ... and {len(occurrences) - 10} more")
            
    if len(file_groups) > 5:
        print(f"\n... and {len(file_groups) - 5} more files")
        

def main():
    """Main entry point with enhanced error handling."""
    try:
        # Get configuration
        config = InteractiveSetup.run_setup()
        
        # Display configuration summary
        print(f"\n{Fore.CYAN}{'=' * 70}{Style.RESET_ALL}")
        print(f"{Fore.CYAN}CONFIGURATION SUMMARY{Style.RESET_ALL}")
        print(f"{Fore.CYAN}{'=' * 70}{Style.RESET_ALL}")
        print(f"Search Text: '{config['search_text']}'")
        print(f"PDF Directory: {config['pdf_directory']}")
        print(f"Include Subdirectories: {'Yes (recursive)' if config['include_subdirs'] else 'No'}")
        print(f"Case Sensitive: {'Yes' if config['case_sensitive'] else 'No'}")
        print(f"Regex Mode: {'Yes' if config['use_regex'] else 'No'}")
        
        formats = []
        if config['generate_txt']: formats.append("TXT")
        if config['generate_csv']: formats.append("CSV")
        if config['generate_json']: formats.append("JSON")
        if config['generate_summary']: formats.append("Summary")
        print(f"Output Formats: {', '.join(formats)}")
        print(f"Output Directory: {config['output_directory']}")
        
        # Confirm
        proceed = InteractiveSetup.get_user_input(
            f"\n{Fore.YELLOW}Proceed with scan?{Style.RESET_ALL}", "y", ["y", "n"]
        )
        
        if proceed == "n":
            print("Scan cancelled.")
            return
            
        # Create scanner
        scanner = PDFTextScanner(
            config['search_text'],
            config['pdf_directory'],
            config['output_directory']
        )
        
        # Perform scan
        print(f"\n{Fore.CYAN}Starting scan...{Style.RESET_ALL}\n")
        start_time = time.time()
        
        scanner.scan_all_pdfs(
            case_sensitive=config['case_sensitive'],
            use_regex=config['use_regex'],
            include_subdirs=config['include_subdirs']
        )
        
        elapsed_time = time.time() - start_time
        
        # Save results
        if scanner.results:
            print(f"\n{Fore.CYAN}Saving results...{Style.RESET_ALL}")
            
            saved_files = []
            if config['generate_txt']:
                saved_files.append(scanner.save_txt_results())
            if config['generate_csv']:
                saved_files.append(scanner.save_csv_results())
            if config['generate_json']:
                saved_files.append(scanner.save_json_results())
            if config['generate_summary']:
                saved_files.append(scanner.generate_summary())
                
            # Display results preview
            display_results_preview(scanner)
            
            # Final summary
            print(f"\n{Fore.GREEN}{'=' * 70}{Style.RESET_ALL}")
            print(f"{Fore.GREEN}SCAN COMPLETE!{Style.RESET_ALL}")
            print(f"{Fore.GREEN}{'=' * 70}{Style.RESET_ALL}")
            print(f"Time elapsed: {elapsed_time:.1f} seconds")
            print(f"Files processed: {scanner.processed_files}")
            print(f"Total occurrences: {len(scanner.results)}")
            print(f"\nResults saved to: {scanner.output_directory}")
            for file in saved_files:
                print(f"   {file.name}")
                
        else:
            print(f"\n{Fore.YELLOW}No occurrences found.{Style.RESET_ALL}")
            
        if scanner.failed_files:
            print(f"\n{Fore.YELLOW}Warning: {len(scanner.failed_files)} files could not be processed.{Style.RESET_ALL}")
            print("Check the log file for details.")
            
    except KeyboardInterrupt:
        print(f"\n\n{Fore.YELLOW}Scan interrupted by user.{Style.RESET_ALL}")
        sys.exit(0)
    except Exception as e:
        print(f"\n{Fore.RED}An error occurred: {e}{Style.RESET_ALL}")
        print(f"Full error details:\n{traceback.format_exc()}")
        sys.exit(1)
    finally:
        print(f"\n{Fore.CYAN}Press Enter to exit...{Style.RESET_ALL}")
        input()


if __name__ == "__main__":
    main()
========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/protonizer.py
========================================

#!/usr/bin/env python3
"""
WebSum - Enhanced Photon Results Summarizer

This script:
1. Traverses through /home/jarvis/photon_results and its subfolders
2. Extracts URLs from text files in these folders
3. Fetches and extracts content from these URLs using BeautifulSoup
4. Summarizes content using improved algorithms:
   - Enhanced TextRank summarizer with NLP capabilities
   - Ollama models (local)
   - ChatGPT (OpenAI API)
   - Hugging Face models
5. Processes URLs in batches with intelligent theme recognition
6. Creates structured summaries with key themes, entities and metadata
7. Supports hierarchical summarization for better batch analysis
8. Saves comprehensive summary reports to summary.txt in each subfolder
"""

import os
import re
import json
import time
import argparse
import requests
import sys
import logging
import math
import heapq
import string
import nltk
from bs4 import BeautifulSoup
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import Counter, defaultdict
import hashlib
from datetime import datetime
from textwrap import dedent

# Try to download nltk data if it's not already available
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    print("Downloading NLTK punkt tokenizer...")
    nltk.download('punkt', quiet=True)

try:
    nltk.data.find('taggers/averaged_perceptron_tagger')
except LookupError:
    print("Downloading NLTK POS tagger...")
    nltk.download('averaged_perceptron_tagger', quiet=True)

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    print("Downloading NLTK stopwords...")
    nltk.download('stopwords', quiet=True)

__version__ = "2.0"

# Default configuration
PHOTON_ROOT = "/home/jarvis/photon_results"
BATCH_SIZE = 10
DEFAULT_OLLAMA_API_URL = "http://localhost:11434/api/generate"
DEFAULT_OPENAI_API_URL = "https://api.openai.com/v1/chat/completions"
DEFAULT_HF_API_URL = "https://api-inference.huggingface.co/models/"
DEFAULT_TIMEOUT = 90  # seconds
MAX_CONTENT_LENGTH = 8000  # characters

# Default API keys (can be overridden by environment variables or user input)
DEFAULT_OPENAI_API_KEY = ""
DEFAULT_HF_API_KEY = ""

# Regular expression for extracting URLs
URL_PATTERN = re.compile(r'https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+(?:/[^)\s]*)?')

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class EnhancedTextRankSummarizer:
    """Enhanced implementation of TextRank algorithm for text summarization with advanced NLP features"""

    def __init__(self):
        # Enhanced stop words list with more common English stopwords
        self.stop_words = self._load_stop_words()

        # Initialize NLP components
        self._init_nlp()

        # Keep track of extracted entities for summarization
        self.entities = []

        # Track content topics and categories
        self.content_category = None
        self.topic_keywords = []

    def _load_stop_words(self):
        """Load an expanded set of stop words for better filtering"""
        # Core stop words from NLTK if available
        try:
            from nltk.corpus import stopwords
            stop_words = set(stopwords.words('english'))
        except (ImportError, LookupError):
            # Fallback to basic stop words
            stop_words = set([
                'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from',
                'has', 'he', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the',
                'to', 'was', 'were', 'will', 'with', 'about', 'above', 'after',
                'again', 'all', 'am', 'any', 'because', 'been', 'before', 'being',
                'below', 'between', 'both', 'but', 'can', 'did', 'do', 'does',
                'doing', 'down', 'during', 'each', 'few', 'further', 'had', 'has',
                'have', 'having', 'here', 'how', 'i', 'if', 'into', 'just', 'me',
                'more', 'most', 'my', 'no', 'nor', 'not', 'now', 'of', 'off',
                'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'out', 'over',
                'own', 'same', 'so', 'some', 'such', 'than', 'that', 'the', 'their',
                'theirs', 'them', 'then', 'there', 'these', 'they', 'this', 'those',
                'through', 'to', 'too', 'under', 'until', 'up', 'very', 'we', 'what',
                'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will',
                'with', 'would', 'you', 'your', 'yours'
            ])

        # Add common web content words that aren't useful for summarization
        web_specific_stops = {
            'click', 'subscribe', 'comment', 'share', 'like', 'follow', 'website',
            'cookie', 'privacy', 'policy', 'terms', 'conditions', 'rights',
            'reserved', 'copyright', 'site', 'http', 'https', 'www', 'html',
            'login', 'sign', 'menu', 'search', 'home', 'page', 'contact'
        }

        stop_words.update(web_specific_stops)
        return stop_words

    def _init_nlp(self):
        """Initialize NLP components for advanced analysis"""
        self.use_advanced_nlp = False

        try:
            # Check for additional NLTK packages
            try:
                nltk.data.find('corpora/wordnet')
            except LookupError:
                nltk.download('wordnet', quiet=True)

            try:
                nltk.data.find('chunkers/maxent_ne_chunker')
                nltk.data.find('corpora/words')
            except LookupError:
                nltk.download('maxent_ne_chunker', quiet=True)
                nltk.download('words', quiet=True)

            # Try to import NLTK components for sentiment analysis
            try:
                nltk.data.find('sentiment/vader_lexicon')
            except LookupError:
                nltk.download('vader_lexicon', quiet=True)

            from nltk.stem import WordNetLemmatizer
            self.lemmatizer = WordNetLemmatizer()

            self.use_advanced_nlp = True
            logger.debug("Advanced NLP features enabled")
        except Exception as e:
            logger.debug(f"Advanced NLP features not available: {e}")
            self.use_advanced_nlp = False

    def _clean_text(self, text):
        """Enhanced text cleaning with lemmatization and normalization"""
        if not text:
            return ""

        # Basic cleaning
        text = re.sub(r'\s+', ' ', text)  # Replace multiple spaces with single space
        text = text.lower().strip()

        if self.use_advanced_nlp:
            try:
                # Tokenize and lemmatize words
                words = nltk.word_tokenize(text)
                pos_tags = nltk.pos_tag(words)

                # Convert POS tags to WordNet format for lemmatization
                cleaned_words = []
                for word, tag in pos_tags:
                    # Skip punctuation and stopwords
                    if word in string.punctuation or word in self.stop_words:
                        continue

                    # Convert POS tag to WordNet format
                    if tag.startswith('J'):
                        wordnet_pos = 'a'  # adjective
                    elif tag.startswith('V'):
                        wordnet_pos = 'v'  # verb
                    elif tag.startswith('N'):
                        wordnet_pos = 'n'  # noun
                    elif tag.startswith('R'):
                        wordnet_pos = 'r'  # adverb
                    else:
                        wordnet_pos = 'n'  # default to noun

                    # Lemmatize the word
                    lemma = self.lemmatizer.lemmatize(word, pos=wordnet_pos)

                    # Only add words longer than 2 characters
                    if len(lemma) > 2:
                        cleaned_words.append(lemma)

                return ' '.join(cleaned_words)
            except Exception as e:
                logger.debug(f"Advanced text cleaning failed: {e}")
                # Fall back to basic cleaning
                return ' '.join([word for word in text.split()
                                if word not in self.stop_words
                                and word not in string.punctuation
                                and len(word) > 2])
        else:
            # Basic cleaning without advanced NLP
            return ' '.join([word for word in text.split()
                            if word not in self.stop_words
                            and word not in string.punctuation
                            and len(word) > 2])

    def _extract_sentences(self, text):
        """Extract sentences with improved boundary detection"""
        if not text:
            return []

        # Handle common abbreviations to prevent incorrect sentence splits
        text = re.sub(r'(?i)(\s|^)(mr\.|mrs\.|ms\.|dr\.|prof\.|inc\.|ltd\.|sr\.|jr\.)',
                      lambda m: m.group(1) + m.group(2).replace('.', '[DOT]'), text)

        # Extract sentences using NLTK if available
        try:
            sentences = nltk.sent_tokenize(text)
        except:
            # Fallback to regex-based sentence extraction
            sentences = re.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?|\!)\s', text)

        # Restore the original periods in abbreviations
        sentences = [s.replace('[DOT]', '.') for s in sentences]

        # Filter out empty sentences and those that are too short
        filtered_sentences = []
        for s in sentences:
            s = s.strip()
            if s and len(s.split()) > 3:  # Skip sentences with 3 or fewer words
                filtered_sentences.append(s)

        return filtered_sentences

    def _extract_entities(self, text):
        """Extract named entities for identifying important content elements"""
        entities = {"PERSON": [], "ORGANIZATION": [], "LOCATION": [], "DATE": [], "OTHER": []}

        if not self.use_advanced_nlp or not text:
            return entities

        try:
            sentences = nltk.sent_tokenize(text)
            for sentence in sentences:
                words = nltk.word_tokenize(sentence)
                tagged = nltk.pos_tag(words)
                named_entities = nltk.ne_chunk(tagged)

                for chunk in named_entities:
                    if hasattr(chunk, 'label'):
                        entity_text = ' '.join([word for word, tag in chunk.leaves()])
                        entity_type = chunk.label()

                        # Map to simplified entity types
                        if entity_type in ['PERSON']:
                            entities["PERSON"].append(entity_text)
                        elif entity_type in ['ORGANIZATION', 'GPE']:
                            entities["ORGANIZATION"].append(entity_text)
                        elif entity_type in ['LOCATION', 'GPE']:
                            entities["LOCATION"].append(entity_text)
                        elif entity_type in ['DATE', 'TIME']:
                            entities["DATE"].append(entity_text)
                        else:
                            entities["OTHER"].append(entity_text)
        except Exception as e:
            logger.debug(f"Entity extraction failed: {e}")

        # Remove duplicates and keep the most frequent entities
        for entity_type in entities:
            if entities[entity_type]:
                counter = Counter(entities[entity_type])
                entities[entity_type] = [item for item, count in counter.most_common(5)]

        # Store for later use in summarization
        self.entities = entities
        return entities

    def _compute_word_frequencies(self, text):
        """Calculate word importance using TF-IDF-like weighting"""
        if not text:
            return Counter()

        # Clean the text first
        clean_text = self._clean_text(text)

        # Split into words and count frequencies
        word_freq = Counter(clean_text.split())

        # Apply TF-IDF-like weighting to reduce common word importance
        total_words = sum(word_freq.values())
        unique_words = len(word_freq)

        if unique_words == 0:
            return Counter()

        # Calculate average frequency
        avg_freq = total_words / unique_words

        # Apply dampening to very common words to prevent them from dominating
        for word in list(word_freq.keys()):
            if word_freq[word] > 2 * avg_freq:
                word_freq[word] = 2 * avg_freq + math.log(word_freq[word] - 2 * avg_freq + 1)

        # Normalize frequencies
        max_freq = max(word_freq.values()) if word_freq else 1
        for word in word_freq:
            word_freq[word] = word_freq[word] / max_freq

        return word_freq

    def _build_sentence_graph(self, sentences, word_freq):
        """Build a graph of sentence similarities for PageRank algorithm"""
        n = len(sentences)
        similarity_matrix = [[0.0 for _ in range(n)] for _ in range(n)]

        for i in range(n):
            # Skip very short sentences
            if len(sentences[i].split()) <= 3:
                continue

            # Clean and extract words from sentence i
            sentence_i_words = set(self._clean_text(sentences[i]).split())

            for j in range(i+1, n):  # Only calculate upper triangle to avoid duplication
                # Skip very short sentences
                if len(sentences[j].split()) <= 3:
                    continue

                # Clean and extract words from sentence j
                sentence_j_words = set(self._clean_text(sentences[j]).split())

                # Calculate intersection
                intersection = sentence_i_words.intersection(sentence_j_words)

                # Skip if no common words
                if not intersection:
                    continue

                # Calculate weighted similarity based on word importance
                similarity = 0.0
                for word in intersection:
                    if word in word_freq:
                        similarity += word_freq[word]

                # Normalize by the lengths to favor sentences with more information overlap
                norm = math.log(1 + len(sentence_i_words) + len(sentence_j_words))
                if norm > 0:
                    similarity = similarity / norm

                # Apply a threshold to remove very weak connections
                if similarity > 0.05:
                    similarity_matrix[i][j] = similarity
                    similarity_matrix[j][i] = similarity  # Make the matrix symmetric

        return similarity_matrix

    def _apply_page_rank(self, similarity_matrix, damping=0.85, max_iterations=50, tolerance=1e-6):
        """Apply the PageRank algorithm to rank sentences by importance"""
        n = len(similarity_matrix)

        if n == 0:
            return {}

        # Initialize PageRank scores
        scores = [1.0 / n] * n

        # Create column-normalized transition matrix for faster convergence
        transition_matrix = []
        for j in range(n):
            column_sum = sum(similarity_matrix[i][j] for i in range(n))
            if column_sum == 0:
                transition_matrix.append([0.0] * n)
            else:
                transition_matrix.append([similarity_matrix[i][j] / column_sum for i in range(n)])

        # Iterative PageRank computation
        for _ in range(max_iterations):
            new_scores = [(1 - damping) / n] * n

            for i in range(n):
                for j in range(n):
                    if similarity_matrix[j][i] > 0:
                        new_scores[i] += damping * scores[j] * transition_matrix[i][j]

            # Check for convergence
            score_diff = sum(abs(new_scores[i] - scores[i]) for i in range(n))
            if score_diff < tolerance:
                break

            scores = new_scores

        # Return scores as a dictionary
        return {i: scores[i] for i in range(n)}

    def _score_sentences(self, sentences, word_freq):
        """Score sentences using PageRank and semantic enhancements"""
        # For very short texts, use a simpler approach
        if len(sentences) <= 3:
            return {i: 1.0 for i in range(len(sentences))}

        # Build sentence similarity graph
        similarity_matrix = self._build_sentence_graph(sentences, word_freq)

        # Apply PageRank algorithm
        scores = self._apply_page_rank(similarity_matrix)

        # Apply position-based heuristics
        if len(sentences) > 2:
            # First paragraph sentences often contain important context
            for i in range(min(3, len(sentences))):
                if i in scores:
                    scores[i] *= 1.25

            # Last few sentences often contain conclusions
            for i in range(max(0, len(sentences) - 3), len(sentences)):
                if i in scores:
                    scores[i] *= 1.15

        # Apply entity-based importance
        all_entities = []
        for entity_type in self.entities:
            all_entities.extend(self.entities[entity_type])

        if all_entities:
            for i, sentence in enumerate(sentences):
                # Count entities in this sentence
                entity_matches = sum(1 for entity in all_entities if entity.lower() in sentence.lower())
                if entity_matches > 0 and i in scores:
                    scores[i] *= (1 + 0.1 * min(entity_matches, 3))  # Boost based on entity count

        # Apply length-based normalization (penalize very short or very long sentences)
        for i, sentence in enumerate(sentences):
            words = sentence.split()
            word_count = len(words)

            if i in scores:
                # Penalize extremely short sentences
                if word_count <= 5:
                    scores[i] *= 0.7
                # Penalize overly long sentences that might be hard to read
                elif word_count > 40:
                    scores[i] *= 0.8

        return scores

    def _analyze_sentiment(self, text):
        """Analyze the sentiment of text to provide tone information"""
        if not text or not self.use_advanced_nlp:
            return {"sentiment": "neutral", "score": 0.0}

        try:
            from nltk.sentiment import SentimentIntensityAnalyzer
            sia = SentimentIntensityAnalyzer()
            sentiment_scores = sia.polarity_scores(text)

            # Determine overall sentiment
            compound_score = sentiment_scores['compound']
            if compound_score >= 0.05:
                sentiment = "positive"
            elif compound_score <= -0.05:
                sentiment = "negative"
            else:
                sentiment = "neutral"

            return {
                "sentiment": sentiment,
                "score": compound_score,
                "details": sentiment_scores
            }
        except Exception as e:
            logger.debug(f"Sentiment analysis failed: {e}")
            return {"sentiment": "neutral", "score": 0.0}

    def _detect_category(self, text):
        """Detect the category of content for topical organization"""
        if not text:
            return {"category": "unknown", "confidence": 0.0}

        # Define category keywords
        categories = {
            "technology": ['software', 'hardware', 'programming', 'code', 'algorithm', 'data', 'internet',
                         'website', 'app', 'digital', 'tech', 'computer', 'artificial intelligence',
                         'machine learning', 'developers', 'cybersecurity', 'cloud', 'automation'],

            "business": ['company', 'business', 'market', 'stock', 'investment', 'profit', 'revenue',
                        'sales', 'customer', 'strategy', 'startup', 'entrepreneur', 'industry',
                        'commerce', 'corporate', 'management', 'finance', 'economic'],

            "health": ['health', 'medical', 'doctor', 'patient', 'hospital', 'disease', 'treatment',
                      'therapy', 'medicine', 'wellness', 'fitness', 'nutrition', 'mental health',
                      'healthcare', 'diagnosis', 'symptoms', 'surgery', 'clinical'],

            "news": ['news', 'report', 'update', 'latest', 'breaking', 'headlines', 'current events',
                    'world', 'national', 'announcement', 'press release', 'journalist', 'media'],

            "politics": ['government', 'political', 'policy', 'election', 'vote', 'campaign', 'democrat',
                        'republican', 'law', 'legislation', 'congress', 'senate', 'parliament',
                        'president', 'politician', 'party', 'administration'],

            "education": ['education', 'school', 'university', 'college', 'student', 'teacher',
                         'learning', 'academic', 'study', 'research', 'knowledge', 'curriculum',
                         'classroom', 'professor', 'course', 'degree', 'lecture'],

            "science": ['science', 'scientific', 'research', 'study', 'experiment', 'theory',
                       'discovery', 'biology', 'chemistry', 'physics', 'astronomy', 'psychology',
                       'neuroscience', 'environmental', 'ecology', 'laboratory'],

            "entertainment": ['entertainment', 'movie', 'film', 'television', 'music', 'celebrity',
                            'actor', 'actress', 'director', 'artist', 'game', 'show', 'performance',
                            'theater', 'concert', 'streaming', 'media']
        }

        # Count category keywords in the text
        text_lower = text.lower()
        category_scores = {category: 0 for category in categories}

        for category, keywords in categories.items():
            for keyword in keywords:
                count = len(re.findall(r'\b' + re.escape(keyword) + r'\b', text_lower))
                category_scores[category] += count

        # Get total keyword matches
        total_matches = sum(category_scores.values())

        # If no matches, return unknown
        if total_matches == 0:
            return {"category": "unknown", "confidence": 0.0}

        # Find category with highest count
        top_category = max(category_scores.items(), key=lambda x: x[1])

        # Calculate confidence score (ratio of top category matches to total matches)
        confidence = top_category[1] / total_matches

        self.content_category = {
            "category": top_category[0],
            "confidence": confidence,
            "distribution": {k: v/total_matches for k, v in category_scores.items() if v > 0}
        }

        return self.content_category

    def extract_keywords(self, text, max_keywords=8):
        """Extract the most important keywords and keyphrases from text"""
        if not text:
            return []

        # Get word frequencies
        word_freq = self._compute_word_frequencies(text)

        # Extract single-word keywords
        keywords = [(word, score) for word, score in word_freq.items()
                   if len(word) > 3 and score > 0.3]

        # Extract multi-word keyphrases if advanced NLP is available
        keyphrases = []
        if self.use_advanced_nlp:
            try:
                # Extract noun phrases using POS tagging
                sentences = nltk.sent_tokenize(text)
                for sentence in sentences:
                    words = nltk.word_tokenize(sentence)
                    tagged = nltk.pos_tag(words)

                    # Find noun phrases (adjective + noun or noun + noun)
                    i = 0
                    while i < len(tagged) - 1:
                        # Adjective + Noun pattern
                        if tagged[i][1].startswith('JJ') and tagged[i+1][1].startswith('NN'):
                            phrase = f"{tagged[i][0]} {tagged[i+1][0]}".lower()
                            if all(word not in self.stop_words for word in phrase.split()):
                                # Calculate score based on constituent words
                                score = sum(word_freq.get(word, 0) for word in phrase.split()) / 2
                                keyphrases.append((phrase, score))

                        # Noun + Noun pattern (compound nouns)
                        elif tagged[i][1].startswith('NN') and tagged[i+1][1].startswith('NN'):
                            phrase = f"{tagged[i][0]} {tagged[i+1][0]}".lower()
                            if all(word not in self.stop_words for word in phrase.split()):
                                # Calculate score based on constituent words
                                score = sum(word_freq.get(word, 0) for word in phrase.split()) / 2
                                keyphrases.append((phrase, score))

                        i += 1
            except Exception as e:
                logger.debug(f"Keyphrase extraction failed: {e}")

        # Combine keywords and keyphrases, remove duplicates
        all_terms = {}
        for term, score in keywords + keyphrases:
            if term not in all_terms or score > all_terms[term]:
                all_terms[term] = score

        # Add named entities with high scores
        for entity_type in self.entities:
            for entity in self.entities[entity_type]:
                if entity.lower() not in all_terms:
                    all_terms[entity.lower()] = 0.9  # High score for named entities

        # Convert to list and sort by score
        terms_list = [(term, score) for term, score in all_terms.items()]
        terms_list.sort(key=lambda x: x[1], reverse=True)

        # Store for later use
        self.topic_keywords = [term for term, _ in terms_list[:max_keywords]]

        # Return the top terms
        return terms_list[:max_keywords]

    def summarize(self, text, ratio=0.2, min_sentences=None, max_sentences=None):
        """
        Generate a comprehensive summary with metadata

        Parameters:
        - text: Input text to summarize
        - ratio: Target summary length ratio (0.0 to 1.0)
        - min_sentences: Minimum number of sentences to include
        - max_sentences: Maximum number of sentences to include

        Returns:
        - Dictionary with summary text and metadata
        """
        if not text or len(text.strip()) == 0:
            return {
                "summary": "",
                "metadata": {"error": "No content to summarize"}
            }

        # Record original text length
        original_text_length = len(text.split())

        # Extract named entities for context
        self._extract_entities(text)

        # Extract sentences
        sentences = self._extract_sentences(text)

        # Handle very short texts
        if len(sentences) <= 3:
            return {
                "summary": text,
                "metadata": {
                    "original_length": original_text_length,
                    "summary_length": original_text_length,
                    "compression_ratio": 1.0,
                    "sentence_count": len(sentences),
                    "reading_time_min": max(1, round(original_text_length / 200))
                }
            }

        # Calculate word frequencies for importance weighting
        word_freq = self._compute_word_frequencies(text)

        # Score sentences using PageRank and enhancements
        sentence_scores = self._score_sentences(sentences, word_freq)

        # Calculate number of sentences to include in the summary
        if min_sentences is not None and max_sentences is not None:
            target_count = max(min_sentences, min(max_sentences, int(len(sentences) * ratio)))
        elif min_sentences is not None:
            target_count = max(min_sentences, int(len(sentences) * ratio))
        elif max_sentences is not None:
            target_count = min(max_sentences, int(len(sentences) * ratio))
        else:
            target_count = max(1, int(len(sentences) * ratio))

        # Get the highest-scoring sentences
        top_indices = heapq.nlargest(target_count, sentence_scores, key=sentence_scores.get)

        # Sort indices to maintain original sentence order
        top_indices.sort()

        # Create the summary from selected sentences
        summary_sentences = [sentences[i] for i in top_indices]
        summary_text = ' '.join(summary_sentences)

        # Calculate metadata
        summary_length = len(summary_text.split())
        compression_ratio = summary_length / original_text_length if original_text_length > 0 else 1.0

        # Get additional analytics
        sentiment = self._analyze_sentiment(text)
        category = self._detect_category(text)
        keywords = self.extract_keywords(text)

        # Create metadata object
        metadata = {
            "original_length": original_text_length,
            "summary_length": summary_length,
            "compression_ratio": compression_ratio,
            "sentence_count": len(sentences),
            "summary_sentence_count": len(summary_sentences),
            "reading_time_min": max(1, round(original_text_length / 200)),
            "summary_reading_time_min": max(1, round(summary_length / 200)),
            "sentiment": sentiment["sentiment"],
            "category": category["category"] if category["confidence"] > 0.3 else "general",
            "keywords": [kw for kw, _ in keywords[:5]],
            "entities": {k: v for k, v in self.entities.items() if v}
        }

        return {
            "summary": summary_text,
            "metadata": metadata
        }

def setup_argparse():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Extract, fetch, and summarize URLs from Photon results with enhanced NLP features",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument("--root-dir", type=str, default=PHOTON_ROOT,
                        help="Root directory containing Photon results")
    parser.add_argument("--batch-size", type=int, default=BATCH_SIZE,
                        help="Number of URLs to process in each batch")
    parser.add_argument("--timeout", type=int, default=DEFAULT_TIMEOUT,
                        help="Timeout for HTTP requests and API calls (seconds)")
    parser.add_argument("--ollama-url", type=str, default=DEFAULT_OLLAMA_API_URL,
                        help="Ollama API endpoint URL")
    parser.add_argument("--max-content", type=int, default=MAX_CONTENT_LENGTH,
                        help="Maximum content length to send for summarization")
    parser.add_argument("--verbose", action="store_true",
                        help="Enable verbose logging")
    parser.add_argument("--debug", action="store_true",
                        help="Enable debug mode with additional information")
    parser.add_argument("--auto", action="store_true",
                        help="Start in automatic mode (no prompts between batches)")
    parser.add_argument("--format", choices=["text", "markdown", "json"], default="markdown",
                        help="Output format for summaries")
    parser.add_argument("--depth", choices=["short", "medium", "detailed"], default="medium",
                        help="Depth of summaries to generate")

    return parser.parse_args()

def configure_logging(verbose, debug):
    """Configure logging level based on command line arguments."""
    if debug:
        logger.setLevel(logging.DEBUG)
    elif verbose:
        logger.setLevel(logging.INFO)
    else:
        logger.setLevel(logging.WARNING)

def print_header(text):
    """Print a formatted header."""
    try:
        width = min(os.get_terminal_size().columns, 80)
    except OSError:
        width = 80
    print(f"\n\033[1;36m{'=' * width}\n{text.center(width)}\n{'=' * width}\033[0m\n")

def extract_urls_from_file(file_path):
    """Extract URLs from a file using regex."""
    try:
        with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
            content = f.read()

        # Find all URLs in the content
        urls = URL_PATTERN.findall(content)

        # Clean and deduplicate URLs
        clean_urls = []
        seen = set()
        for url in urls:
            # Clean up the URL to remove trailing punctuation
            url = url.rstrip('.,;:\'\"!?)')

            # Skip duplicates
            if url not in seen:
                seen.add(url)
                clean_urls.append(url)

        return clean_urls
    except Exception as e:
        logger.error(f"Error reading {file_path}: {e}")
        return []

def get_domain(url):
    """Extract domain from URL."""
    parsed = urlparse(url)
    domain = parsed.netloc
    if domain.startswith('www.'):
        domain = domain[4:]
    return domain

def fetch_webpage_content(url, timeout=DEFAULT_TIMEOUT):
    """Fetch and extract text content from a webpage with improved parsing."""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml",
            "Accept-Language": "en-US,en;q=0.9"
        }

        # Make the request
        response = requests.get(url, timeout=timeout, headers=headers, allow_redirects=True)

        # Handle common error cases
        if response.status_code == 404:
            logger.warning(f"404 Not Found: {url}")
            return {
                "title": "404 Not Found",
                "text": "",
                "url": url,
                "status_code": 404,
                "error": True
            }

        if response.status_code >= 400:
            logger.warning(f"HTTP Error {response.status_code}: {url}")
            return {
                "title": f"HTTP Error {response.status_code}",
                "text": "",
                "url": url,
                "status_code": response.status_code,
                "error": True
            }

        # Check content type
        content_type = response.headers.get('Content-Type', '').lower()
        if 'text/html' not in content_type and 'application/json' not in content_type:
            return {
                "title": f"Unsupported content type: {content_type}",
                "text": "",
                "url": url,
                "status_code": response.status_code,
                "error": True
            }

        # Parse HTML with BeautifulSoup
        soup = BeautifulSoup(response.content, "html.parser")

        # Extract title
        title_tag = soup.find('title')
        title = title_tag.get_text(strip=True) if title_tag else "No title found"

        # Remove unwanted elements
        for tag in soup(["script", "style", "nav", "footer", "header", "aside", "form",
                         "meta", "button", "svg", "iframe", "noscript"]):
            tag.extract()

        # Extract meta description for context
        meta_description = ""
        meta_desc_tag = soup.find("meta", attrs={"name": "description"})
        if meta_desc_tag:
            meta_description = meta_desc_tag.get("content", "")

        # Try different content extraction strategies
        content_text = ""

        # Strategy 1: Look for article or main content tag
        main_content = soup.find(["article", "main", "section", "div"],
                                class_=lambda c: c and any(s in str(c).lower() for s in
                                                     ["content", "article", "main", "body", "post"]))
        if main_content:
            # Extract paragraphs from main content
            paragraphs = main_content.find_all("p")
            if paragraphs:
                content_text = "\n\n".join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])

        # Strategy 2: If no content found, try all paragraphs
        if not content_text:
            paragraphs = soup.find_all("p")
            if paragraphs:
                content_text = "\n\n".join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])

        # Strategy 3: Fallback to general text extraction
        if not content_text:
            # Look for content in specific divs or sections
            content_containers = soup.select("article, .content, .main, #content, #main, .post, .entry")
            if content_containers:
                content_text = "\n\n".join([el.get_text(strip=True) for el in content_containers])
            else:
                # Last resort: extract all text
                content_text = soup.get_text(separator="\n\n", strip=True)

        # Clean up extracted text
        content_text = re.sub(r'\n{3,}', '\n\n', content_text)  # Remove excess newlines
        content_text = re.sub(r'\s{2,}', ' ', content_text)     # Remove excess spaces

        # Add meta description to the beginning if available
        if meta_description and meta_description not in content_text:
            content_text = meta_description + "\n\n" + content_text

        # Return content with success status
        return {
            "title": title,
            "text": content_text,
            "url": url,
            "status_code": response.status_code,
            "error": False
        }
    except Exception as e:
        logger.error(f"Error fetching {url}: {str(e)}")
        return {
            "title": f"Error: {str(e)[:100]}",
            "text": "",
            "url": url,
            "status_code": 0,  # Use 0 to indicate a connection/non-HTTP error
            "error": True
        }

def check_ollama_connection(url, timeout=5):
    """Check if Ollama server is running and reachable."""
    try:
        base_url = url
        if "/api/generate" in url:
            base_url = url.split("/api/generate")[0]

        response = requests.get(f"{base_url}/api/tags", timeout=timeout)
        response.raise_for_status()

        # Check if we got a valid response with models
        data = response.json()
        if "models" in data and len(data["models"]) > 0:
            logger.debug(f"Available Ollama models: {[m['name'] for m in data['models']]}")
            return True
        else:
            logger.warning("No models found in Ollama")
            return False
    except Exception as e:
        logger.error(f"Ollama connection failed: {e}")
        return False

def create_spinner():
    """Create a simple spinner to indicate progress during API calls."""
    import itertools
    import threading

    spinner_active = [True]
    spinner_thread = None

    def spin():
        for c in itertools.cycle('|/-\\'):
            if not spinner_active[0]:
                break
            sys.stdout.write(f"\r{c} ")
            sys.stdout.flush()
            time.sleep(0.1)
        sys.stdout.write('\r')
        sys.stdout.flush()

    spinner_thread = threading.Thread(target=spin)
    spinner_thread.daemon = True
    spinner_thread.start()

    def stop_spinner():
        spinner_active[0] = False
        if spinner_thread:
            spinner_thread.join(0.5)

    return stop_spinner

def summarize_with_enhanced_textrank(content, depth, timeout=DEFAULT_TIMEOUT):
    """Generate comprehensive summary using enhanced TextRank algorithm."""
    text = content["text"]
    if not text:
        return "No content to summarize."

    print("Generating enhanced summary with TextRank...", end="", flush=True)
    stop_spinner = create_spinner()

    try:
        # Create enhanced TextRank summarizer
        summarizer = EnhancedTextRankSummarizer()

        # Configure parameters based on desired depth
        if depth == "short":
            summary_params = {
                "ratio": 0.05,  # Very concise summary
                "min_sentences": 1,
                "max_sentences": 3
            }
        elif depth == "medium":
            summary_params = {
                "ratio": 0.15,  # Balanced summary
                "min_sentences": 3,
                "max_sentences": 7
            }
        else:  # detailed
            summary_params = {
                "ratio": 0.25,  # Comprehensive summary
                "min_sentences": 5,
                "max_sentences": 15
            }

        # Generate the summary with metadata
        result = summarizer.summarize(text, **summary_params)
        summary = result["summary"]
        metadata = result["metadata"]

        # Handle empty summary (can happen with poor quality content)
        if not summary:
            if len(text) > 200:
                summary = text[:200] + "..."
            else:
                summary = text

            stop_spinner()
            print(" Done! (Used original text due to quality issues)")
            return summary

        # Add context and metadata based on depth
        if depth in ["medium", "detailed"]:
            # Extract keywords
            keywords = summarizer.extract_keywords(text)
            if keywords:
                key_terms = ", ".join([term for term, _ in keywords[:5]])
                summary += f"\n\nKey terms: {key_terms}"

            # Add category for medium and detailed summaries
            if "category" in metadata and metadata["category"] != "general":
                summary += f"\nTopic: {metadata['category'].capitalize()}"

            # Add additional metadata for detailed summaries
            if depth == "detailed":
                # Add sentiment information
                if "sentiment" in metadata and metadata["sentiment"] != "neutral":
                    summary += f"\nTone: {metadata['sentiment'].capitalize()}"

                # Add reading time
                if "reading_time_min" in metadata:
                    summary += f"\nOriginal reading time: ~{metadata['reading_time_min']} min"

                # Add named entities if available
                entities = []
                if "entities" in metadata:
                    for entity_type, values in metadata["entities"].items():
                        if values and entity_type in ["PERSON", "ORGANIZATION", "LOCATION"]:
                            entity_str = ", ".join(values[:3])
                            entities.append(f"{entity_type.capitalize()}: {entity_str}")

                if entities:
                    summary += "\n\nNamed entities: " + "; ".join(entities)

        stop_spinner()
        print(" Done!")
        return summary
    except Exception as e:
        stop_spinner()
        print(f" Error: {str(e)[:100]}")
        logger.error(f"Enhanced TextRank summarization error: {e}")
        return f"Error generating summary: {str(e)[:100]}"

def summarize_with_ollama(content, model, depth, api_url=DEFAULT_OLLAMA_API_URL, timeout=DEFAULT_TIMEOUT):
    """Summarize content using Ollama API with improved prompt engineering."""
    # Truncate content if too long
    text = content["text"]
    if len(text) > MAX_CONTENT_LENGTH:
        text = text[:MAX_CONTENT_LENGTH] + "... [truncated]"

    # Build advanced prompt based on depth
    if depth == "short":
        prompt = dedent(f"""
        Create a concise 1-2 sentence summary of the following content.
        Focus on the core information and key points only.
        Maintain factual accuracy and exclude your own opinions.

        CONTENT:
        {text}

        SUMMARY:
        """)
    elif depth == "medium":
        prompt = dedent(f"""
        Summarize the following content in a single coherent paragraph (3-5 sentences).
        Include the main points, key details, and important context.
        Be factual, objective, and precise. Synthesize the information rather than just
        shortening the text.

        CONTENT:
        {text}

        SUMMARY:
        """)
    else:  # detailed
        prompt = dedent(f"""
        Create a comprehensive summary of the following content. Include:
        1. Main ideas and key supporting points (5-7 sentences)
        2. Important details, facts, and figures
        3. Key terms/concepts with brief explanations
        4. Overall topic/theme
        5. Tone or perspective of the content

        Be factual, objective, and well-structured.

        CONTENT:
        {text}

        DETAILED SUMMARY:
        """)

    print("Generating summary with Ollama...", end="", flush=True)
    stop_spinner = create_spinner()

    try:
        response = requests.post(
            api_url,
            json={
                "model": model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.1,  # Low temperature for factual summaries
                    "top_p": 0.95        # High precision
                }
            },
            timeout=timeout
        )
        response.raise_for_status()
        result = response.json()
        summary = result.get("response", "No summary generated.")

        stop_spinner()
        print(" Done!")
        return summary.strip()
    except Exception as e:
        stop_spinner()
        print(f" Error: {str(e)[:100]}")
        logger.error(f"Ollama API error: {e}")
        return f"Error generating summary: {str(e)[:100]}"

def summarize_with_openai(content, api_key, depth, model="gpt-3.5-turbo", timeout=DEFAULT_TIMEOUT):
    """Summarize content using OpenAI API with improved prompts."""
    text = content["text"]
    title = content["title"]

    if len(text) > MAX_CONTENT_LENGTH:
        text = text[:MAX_CONTENT_LENGTH] + "... [truncated]"

    # Create system prompt based on depth
    if depth == "short":
        system_prompt = "You are an expert summarizer. Create a concise 1-2 sentence summary capturing only the essential information. Be objective and factual."
    elif depth == "medium":
        system_prompt = "You are an expert summarizer. Create a single coherent paragraph (3-5 sentences) that captures the main points and key context. Be objective and factual."
    else:  # detailed
        system_prompt = "You are an expert summarizer. Create a comprehensive, well-structured summary that includes main ideas, important details, key concepts, and overall theme. Be objective, factual, and informative."

    # Create user prompt with title for context
    user_prompt = f"Title: {title}\n\nContent to summarize:\n{text}"

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }

    data = {
        "model": model,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        "temperature": 0.3,  # Lower temperature for more focused summaries
        "max_tokens": 1024 if depth == "detailed" else 512  # Adjust token limit based on depth
    }

    print(f"Generating summary with {model}...", end="", flush=True)
    stop_spinner = create_spinner()

    try:
        response = requests.post(
            DEFAULT_OPENAI_API_URL,
            headers=headers,
            json=data,
            timeout=timeout
        )
        response.raise_for_status()
        result = response.json()
        summary = result["choices"][0]["message"]["content"]

        stop_spinner()
        print(" Done!")
        return summary.strip()
    except Exception as e:
        stop_spinner()
        print(f" Error: {str(e)[:100]}")
        logger.error(f"OpenAI API error: {e}")
        return f"Error generating summary: {str(e)[:100]}"

def summarize_with_huggingface(content, api_key, model=None, depth="medium", timeout=DEFAULT_TIMEOUT):
    """Summarize content using Hugging Face API with improved parameters."""
    if not model:
        if depth == "detailed":
            model = "facebook/bart-large-cnn"  # Better for longer summaries
        else:
            model = "sshleifer/distilbart-cnn-12-6"  # Faster for short/medium summaries

    text = content["text"]
    if len(text) > MAX_CONTENT_LENGTH:
        text = text[:MAX_CONTENT_LENGTH] + "... [truncated]"

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    # Adjust parameters based on depth
    parameters = {}
    if depth == "short":
        parameters = {
            "max_length": 50,
            "min_length": 20,
            "do_sample": False,
            "truncation": True
        }
    elif depth == "medium":
        parameters = {
            "max_length": 150,
            "min_length": 80,
            "do_sample": False,
            "truncation": True
        }
    else:  # detailed
        parameters = {
            "max_length": 300,
            "min_length": 150,
            "do_sample": False,
            "truncation": True
        }

    payload = {
        "inputs": text,
        "parameters": parameters
    }

    print(f"Generating summary with Hugging Face ({model})...", end="", flush=True)
    stop_spinner = create_spinner()

    try:
        response = requests.post(
            f"{DEFAULT_HF_API_URL}{model}",
            headers=headers,
            json=payload,
            timeout=timeout
        )
        response.raise_for_status()
        result = response.json()

        stop_spinner()
        print(" Done!")

        if isinstance(result, list) and len(result) > 0:
            summary = result[0].get("summary_text", "No summary generated.").strip()

            # Clean up and improve formatting
            summary = re.sub(r'\s+', ' ', summary)  # Fix spacing issues
            summary = summary.replace(" .", ".")     # Fix spacing before periods
            summary = summary.replace(" ,", ",")     # Fix spacing before commas

            return summary
        else:
            return "No summary generated."
    except Exception as e:
        stop_spinner()
        print(f" Error: {str(e)[:100]}")
        logger.error(f"Hugging Face API error: {e}")
        return f"Error generating summary: {str(e)[:100]}"

def summarize_content(content, provider, model, depth, api_keys, api_urls, timeout=DEFAULT_TIMEOUT):
    """Summarize content using the selected provider with enhanced capabilities."""
    if not content["text"]:
        return f"No content available for {content['url']}"

    if provider == "textrank":
        return summarize_with_enhanced_textrank(content, depth, timeout)
    elif provider == "ollama":
        return summarize_with_ollama(content, model, depth, api_urls["ollama"], timeout)
    elif provider == "openai":
        if not api_keys["openai"]:
            return "Error: OpenAI API key not provided."
        return summarize_with_openai(content, api_keys["openai"], depth, model, timeout)
    elif provider == "huggingface":
        if not api_keys["huggingface"]:
            return "Error: Hugging Face API key not provided."
        return summarize_with_huggingface(content, api_keys["huggingface"], model, depth, timeout)
    else:
        return f"Error: Unknown provider '{provider}'."

def generate_hierarchical_batch_summary(summaries, depth="medium"):
    """
    Generate a hierarchical batch summary that identifies common themes and topics

    Args:
        summaries: List of summary dictionaries
        depth: Desired summary depth

    Returns:
        Dictionary with batch summary info
    """
    if not summaries:
        return {
            "overview": "No valid summaries to create an overall summary.",
            "themes": [],
            "count": 0
        }

    # Combine all the individual summaries for text analysis
    combined_text = "\n\n".join([
        f"Title: {s['title']}\nSummary: {s['summary']}"
        for s in summaries
        if s["summary"] and not s["summary"].startswith("Error")
    ])

    if not combined_text:
        return {
            "overview": "No valid summaries to create an overall summary.",
            "themes": [],
            "count": 0
        }

    try:
        # Use the EnhancedTextRank for analysis
        summarizer = EnhancedTextRankSummarizer()

        # Extract common themes (keywords and topics)
        keywords = summarizer.extract_keywords(combined_text, max_keywords=10)

        # Detect category/topic
        category = summarizer._detect_category(combined_text)

        # Extract named entities
        entities = summarizer._extract_entities(combined_text)

        # Generate hierarchical summary based on depth
        if depth == "short":
            # Very concise batch overview
            overview = summarizer.summarize(
                combined_text, ratio=0.03, min_sentences=1, max_sentences=2
            )["summary"]

            # Include top keywords/themes only
            themes = [kw for kw, _ in keywords[:3]]

        elif depth == "medium":
            # Moderate batch overview
            overview = summarizer.summarize(
                combined_text, ratio=0.1, min_sentences=2, max_sentences=4
            )["summary"]

            # Include more themes/topics with categories
            themes = [kw for kw, _ in keywords[:5]]

            # Add category if available
            if category["category"] != "unknown" and category["confidence"] > 0.3:
                overview += f"\n\nPrimary topic: {category['category'].capitalize()}"

        else:  # detailed
            # Comprehensive batch overview
            overview = summarizer.summarize(
                combined_text, ratio=0.15, min_sentences=3, max_sentences=6
            )["summary"]

            # Include detailed themes and entities
            themes = [kw for kw, _ in keywords[:8]]

            # Add category breakdown if available
            if category["category"] != "unknown" and category["confidence"] > 0.3:
                overview += f"\n\nPrimary topic: {category['category'].capitalize()}"

                # Add distribution of topics if available
                if "distribution" in category:
                    topic_dist = sorted(
                        [(k, v) for k, v in category["distribution"].items() if v > 0.1],
                        key=lambda x: x[1],
                        reverse=True
                    )
                    if topic_dist:
                        topic_str = ", ".join([f"{k.capitalize()} ({int(v*100)}%)" for k, v in topic_dist[:3]])
                        overview += f"\nTopic distribution: {topic_str}"

            # Add named entities if any significant ones were found
            significant_entities = []
            for entity_type in ["ORGANIZATION", "PERSON", "LOCATION"]:
                if entities.get(entity_type):
                    entity_items = entities[entity_type][:3]  # Top 3 entities of each type
                    if entity_items:
                        significant_entities.append(f"{entity_type.capitalize()}: {', '.join(entity_items)}")

            if significant_entities:
                overview += "\n\nCommon entities: " + "; ".join(significant_entities)

        # Return structured batch summary
        batch_summary = {
            "overview": overview,
            "themes": themes,
            "count": len([s for s in summaries if s["summary"] and not s["summary"].startswith("Error")]),
            "category": category["category"] if category["confidence"] > 0.3 else "mixed"
        }

        # Add entity count if available for detailed summaries
        if depth == "detailed":
            entity_counts = {}
            for entity_type, values in entities.items():
                if values:
                    entity_counts[entity_type] = len(values)

            if entity_counts:
                batch_summary["entities"] = entity_counts

        return batch_summary

    except Exception as e:
        logger.error(f"Error generating hierarchical batch summary: {e}")
        return {
            "overview": "Could not generate an overall batch summary due to an error.",
            "themes": [],
            "count": len([s for s in summaries if s["summary"] and not s["summary"].startswith("Error")])
        }

def process_batch(urls, provider, model, depth, api_keys, api_urls, timeout=DEFAULT_TIMEOUT, automatic_mode=False):
    """Process a batch of URLs: fetch, summarize, and display results with enhanced analytics."""
    valid_contents = []
    invalid_urls = []
    summaries = []
    all_404 = True  # Flag to track if all URLs result in 404 errors

    print(f"\nProcessing {len(urls)} URLs...")

    # Fetch content from URLs using parallel processing
    contents = []
    with ThreadPoolExecutor(max_workers=min(len(urls), 5)) as executor:
        future_to_url = {executor.submit(fetch_webpage_content, url, timeout): url for url in urls}
        for i, future in enumerate(as_completed(future_to_url), 1):
            url = future_to_url[future]
            try:
                content = future.result()
                # Print appropriate message based on content status
                if content["error"]:
                    if content["status_code"] == 404:
                        # In automatic mode, don't show 404 messages
                        if not automatic_mode:
                            print(f"[{i}/{len(urls)}] Skipping 404 Not Found: {url}")
                        invalid_urls.append(url)
                    else:
                        print(f"[{i}/{len(urls)}] Error: {content['title']} for {url}")
                        contents.append(content)
                        all_404 = False  # Found an error that's not a 404
                else:
                    # Calculate content size metrics
                    char_count = len(content['text'])
                    word_count = len(content['text'].split())
                    reading_time = max(1, round(word_count / 200))  # Estimated reading time in minutes

                    print(f"[{i}/{len(urls)}] Fetched: {url} ({word_count} words, ~{reading_time} min read)")
                    contents.append(content)
                    valid_contents.append(content)
                    all_404 = False  # Found at least one valid URL
            except Exception as e:
                logger.error(f"Error processing {url}: {e}")
                print(f"[{i}/{len(urls)}] Error fetching {url}: {str(e)[:100]}")
                contents.append({
                    "title": f"Error fetching {url}",
                    "text": "",
                    "url": url,
                    "status_code": 0,
                    "error": True
                })

    # Log the number of URLs being skipped due to 404
    if invalid_urls:
        logger.info(f"Skipping {len(invalid_urls)} URLs due to 404 errors")
        if not automatic_mode:  # Only show in non-automatic mode
            print(f"\n\033[1;33mSkipping {len(invalid_urls)} URLs due to 404 errors\033[0m")

    # Track content categories for batch analysis
    content_categories = Counter()

    # Summarize content (excluding 404 pages)
    for i, content in enumerate([c for c in contents if not (c.get("status_code") == 404)], 1):
        if not content["text"]:
            print(f"\n\033[1;33m[{i}/{len(contents) - len(invalid_urls)}] Error: Could not extract content from {content['url']}\033[0m")
            summaries.append({
                "title": content["title"],
                "url": content["url"],
                "summary": "No content could be extracted from this URL."
            })
            continue

        print(f"\n\033[1;33m[{i}/{len(contents) - len(invalid_urls)}] Summarizing: {content['title']}\033[0m")
        summary = summarize_content(content, provider, model, depth, api_keys, api_urls, timeout)

        # Attempt to detect content category
        if provider == "textrank":
            # We can use the TextRank categorization directly
            try:
                summarizer = EnhancedTextRankSummarizer()
                category = summarizer._detect_category(content["text"])
                if category["category"] != "unknown" and category["confidence"] > 0.3:
                    content_categories[category["category"]] += 1
            except Exception:
                pass

        domain = get_domain(content["url"])
        print(f"\n\033[1;32m[{i}/{len(contents) - len(invalid_urls)}] Summary for {content['title']}\033[0m")
        print(f"\033[0;36m{content['url']}\033[0m \033[0;90m({domain})\033[0m")
        print(f"{summary}")
        print("-" * 60)

        summaries.append({
            "title": content["title"],
            "url": content["url"],
            "summary": summary
        })

    # Generate hierarchical batch summary
    if valid_contents and summaries and any(s["summary"] and not s["summary"].startswith("Error") for s in summaries):
        print("\nGenerating comprehensive batch analysis...")

        batch_summary = generate_hierarchical_batch_summary(summaries, depth)

        # Display batch overview
        print("\n\033[1;32mBATCH SUMMARY:\033[0m")
        print(batch_summary["overview"])

        # Display common themes/topics
        if batch_summary["themes"]:
            print("\n\033[1;32mCOMMON THEMES:\033[0m")
            print(", ".join(batch_summary["themes"]))

        # Display content category distribution for detailed summaries
        if depth == "detailed" and content_categories:
            top_categories = content_categories.most_common(3)
            if top_categories:
                print("\n\033[1;32mCONTENT CATEGORIES:\033[0m")
                for category, count in top_categories:
                    percentage = int((count / len(valid_contents)) * 100)
                    print(f"{category.capitalize()}: {count} URLs ({percentage}%)")

        print("=" * 60)

        return summaries, batch_summary, all_404

    return summaries, {"overview": "No valid summaries generated for this batch.", "themes": [], "count": 0}, all_404

def collect_urls_from_folders(root_dir):
    """Collect URLs from all subfolders in the root directory with improved detection."""
    folder_urls = {}

    if not os.path.isdir(root_dir):
        logger.error(f"Root directory {root_dir} does not exist.")
        return folder_urls

    # List all subfolders
    subfolders = [f for f in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, f))]
    if not subfolders:
        logger.warning(f"No subfolders found in {root_dir}.")
        return folder_urls

    # Process each subfolder
    for subfolder in subfolders:
        subfolder_path = os.path.join(root_dir, subfolder)
        urls = []

        # Get all text files in the subfolder
        text_files = [f for f in os.listdir(subfolder_path)
                      if (f.endswith('.txt') or f.endswith('.lst') or f.endswith('.log'))
                      and f != 'summary.txt' and os.path.isfile(os.path.join(subfolder_path, f))]

        # Extract URLs from each text file
        for text_file in text_files:
            file_path = os.path.join(subfolder_path, text_file)
            file_urls = extract_urls_from_file(file_path)
            urls.extend(file_urls)

        # Remove duplicates while preserving order
        unique_urls = []
        seen = set()
        for url in urls:
            if url not in seen:
                seen.add(url)
                unique_urls.append(url)

        if unique_urls:
            folder_urls[subfolder] = unique_urls

    return folder_urls

def choose_provider():
    """Let user choose the summarization provider."""
    print("\nChoose a summarization provider:")
    print("1) Enhanced TextRank (built-in, no API required)")
    print("2) Ollama (local)")
    print("3) OpenAI (ChatGPT)")
    print("4) Hugging Face")

    while True:
        choice = input("Enter your choice (1-4): ").strip()
        if choice == "1":
            return "textrank"
        elif choice == "2":
            return "ollama"
        elif choice == "3":
            return "openai"
        elif choice == "4":
            return "huggingface"
        else:
            print("Invalid choice. Please enter 1, 2, 3, or 4.")

def choose_model(provider):
    """Let user choose the model for the selected provider."""
    if provider == "textrank":
        # TextRank has no model options
        return "textrank"

    elif provider == "ollama":
        print("\nChoose an Ollama model:")
        print("1) gemma3:12b (best for summarization)")
        print("2) llama3:latest (balanced)")
        print("3) deepseek-r1:8b (faster)")
        print("4) Other (enter name)")

        choice = input("Enter your choice (1-4): ").strip()
        if choice == "1":
            return "gemma3:12b"
        elif choice == "2":
            return "llama3:latest"
        elif choice == "3":
            return "deepseek-r1:8b"
        elif choice == "4":
            return input("Enter the model name: ").strip()
        else:
            print("Invalid choice. Using gemma3:12b.")
            return "gemma3:12b"

    elif provider == "openai":
        print("\nChoose an OpenAI model:")
        print("1) gpt-3.5-turbo (fast)")
        print("2) gpt-4 (more comprehensive)")

        choice = input("Enter your choice (1-2): ").strip()
        if choice == "1":
            return "gpt-3.5-turbo"
        elif choice == "2":
            return "gpt-4"
        else:
            print("Invalid choice. Using gpt-3.5-turbo.")
            return "gpt-3.5-turbo"

    elif provider == "huggingface":
        print("\nChoose a Hugging Face model:")
        print("1) facebook/bart-large-cnn (high quality summarization)")
        print("2) sshleifer/distilbart-cnn-12-6 (faster summarization)")
        print("3) google/pegasus-xsum (concise summaries)")
        print("4) Other (enter name)")

        choice = input("Enter your choice (1-4): ").strip()
        if choice == "1":
            return "facebook/bart-large-cnn"
        elif choice == "2":
            return "sshleifer/distilbart-cnn-12-6"
        elif choice == "3":
            return "google/pegasus-xsum"
        elif choice == "4":
            return input("Enter the model name: ").strip()
        else:
            print("Invalid choice. Using facebook/bart-large-cnn.")
            return "facebook/bart-large-cnn"

    return None

def choose_depth():
    """Let user choose the summarization depth."""
    print("\nChoose summarization depth:")
    print("1) Short (1-2 sentences, concise overview)")
    print("2) Medium (one paragraph, balanced summary)")
    print("3) Detailed (comprehensive with themes and metadata)")

    while True:
        choice = input("Enter your choice (1-3): ").strip()
        if choice == "1":
            return "short"
        elif choice == "2":
            return "medium"
        elif choice == "3":
            return "detailed"
        else:
            print("Invalid choice. Please enter 1, 2, or 3.")

def get_api_keys(provider):
    """Get API keys for the selected provider."""
    api_keys = {
        "openai": DEFAULT_OPENAI_API_KEY,
        "huggingface": DEFAULT_HF_API_KEY
    }

    # No API keys needed for textrank
    if provider == "textrank":
        return api_keys

    if provider == "openai":
        key_input = input(f"\nEnter your OpenAI API key (or press Enter to use default/env var): ").strip()
        if key_input:
            api_keys["openai"] = key_input
        elif os.environ.get("OPENAI_API_KEY"):
            api_keys["openai"] = os.environ.get("OPENAI_API_KEY")

    elif provider == "huggingface":
        key_input = input(f"\nEnter your Hugging Face API key (or press Enter to use default/env var): ").strip()
        if key_input:
            api_keys["huggingface"] = key_input
        elif os.environ.get("HF_API_KEY"):
            api_keys["huggingface"] = os.environ.get("HF_API_KEY")

    return api_keys

def save_enhanced_summary(folder_path, batch_summaries, batch_overall_summaries, format_type="markdown"):
    """Save enhanced summaries to a file in the folder."""
    summary_path = os.path.join(folder_path, "summary.txt")
    current_date = datetime.now().strftime("%Y-%m-%d")

    try:
        with open(summary_path, "w", encoding="utf-8") as f:
            if format_type == "markdown":
                f.write(f"# Website Summaries - {current_date}\n\n")

                # Global summary for all batches
                if batch_overall_summaries:
                    f.write("## Overall Themes\n\n")

                    # Collect all themes from all batches
                    all_themes = []
                    for batch in batch_overall_summaries:
                        if "themes" in batch and batch["themes"]:
                            all_themes.extend(batch["themes"])

                    # Count theme frequencies
                    theme_counter = Counter(all_themes)
                    top_themes = theme_counter.most_common(10)

                    if top_themes:
                        f.write("Common themes across all content:\n\n")
                        for theme, count in top_themes:
                            f.write(f"- **{theme}** ({count} occurrences)\n")
                        f.write("\n")

                    # Overall summary of all batch overviews
                    f.write("### Executive Summary\n\n")
                    all_overviews = "\n\n".join([b["overview"] for b in batch_overall_summaries
                                                 if "overview" in b and b["overview"]])

                    # Create a super-summary using TextRank
                    if all_overviews:
                        try:
                            summarizer = EnhancedTextRankSummarizer()
                            executive_summary = summarizer.summarize(
                                all_overviews, ratio=0.3, min_sentences=3, max_sentences=5
                            )["summary"]
                            f.write(f"{executive_summary}\n\n")
                        except Exception as e:
                            logger.error(f"Error generating executive summary: {e}")
                            f.write("This folder contains multiple batches of website summaries.\n\n")

                # Write each batch with its URLs and summaries
                for batch_idx, (summaries, overall) in enumerate(zip(batch_summaries, batch_overall_summaries), 1):
                    f.write(f"## Batch {batch_idx} ({len(summaries)} URLs)\n\n")

                    # Write batch overview
                    if "overview" in overall and overall["overview"]:
                        f.write("### Batch Overview\n\n")
                        f.write(f"{overall['overview']}\n\n")

                    # Write themes if available
                    if "themes" in overall and overall["themes"]:
                        f.write("### Themes\n\n")
                        for theme in overall["themes"]:
                            f.write(f"- {theme}\n")
                        f.write("\n")

                    # Write individual summaries
                    f.write("### Individual Summaries\n\n")
                    for i, summary in enumerate(summaries, 1):
                        f.write(f"#### {i}. {summary['title']}\n")
                        f.write(f"*URL*: {summary['url']}\n\n")
                        f.write(f"{summary['summary']}\n\n")
                        f.write("---\n\n")

            elif format_type == "text":
                # Simple text format
                f.write(f"WEBSITE SUMMARIES - {current_date}\n")
                f.write("=" * 60 + "\n\n")

                for batch_idx, (summaries, overall) in enumerate(zip(batch_summaries, batch_overall_summaries), 1):
                    f.write(f"BATCH {batch_idx} ({len(summaries)} URLs)\n")
                    f.write("-" * 60 + "\n\n")

                    if "overview" in overall and overall["overview"]:
                        f.write("Batch Overview:\n")
                        f.write(f"{overall['overview']}\n\n")

                    if "themes" in overall and overall["themes"]:
                        f.write("Themes: ")
                        f.write(", ".join(overall["themes"]) + "\n\n")

                    for i, summary in enumerate(summaries, 1):
                        f.write(f"{i}. {summary['title']}\n")
                        f.write(f"URL: {summary['url']}\n\n")
                        f.write(f"{summary['summary']}\n\n")
                        f.write("-" * 40 + "\n\n")

            elif format_type == "json":
                # JSON format for programmatic access
                summary_data = {
                    "metadata": {
                        "generated_date": current_date,
                        "total_batches": len(batch_summaries),
                        "total_urls": sum(len(s) for s in batch_summaries),
                        "total_successful": sum("overview" in b and b["count"] > 0 for b in batch_overall_summaries)
                    },
                    "batches": []
                }

                for batch_idx, (summaries, overall) in enumerate(zip(batch_summaries, batch_overall_summaries), 1):
                    batch_data = {
                        "batch_id": batch_idx,
                        "overview": overall.get("overview", ""),
                        "themes": overall.get("themes", []),
                        "count": overall.get("count", 0),
                        "summaries": []
                    }

                    for summary in summaries:
                        batch_data["summaries"].append({
                            "title": summary["title"],
                            "url": summary["url"],
                            "summary": summary["summary"]
                        })

                    summary_data["batches"].append(batch_data)

                json.dump(summary_data, f, indent=2)

        print(f"\nSummary saved to: {summary_path}")
        return True
    except Exception as e:
        logger.error(f"Error saving summary to {summary_path}: {e}")
        print(f"Error saving summary: {str(e)}")
        return False

def check_for_key_press():
    """Check if a key has been pressed without blocking execution."""
    try:
        # Check if there's any input ready (non-blocking)
        import select
        ready, _, _ = select.select([sys.stdin], [], [], 0)
        if ready:
            # There's input ready, read it
            key = sys.stdin.readline().strip().lower()
            return key
        return None
    except Exception:
        return None

def main():
    """Main function with improved workflow."""
    args = setup_argparse()
    configure_logging(args.verbose, args.debug)

    print_header("WEBSUMMARIZER - ENHANCED PHOTON RESULTS SUMMARIZER")
    print(f"Version: {__version__}")

    # Choose summarization provider, model, and depth
    provider = choose_provider()
    model = choose_model(provider)
    depth = choose_depth()
    api_keys = get_api_keys(provider)

    api_urls = {
        "ollama": args.ollama_url,
        "openai": DEFAULT_OPENAI_API_URL,
        "huggingface": DEFAULT_HF_API_URL
    }

    # Check Ollama connection if needed
    if provider == "ollama" and not check_ollama_connection(args.ollama_url):
        print("\nError: Cannot connect to Ollama. Make sure Ollama is running.")
        print("You can start Ollama with: ollama serve")
        print(f"And pull the model with: ollama pull {model}")
        return 1

    # Collect URLs from folders
    print(f"\nCollecting URLs from subfolders in {args.root_dir}...")
    folder_urls = collect_urls_from_folders(args.root_dir)

    if not folder_urls:
        print("No URLs found in any subfolder.")
        return 1

    print(f"\nFound {len(folder_urls)} subfolders with URLs:")
    for folder, urls in folder_urls.items():
        print(f"- {folder}: {len(urls)} URLs")

    # Track automatic mode state
    automatic_mode = args.auto
    if automatic_mode:
        print("\nAutomatic mode enabled. Processing will continue until complete or interrupted.")
        print(f"Using batch size of 20 for automatic mode (overrides default of {args.batch_size}).")
        print("Press 'n' then Enter at any time to exit automatic mode.")
        print("Or press Ctrl+C at any time to stop processing and save results.")

    # Process each subfolder
    for folder, urls in folder_urls.items():
        folder_path = os.path.join(args.root_dir, folder)
        print_header(f"Processing folder: {folder}")
        print(f"Found {len(urls)} URLs to process")

        # Process URLs in batches
        batch_summaries = []
        batch_overall_summaries = []

        i = 0
        while i < len(urls):
            # Use batch size of 20 when in automatic mode, otherwise use the specified batch size
            current_batch_size = 20 if automatic_mode else args.batch_size
            batch = urls[i:i + current_batch_size]

            batch_number = i // current_batch_size + 1
            total_batches = (len(urls) + current_batch_size - 1) // current_batch_size
            print_header(f"Processing batch {batch_number}/{total_batches} ({len(batch)} URLs)")

            summaries, overall_summary, all_404 = process_batch(
                batch, provider, model, depth, api_keys, api_urls, args.timeout, automatic_mode
            )

            batch_summaries.append(summaries)
            batch_overall_summaries.append(overall_summary)

            # Move to next batch
            i += current_batch_size

            # Check if we should continue to the next batch
            if i < len(urls):
                if all_404:
                    # Automatically continue to next batch if all URLs were 404s
                    if not automatic_mode:  # Only show in non-automatic mode
                        print("\nAll URLs in this batch resulted in 404 errors. Automatically continuing to next batch...")
                    continue
                elif automatic_mode:
                    # In fully automatic mode, just continue without prompting
                    # Just print a brief status message
                    print(f"\nAutomatic mode active - continuing to next batch ({batch_number+1}/{total_batches}).")
                    print("(Press 'n' then Enter to exit automatic mode)")
                    # Brief pause to allow for reading messages
                    time.sleep(1.0)

                    # Non-blocking check if 'n' was pressed to exit automatic mode
                    key = check_for_key_press()
                    if key == 'n':
                        automatic_mode = False
                        print("Automatic mode disabled. Will prompt before each batch.")
                        # Make sure to clearly ask about continuing and enforce input validation
                        while True:
                            choice = input("Continue with next batch? (y/n): ").strip().lower()
                            if choice == 'y':
                                break
                            elif choice == 'n':
                                print("Stopping at user request.")
                                i = len(urls)  # This will exit the outer while loop
                                break
                            else:
                                print("Please enter 'y' or 'n'.")
                else:
                    # Implement better input validation to ensure only valid inputs are accepted
                    valid_response = False
                    while not valid_response:
                        choice = input("\nContinue with next batch? (y/n/a for automatic mode): ").strip().lower()
                        if choice == 'a':
                            automatic_mode = True
                            print("Automatic mode enabled. Processing will continue until complete or interrupted.")
                            print("Using batch size of 20 for remaining batches.")
                            print("Press 'n' then Enter at any time to exit automatic mode.")
                            valid_response = True
                        elif choice == 'y':
                            valid_response = True
                        elif choice == 'n':
                            print("Stopping at user request.")
                            i = len(urls)  # This will exit the outer while loop
                            valid_response = True
                        else:
                            print("Invalid input. Please enter 'y', 'n', or 'a'.")

        # Save summaries to file in the specified format
        save_enhanced_summary(folder_path, batch_summaries, batch_overall_summaries, format_type=args.format)

    print_header("SUMMARIZATION COMPLETE")
    return 0

if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        print("\n\nOperation interrupted by user.")
        print("Saving any completed summaries...")
        # Note: The summaries for the completed batches are already saved by the main loop
        print("Summary files have been saved for completed batches.")
        sys.exit(1)

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/wetails.py
========================================

import requests
from bs4 import BeautifulSoup

def extract_website_info(url):
    try:
        response = requests.get(url)
        response.raise_for_status()  # Raises an HTTPError if the HTTP request returned an unsuccessful status code
    except Exception as e:
        print(f"Error fetching the URL: {e}")
        return None

    # Parse the HTML content with BeautifulSoup
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # Extract the title of the page
    title = soup.title.string if soup.title else "No title found"

    # Extract the meta description (if available)
    meta_description = soup.find("meta", attrs={"name": "description"})
    description = meta_description['content'] if meta_description and meta_description.has_attr('content') else "No description available"

    # Extract text from all paragraph elements
    paragraphs = [p.get_text(strip=True) for p in soup.find_all("p")]
    
    return {
        "title": title,
        "description": description,
        "paragraphs": paragraphs,
    }

def main():
    url = input("Enter a website URL: ")
    info = extract_website_info(url)
    if info:
        print("\nWebsite Information:")
        print("Title:", info["title"])
        print("Meta Description:", info["description"])
        print("\nExtracted Paragraphs:\n")
        for p in info["paragraphs"]:
            print(p)
            print("-" * 80)

if __name__ == "__main__":
    main()

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/runner.py
========================================

#!/usr/bin/env python3
"""
OSINT Suite Runner

This script automates running the entire OSINT workflow:
1. Running proton.py to search and crawl websites
2. Running protonizer.py to analyze and summarize the results
3. Starting the app.py web server
4. Opening the frontend in a web browser

Usage:
    python3 osint_runner.py "search keywords" "example.com" 50
"""

import argparse
import subprocess
import time
import webbrowser
import os
import sys
import signal
from pathlib import Path

def parse_arguments():
    parser = argparse.ArgumentParser(description="Run the complete OSINT workflow")
    parser.add_argument("keyword", help="Search keyword for proton.py")
    parser.add_argument("site", nargs='?', default=None, help="Site to search/crawl (optional)")
    parser.add_argument("depth", type=int, help="Depth value (1-300)")
    parser.add_argument("--skip-proton", action="store_true", 
                        help="Skip running proton.py (use existing results)")
    parser.add_argument("--skip-protonizer", action="store_true", 
                        help="Skip running protonizer.py")
    parser.add_argument("--port", type=int, default=5000, 
                        help="Port for the web server (default: 5000)")
    
    # Handle the case where user provides only keyword and depth
    args = parser.parse_args()
    
    # If user provided just two arguments, the second is actually the depth
    if len(sys.argv) == 3 and args.site and args.site.isdigit() and args.depth is None:
        args.depth = int(args.site)
        args.site = None
    
    return args

def run_command(cmd, description):
    """Run a command and handle any errors."""
    print(f"\n[+] {description}...")
    try:
        print(f"[*] Running: {' '.join(cmd)}")
        subprocess.run(cmd, check=True)
        print(f"[+] Command completed successfully")
        return True
    except subprocess.CalledProcessError as e:
        print(f"[!] Error: {e}")
        return False
    except Exception as e:
        print(f"[!] Unexpected error: {e}")
        return False

def start_web_server(port=5000):
    """Start the Flask web server in the background."""
    print(f"\n[+] Starting web server on port {port}...")
    try:
        # Define the correct static directory path
        static_dir = Path("/home/jarvis/Proton/static")
        
        # Make sure the static directory exists
        static_dir.mkdir(exist_ok=True, parents=True)
        print(f"[*] Ensuring static directory exists: {static_dir}")
        
        # Copy frontend.html to the static directory if needed
        frontend_path = Path("frontend.html")
        static_index_path = static_dir / "index.html"
        
        if frontend_path.exists():
            print(f"[*] Copying frontend.html to {static_index_path}")
            with open(frontend_path, "r") as src, open(static_index_path, "w") as dst:
                dst.write(src.read())
        else:
            print(f"[!] Warning: frontend.html not found in current directory")
            # Check if it's already in the destination
            if not static_index_path.exists():
                print(f"[!] Warning: {static_index_path} doesn't exist either!")
                print(f"[*] The web interface may not function correctly")
        
        # Start the server as a background process
        cmd = ["python3", "app.py"]
        server_process = subprocess.Popen(cmd, 
                                         stdout=subprocess.PIPE,
                                         stderr=subprocess.PIPE)
        
        # Give the server a moment to start up
        time.sleep(2)
        
        # Check if the process is still running
        if server_process.poll() is None:
            print(f"[+] Web server started successfully (PID: {server_process.pid})")
            return server_process
        else:
            stdout, stderr = server_process.communicate()
            print(f"[!] Web server failed to start: {stderr.decode()}")
            return None
    except Exception as e:
        print(f"[!] Error starting web server: {e}")
        return None

def open_web_browser(port=5000):
    """Open the web browser to view the interface."""
    url = f"http://localhost:{port}"
    print(f"\n[+] Opening web browser to {url}...")
    try:
        webbrowser.open(url)
        print(f"[+] Browser opened successfully")
        return True
    except Exception as e:
        print(f"[!] Failed to open browser: {e}")
        print(f"[+] Please manually navigate to {url}")
        return False

def main():
    args = parse_arguments()
    
    # Run proton.py if not skipped
    if not args.skip_proton:
        # Build the command based on whether a site was provided
        if args.site:
            cmd = ["python3", "proton.py", args.keyword, args.site, str(args.depth)]
            description = f"Running proton.py for '{args.keyword}' on site '{args.site}' with depth {args.depth}"
        else:
            cmd = ["python3", "proton.py", args.keyword, str(args.depth)]
            description = f"Running proton.py for '{args.keyword}' with depth {args.depth}"
        
        success = run_command(cmd, description)
        if not success:
            choice = input("[!] proton.py encountered errors. Continue anyway? (y/n): ").lower()
            if choice != 'y':
                return
    else:
        print("\n[*] Skipping proton.py as requested")
    
    # Run protonizer.py if not skipped
    if not args.skip_protonizer:
        success = run_command(
            ["python3", "protonizer.py"],
            "Running protonizer.py to analyze and summarize the results"
        )
        if not success:
            choice = input("[!] protonizer.py encountered errors. Continue anyway? (y/n): ").lower()
            if choice != 'y':
                return
    else:
        print("\n[*] Skipping protonizer.py as requested")
    
    # Start the web server
    server_process = start_web_server(args.port)
    if not server_process:
        print("[!] Failed to start web server. Exiting.")
        return
    
    # Open the web browser
    open_web_browser(args.port)
    
    print("\n[+] OSINT suite is now running")
    print(f"[+] The web interface is available at http://localhost:{args.port}")
    print("[+] Press Ctrl+C to stop the server and exit")
    
    try:
        # Keep the script running until the user interrupts
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("\n[+] Stopping web server...")
        os.kill(server_process.pid, signal.SIGTERM)
        time.sleep(1)
        print("[+] Done. Goodbye!")

if __name__ == "__main__":
    main()

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/miniproton.py
========================================

#!/usr/bin/env python3
import argparse
import subprocess
import sys
import logging
import shutil
import traceback
import json
import os

VENV_PATH = os.path.expanduser("~/newvenv/bin/activate")  # Adjust virtualenv path as needed

def check_command(command):
    """Verify if a command is available in the system PATH."""
    if shutil.which(command) is None:
        logging.error(f"'{command}' is not installed. Please install it to proceed.")
        sys.exit(1)
    logging.debug(f"'{command}' is available.")

def run_command(command, label, debug=False, use_venv=False):
    """Execute a command, optionally using a virtual environment."""
    try:
        check_command(command[0])
        logging.debug(f"Executing command: {' '.join(command)}")

        # If running Photon, ensure it is within the virtual environment
        if use_venv:
            command = f"source {VENV_PATH} && " + " ".join(command)
            result = subprocess.run(command, shell=True, capture_output=True, text=True, executable="/bin/bash")
        else:
            result = subprocess.run(command, capture_output=True, text=True)

        if result.returncode != 0:
            logging.error(f"Error running {label}: {result.stderr.strip()}")
            if debug:
                logging.debug("Full stderr: " + result.stderr)
            return None

        logging.debug(f"{label} output: {result.stdout.strip()}")
        return result.stdout.strip()

    except Exception as exc:
        logging.error(f"Exception running {label}: {exc}")
        if debug:
            traceback.print_exc()
        return None

def run_ddgr(query, debug=False):
    """Run ddgr with the given search query and output formatted results."""
    print(f"\n[+] DuckDuckGo (ddgr) search results for: {query}\n")
    output = run_command(['ddgr', '--json', query], "ddgr", debug)
    
    if output:
        try:
            results = json.loads(output)
            for result in results[:10]:  # Limit output to 10 results
                print(f"- {result.get('title', 'No Title')}\n  {result.get('url', 'No URL')}\n")
        except json.JSONDecodeError:
            logging.error("Failed to parse ddgr JSON output.")

def run_photon(target, debug=False):
    """Run Photon OSINT tool on the given target URL."""
    print(f"\n[+] Photon OSINT results for: {target}\n")
    output = run_command(['python3', 'photon.py', '-u', target, '-o', 'json'], "Photon OSINT", debug, use_venv=True)
    
    if output:
        print(output)

def main():
    parser = argparse.ArgumentParser(description="OSINT tool using Photon OSINT and ddgr.")
    parser.add_argument('--query', type=str, help="Search query for DuckDuckGo (ddgr)")
    parser.add_argument('--target', type=str, help="Target URL for Photon OSINT scraping")
    parser.add_argument('--debug', action='store_true', help="Enable debug mode for detailed output")
    args = parser.parse_args()

    # Configure logging based on the debug flag.
    logging_level = logging.DEBUG if args.debug else logging.INFO
    logging.basicConfig(level=logging_level, format='[%(levelname)s] %(message)s')

    if args.debug:
        logging.debug("Debug mode enabled.")

    if not args.query and not args.target:
        parser.error("You must provide at least --query or --target (or both).")

    if args.query:
        run_ddgr(args.query, args.debug)

    if args.target:
        run_photon(args.target, args.debug)

if __name__ == '__main__':
    main()

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/deepsearch-proton.py
========================================

#!/usr/bin/env python3
import argparse
import subprocess
import sys
import logging
import shutil
import traceback
import json
import os
import re
import time
import textwrap
from collections import OrderedDict

__version__ = "2.0"

# Path to your virtual environment activation script and Photon script
VENV_PATH = os.path.expanduser("~/newvenv/bin/activate")
PHOTON_SCRIPT = os.path.join(os.path.dirname(os.path.realpath(__file__)), "photon.py")

def check_command(command):
    """Verify if a command is available in the system PATH."""
    if shutil.which(command) is None:
        logging.error(f"'{command}' is not installed. Please install it to proceed.")
        sys.exit(1)
    logging.debug(f"'{command}' is available.")

def pretty_print_json(output):
    """Attempt to parse and pretty-print JSON output; if it fails, return raw output."""
    try:
        data = json.loads(output)
        return json.dumps(data, indent=2)
    except json.JSONDecodeError:
        return output

def run_command(command, label, debug=False, use_venv=False, ignore_errors=False):
    """
    Execute a command, optionally within a virtual environment.
    Returns the command output as a string.
    """
    try:
        if use_venv:
            # For commands like Photon that must run inside your virtualenv,
            # we assume python3 is available.
            check_command("python3")
        else:
            check_command(command[0])
        logging.debug(f"Executing command: {' '.join(command)}")
        
        if use_venv:
            # Combine the activation command with the target command
            full_command = f"source {VENV_PATH} && " + " ".join(command)
            result = subprocess.run(full_command, shell=True, capture_output=True, text=True, executable="/bin/bash")
        else:
            result = subprocess.run(command, capture_output=True, text=True)

        if result.returncode != 0:
            logging.error(f"Error running {label}: {result.stderr.strip()}")
            if debug:
                logging.debug("Full stderr: " + result.stderr)
            if ignore_errors:
                # Return empty result but don't fail completely
                return ""
            return None
        logging.debug(f"{label} output: {result.stdout.strip()}")
        return result.stdout.strip()
    except Exception as exc:
        logging.error(f"Exception running {label}: {exc}")
        if debug:
            traceback.print_exc()
        return None

def calculate_resource_values(depth):
    """
    Calculate appropriate resource values based on the depth parameter (1-100).
    Returns a dictionary with calibrated values for different tools.
    
    Depth scale:
    1-20: Light/Quick search
    21-50: Medium search
    51-80: Thorough search
    81-100: Deep/Extensive search
    """
    # Ensure depth is within valid range (1-100)
    depth = max(1, min(100, depth))
    
    # Always use maximum page size for ddgr (25 results)
    ddgr_page_size = 25
    ddgr_results = 25
    
    # Calculate Photon crawl parameters
    # Level: 1 to 5 (Photon's internal crawl depth)
    photon_level = 1 + int((depth - 1) / 20)  # Ranges from 1-5
    
    # Threads: 2 to 50 (threading for parallel processing)
    photon_threads = 2 + int((depth - 1) * 0.48)  # Linear scaling
    
    # Timeout: 5s to 30s (longer for deeper crawls)
    photon_timeout = 5 + int((depth - 1) * 0.25)  # Linear scaling
    
    # URLs to crawl per page: proportional to depth
    if depth <= 20:
        urls_percent = 0.6  # Crawl 60% of results for light search
    elif depth <= 50:
        urls_percent = 0.8  # Crawl 80% of results for medium search
    elif depth <= 80:
        urls_percent = 0.9  # Crawl 90% of results for thorough search
    else:
        urls_percent = 1.0  # Crawl 100% of results for deep search
    
    urls_to_crawl = max(1, min(ddgr_page_size, round(ddgr_page_size * urls_percent)))
    
    return {
        "ddgr_results": ddgr_results,
        "ddgr_page_size": ddgr_page_size,
        "photon_threads": photon_threads,
        "photon_level": photon_level,
        "photon_timeout": photon_timeout,
        "urls_to_crawl": urls_to_crawl
    }

def deduplicate_urls(new_urls, existing_urls):
    """
    Add new URLs to the existing set while avoiding duplicates.
    
    Parameters:
    - new_urls: List of new URLs to add
    - existing_urls: Set of already processed URLs
    
    Returns:
    - List of URLs that aren't already in existing_urls
    """
    unique_new_urls = []
    for url in new_urls:
        if url not in existing_urls:
            existing_urls.add(url)
            unique_new_urls.append(url)
    
    return unique_new_urls

def build_ddgr_command(query, ddgr_args, page_size=25):
    """Build the ddgr command with all user-supplied arguments."""
    command = ['ddgr', '--json']
    
    # Ensure we're using maximum page size for ddgr
    command.extend(['--num', str(page_size)])
    
    # Add all ddgr-specific arguments
    if ddgr_args.get('region'):
        command.extend(['--reg', ddgr_args.get('region')])
    if ddgr_args.get('colorize'):
        command.extend(['--colorize', ddgr_args.get('colorize')])
    if ddgr_args.get('nocolor'):
        command.append('--nocolor')
    if ddgr_args.get('colors'):
        command.extend(['--colors', ddgr_args.get('colors')])
    if ddgr_args.get('time'):
        command.extend(['--time', ddgr_args.get('time')])
    if ddgr_args.get('site'):
        for site in ddgr_args.get('site'):
            command.extend(['--site', site])
    if ddgr_args.get('expand'):
        command.append('--expand')
    if ddgr_args.get('proxy'):
        command.extend(['--proxy', ddgr_args.get('proxy')])
    if ddgr_args.get('unsafe'):
        command.append('--unsafe')
    if ddgr_args.get('noua'):
        command.append('--noua')
    
    # Add the query
    command.append(query)
    
    return command

def run_ddgr_with_pagination(query, ddgr_args, depth=30, debug=False, page=1):
    """
    Run DuckDuckGo search using ddgr with proper pagination support.
    This function uses ddgr's interactive mode and simulates pagination commands.
    
    Parameters:
    - query: The search query string
    - ddgr_args: Dictionary of ddgr-specific arguments
    - depth: Depth parameter (1-100)
    - debug: Whether to enable debug output
    - page: Page number (1-based for user display)
    
    Returns:
    - List of URLs from search results
    - Boolean indicating if there might be more results
    """
    resources = calculate_resource_values(depth)
    page_size = resources["ddgr_page_size"]  # Now this will always be 25
    
    # For first page, search normally
    if page == 1:
        print(f"\n[+] DuckDuckGo (ddgr) search results for: {query}")
        print(f"[+] Search depth: {depth}/100 (Page {page}, retrieving {page_size} results)")
        command = build_ddgr_command(query, ddgr_args, page_size)
    else:
        print(f"\n[+] DuckDuckGo (ddgr) search results - Page {page} for: {query}")
        print(f"[+] Search depth: {depth}/100 (retrieving {page_size} results per page)")
        
        # For subsequent pages, we need to simulate pagination
        # Always use maximum results (25) to reduce pagination steps
        initial_results = 25  # This is the maximum for ddgr
        command = ['ddgr', '--json', '--num', str(initial_results), query]
        # Add other args
        if ddgr_args.get('region'):
            command.extend(['--reg', ddgr_args.get('region')])
        if ddgr_args.get('time'):
            command.extend(['--time', ddgr_args.get('time')])
        if ddgr_args.get('site'):
            for site in ddgr_args.get('site'):
                command.extend(['--site', site])
        if ddgr_args.get('unsafe'):
            command.append('--unsafe')
    
    output = run_command(command, "ddgr", debug)
    if not output:
        print("[-] No results from ddgr search")
        return [], False
    
    urls = []
    try:
        results = json.loads(output)
        
        # If we're on a later page, we need to extract the correct subset of results
        result_offset = 0
        if page > 1:
            # Calculate the offset for the current page
            # For page 2, we want results 25-49 (assuming page_size=25)
            # For page 3, we want results 50-74, etc.
            result_offset = (page - 1) * page_size
            
            # If offset is beyond available results, no more results
            if result_offset >= len(results):
                print("[-] No more results available.")
                return [], False
            
            # Get the slice of results for this page
            end_offset = min(result_offset + page_size, len(results))
            page_results = results[result_offset:end_offset]
        else:
            # First page, just take the first page_size results
            page_results = results[:page_size]
        
        # Process and display the results
        for i, result in enumerate(page_results, 1):
            title = result.get("title", "No Title")
            url = result.get("url", "No URL")
            abstract = result.get("abstract", "")
            
            # Add URLs to the list for Photon crawling
            urls.append(url)
            
            # Display result with correct global index
            global_index = result_offset + i
            print(f"{global_index}. {title}")
            print(f"   {url}")
            if abstract:
                wrapped_abstract = textwrap.fill(abstract, width=80, initial_indent="   ", subsequent_indent="   ")
                print(f"{wrapped_abstract}\n")
            else:
                print()  # Empty line for spacing
        
        # Determine if there might be more results
        # We consider there are more if:
        # 1. We got a full page of results, or
        # 2. We know there are more results in our fetched batch
        has_more = (len(page_results) == page_size) or (result_offset + len(page_results) < len(results))
        
        return urls, has_more
    
    except json.JSONDecodeError:
        logging.error("Failed to parse ddgr JSON output.")
        print(output)
        return [], False

def build_photon_command(target, photon_args, output_dir):
    """Build the Photon command with all user-supplied arguments."""
    command = [
        'python3', PHOTON_SCRIPT,
        '-u', target,
        '-o', output_dir
    ]
    
    # Add all photon-specific arguments
    if photon_args.get('level') is not None:
        command.extend(['-l', str(photon_args.get('level'))])
    if photon_args.get('threads') is not None:
        command.extend(['-t', str(photon_args.get('threads'))])
    if photon_args.get('delay') is not None:
        command.extend(['-d', str(photon_args.get('delay'))])
    if photon_args.get('timeout') is not None:
        command.extend(['--timeout', str(photon_args.get('timeout'))])
    if photon_args.get('cookie'):
        command.extend(['-c', photon_args.get('cookie')])
    if photon_args.get('regex'):
        command.extend(['-r', photon_args.get('regex')])
    if photon_args.get('export'):
        command.extend(['-e', photon_args.get('export')])
    if photon_args.get('seeds'):
        command.extend(['-s'] + photon_args.get('seeds'))
    if photon_args.get('user_agent'):
        command.extend(['--user-agent', photon_args.get('user_agent')])
    if photon_args.get('exclude'):
        command.extend(['--exclude', photon_args.get('exclude')])
    if photon_args.get('proxy'):
        command.extend(['-p', photon_args.get('proxy')])
    
    # Add boolean flags
    if photon_args.get('verbose'):
        command.append('-v')
    if photon_args.get('headers'):
        command.append('--headers')
    if photon_args.get('dns'):
        command.append('--dns')
    if photon_args.get('keys'):
        command.append('--keys')
    if photon_args.get('only_urls'):
        command.append('--only-urls')
    if photon_args.get('wayback'):
        command.append('--wayback')
    
    return command

def run_photon_on_single_target(target, photon_args, depth=30, debug=False, index=None, total=None):
    """Run Photon OSINT on a single target URL with user-specified arguments."""
    resources = calculate_resource_values(depth)
    
    # Use calculated resources if not provided in photon_args
    level = photon_args.get('level') or resources["photon_level"]
    threads = photon_args.get('threads') or resources["photon_threads"]
    timeout = photon_args.get('timeout') or resources["photon_timeout"]
    
    # Create a progress indicator if we're processing multiple URLs
    progress_str = ""
    if index is not None and total is not None:
        progress_str = f"[{index}/{total}] "
    
    print(f"\n[+] {progress_str}Photon crawling target: {target}")
    print(f"[+] Crawl depth: {depth}/100 (level: {level}, threads: {threads}, timeout: {timeout}s)")
    
    if not os.path.isfile(PHOTON_SCRIPT):
        logging.error(f"Photon script not found at {PHOTON_SCRIPT}. Please ensure photon.py is available.")
        return None
    
    # Create organized output folder structure
    main_output_dir = "photon_results"
    if not os.path.exists(main_output_dir):
        os.makedirs(main_output_dir)
    
    # Get domain name for subfolder
    domain = target.replace("https://", "").replace("http://", "").split("/")[0]
    domain_safe = re.sub(r'[^\w\-_]', '_', domain)  # Make domain name safe for filesystem
    
    # Create a unique subfolder for this target
    timestamp = int(time.time())
    target_dir = f"{domain_safe}_{timestamp}"
    output_dir = os.path.join(main_output_dir, target_dir)
    
    # Build the Photon command
    command = build_photon_command(target, photon_args, output_dir)
    
    # Run the command with the proper settings and handle failures gracefully
    output = run_command(command, f"Photon OSINT on {target}", debug, use_venv=True, ignore_errors=True)
    
    # Create the output directory even if the command failed
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        
    if output is not None:
        print(f"[+] Completed scanning {target}")
        print(f"[+] Results saved to {output_dir}/")
        return output_dir
    
    # If the output is None, it means the command completely failed
    print(f"[!] Issues encountered while scanning {target}, but continuing...")
    print(f"[+] Partial results may be available in {output_dir}/")
    return output_dir

def run_photon_on_multiple_targets(targets, photon_args, depth=30, debug=False):
    """Run Photon OSINT on multiple target URLs with user-specified arguments."""
    if not targets:
        print("[-] No targets to crawl with Photon.")
        return
    
    resources = calculate_resource_values(depth)
    max_targets = photon_args.get('max_targets') or resources["urls_to_crawl"]
    
    # Deduplicate targets while preserving order
    unique_targets = list(OrderedDict.fromkeys(targets))
    
    if len(unique_targets) > max_targets:
        print(f"[*] Limiting Photon crawl to top {max_targets} targets based on depth setting {depth}/100")
        targets_to_crawl = unique_targets[:max_targets]
    else:
        targets_to_crawl = unique_targets
    
    print(f"\n[+] Starting Photon crawler on {len(targets_to_crawl)} targets")
    
    results = []
    for i, target in enumerate(targets_to_crawl, 1):
        result_dir = run_photon_on_single_target(
            target, photon_args, depth, debug, i, len(targets_to_crawl)
        )
        if result_dir:
            results.append((target, result_dir))
    
    if results:
        print("\n[+] Photon crawling complete. Summary:")
        print(f"[+] All results saved to the 'photon_results/' directory")
        for target, output_dir in results:
            print(f"  - {target} -> {os.path.basename(output_dir)}/")
    else:
        print("\n[-] No successful Photon crawls.")

def ask_for_more():
    """Ask the user if they want more results."""
    while True:
        answer = input("\nMore results? (y/n): ").lower().strip()
        if answer in ['y', 'yes']:
            return True
        elif answer in ['n', 'no']:
            return False
        else:
            print("Please answer 'y' or 'n'.")

def is_url(text):
    """Check if the given text is a URL."""
    url_pattern = re.compile(r'^https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+')
    return bool(url_pattern.match(text))

def main():
    # Create argument parser with extensive options for both ddgr and photon
    parser = argparse.ArgumentParser(
        description="Enhanced Deep Search OSINT tool using Photon OSINT and ddgr with maximum results mode.",
        epilog=("Examples:\n"
                "  python3 deepsearch-proton.py \"search keywords\" 25                          # Search and crawl results with medium depth\n"
                "  python3 deepsearch-proton.py \"search keywords\" \"https://example.com\" 45    # Search, crawl specific site and search results\n"
                "  python3 deepsearch-proton.py --query 'osint tools' --depth 75 --no-crawl   # Search only, no crawling\n"
                "  python3 deepsearch-proton.py --target 'https://example.com' --depth 100     # Crawl only a specific site deeply\n"
                "  python3 deepsearch-proton.py \"keywords\" --max-pages 10 --auto-paginate     # Auto-paginate through 10 pages of results\n"
                "\nDepth Values (1-100):\n"
                "  1-20:   Quick/light crawl (fewer results, shallow depth, faster)\n"
                "  21-50:  Medium crawl (moderate results and depth)\n"
                "  51-80:  Thorough crawl (more results, deeper level)\n"
                "  81-100: Deep crawl (maximum results and depth level, longer runtime)"),
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    # Basic options
    basic_group = parser.add_argument_group('Basic Options')
    basic_group.add_argument('--query', type=str, 
                    help="Search query for DuckDuckGo (ddgr)")
    basic_group.add_argument('--target', type=str, 
                    help="Target URL for Photon OSINT scraping")
    basic_group.add_argument('--depth', type=int, default=30, 
                    help="Search depth (1-100): 1=quickest, 100=deepest. Controls number of results, crawl depth, and threads")
    basic_group.add_argument('--debug', action='store_true', 
                    help="Enable debug mode for detailed output and error messages")
    basic_group.add_argument('--no-crawl', action='store_true',
                    help="Disable automatic crawling of search results (search only)")
    basic_group.add_argument('--no-pagination', action='store_true',
                    help="Disable pagination ('more' prompt at the end)")
    basic_group.add_argument('--max-pages', type=int, metavar='N', default=0,
                    help="Maximum number of search result pages to fetch (0 for unlimited)")
    basic_group.add_argument('--auto-paginate', action='store_true',
                    help="Automatically fetch all pages without prompting")
    basic_group.add_argument('--version', action='version', 
                    version=f"%(prog)s {__version__}")
    
    # ddgr-specific options
    ddgr_group = parser.add_argument_group('DuckDuckGo (ddgr) Options')
    ddgr_group.add_argument('--ddgr-region', type=str, metavar='REG', default='us-en',
                    help="region-specific search e.g. 'us-en' for US (default)")
    ddgr_group.add_argument('--ddgr-colorize', type=str, choices=['auto', 'always', 'never'], default='auto',
                    help="whether to colorize output")
    ddgr_group.add_argument('--ddgr-nocolor', action='store_true',
                    help="equivalent to --ddgr-colorize=never")
    ddgr_group.add_argument('--ddgr-colors', type=str, metavar='COLORS',
                    help="set output colors")
    ddgr_group.add_argument('--ddgr-time', type=str, metavar='SPAN', choices=('d', 'w', 'm', 'y'),
                    help="time limit search [d (1 day), w (1 wk), m (1 month), y (1 year)]")
    ddgr_group.add_argument('--ddgr-site', type=str, metavar='SITE', action='append',
                    help="search sites using DuckDuckGo")
    ddgr_group.add_argument('--ddgr-expand', action='store_true',
                    help="Show complete url in search results")
    ddgr_group.add_argument('--ddgr-proxy', type=str, metavar='URI',
                    help="tunnel traffic through an HTTPS proxy; URI format: [http[s]://][user:pwd@]host[:port]")
    ddgr_group.add_argument('--ddgr-unsafe', action='store_true',
                    help="disable safe search")
    ddgr_group.add_argument('--ddgr-noua', action='store_true',
                    help="disable user agent")
    
    # Photon-specific options
    photon_group = parser.add_argument_group('Photon OSINT Options')
    photon_group.add_argument('--photon-level', type=int, metavar='LEVEL', 
                    help="levels to crawl (1-5)")
    photon_group.add_argument('--photon-threads', type=int, metavar='THREADS',
                    help="number of threads")
    photon_group.add_argument('--photon-delay', type=float, metavar='DELAY',
                    help="delay between requests")
    photon_group.add_argument('--photon-timeout', type=float, metavar='TIMEOUT',
                    help="http request timeout")
    photon_group.add_argument('--photon-cookie', type=str, metavar='COOKIE',
                    help="cookie")
    photon_group.add_argument('--photon-regex', type=str, metavar='REGEX',
                    help="regex pattern")
    photon_group.add_argument('--photon-export', type=str, metavar='FORMAT', choices=['csv', 'json'],
                    help="export format (csv, json)")
    photon_group.add_argument('--photon-seeds', type=str, metavar='SEEDS', action='append',
                    help="additional seed URLs")
    photon_group.add_argument('--photon-user-agent', type=str, metavar='UA',
                    help="custom user agent(s)")
    photon_group.add_argument('--photon-exclude', type=str, metavar='REGEX',
                    help="exclude URLs matching this regex")
    photon_group.add_argument('--photon-proxy', type=str, metavar='PROXY',
                    help="Proxy server IP:PORT or DOMAIN:PORT")
    photon_group.add_argument('--photon-verbose', action='store_true',
                    help="verbose output")
    photon_group.add_argument('--photon-headers', action='store_true',
                    help="add headers")
    photon_group.add_argument('--photon-dns', action='store_true',
                    help="enumerate subdomains and DNS data")
    photon_group.add_argument('--photon-keys', action='store_true',
                    help="find secret keys")
    photon_group.add_argument('--photon-only-urls', action='store_true',
                    help="only extract URLs")
    photon_group.add_argument('--photon-wayback', action='store_true',
                    help="fetch URLs from archive.org as seeds")
    photon_group.add_argument('--photon-max-targets', type=int, metavar='N',
                    help="maximum number of targets to crawl (overrides automatic scaling)")
    
    # Positional arguments (for simpler command line usage)
    parser.add_argument('keywords', type=str, nargs='?', 
                    help="Search keywords (e.g., \"George Washington\")")
    parser.add_argument('url_or_depth', type=str, nargs='?',
                    help="Either a URL to crawl (e.g., \"https://example.com\") or depth value (e.g., \"45\")")
    parser.add_argument('positional_depth', type=str, nargs='?',
                    help="Depth value when URL is provided (e.g., \"45\" when using format: \"keywords URL depth\")")
    
    args = parser.parse_args()

    # Configure logging based on the debug flag
    logging_level = logging.DEBUG if args.debug else logging.INFO
    logging.basicConfig(level=logging_level, format='[%(levelname)s] %(message)s')
    
    if args.debug:
        logging.debug("Debug mode enabled.")

    # Process positional arguments if provided
    if args.keywords:
        # We have at least the keywords argument
        query = args.keywords
        target = None
        depth = args.depth  # Default to named argument depth
        
        if args.url_or_depth:
            # Second argument could be either a URL or depth
            if is_url(args.url_or_depth):
                # It's a URL
                target = args.url_or_depth
                # If we have a third positional argument, it's the depth
                if args.positional_depth and args.positional_depth.isdigit():
                    depth = int(args.positional_depth)
            elif args.url_or_depth.isdigit():
                # It's a depth value
                depth = int(args.url_or_depth)
    else:
        # Use named arguments
        query = args.query
        target = args.target
        depth = args.depth
    
    # Ensure depth is within valid range
    if depth is not None:
        depth = max(1, min(100, depth))
    
    # Ensure we have at least one action to perform
    if not query and not target:
        parser.error("You must provide either keywords, a URL, or use --query/--target arguments.")
    
    # Initialize list of URLs to crawl with Photon
    urls_to_crawl = []
    
    # Add specifically provided target URL if any
    if target:
        urls_to_crawl.append(target)
    
    # Prepare ddgr arguments
    ddgr_args = {
        'region': args.ddgr_region,
        'colorize': args.ddgr_colorize,
        'nocolor': args.ddgr_nocolor,
        'colors': args.ddgr_colors,
        'time': args.ddgr_time,
        'site': args.ddgr_site,
        'expand': args.ddgr_expand,
        'proxy': args.ddgr_proxy,
        'unsafe': args.ddgr_unsafe,
        'noua': args.ddgr_noua
    }
    
    # Prepare photon arguments
    photon_args = {
        'level': args.photon_level,
        'threads': args.photon_threads,
        'delay': args.photon_delay,
        'timeout': args.photon_timeout,
        'cookie': args.photon_cookie,
        'regex': args.photon_regex,
        'export': args.photon_export,
        'seeds': args.photon_seeds,
        'user_agent': args.photon_user_agent,
        'exclude': args.photon_exclude,
        'proxy': args.photon_proxy,
        'verbose': args.photon_verbose,
        'headers': args.photon_headers,
        'dns': args.photon_dns,
        'keys': args.photon_keys,
        'only_urls': args.photon_only_urls,
        'wayback': args.photon_wayback,
        'max_targets': args.photon_max_targets
    }
    
    # Run ddgr search if query is provided
    if query:
        page = 1  # Start at page 1 (1-based for user display)
        has_more = True
        processed_urls = set()  # Keep track of all URLs found (as a set to avoid duplicates)
        stagnant_page_count = 0  # Track how many consecutive pages yield no new URLs
        
        print(f"\n[+] Running enhanced deep search for: {query}")
        print(f"[+] Search depth: {depth}/100 (Maximum results mode enabled)")
        
        while has_more:
            # Run search for current page with proper pagination
            search_urls, has_more = run_ddgr_with_pagination(query, ddgr_args, depth, args.debug, page)
            
            # Track if we found new URLs in this page
            found_new_urls = False
            
            # Process search results and add to deduplication set
            if search_urls:
                # Get only unique new URLs
                unique_new_urls = deduplicate_urls(search_urls, processed_urls)
                
                if unique_new_urls:
                    found_new_urls = True
                    print(f"[+] Found {len(unique_new_urls)} new unique URLs on page {page}")
                    print(f"[+] Total unique URLs found so far: {len(processed_urls)}")
                    
                    # Add search results to crawl list if auto-crawl is enabled
                    if not args.no_crawl:
                        # Run Photon only on the unique new URLs
                        run_photon_on_multiple_targets(unique_new_urls, photon_args, depth, args.debug)
                else:
                    print(f"[+] No new unique URLs found on page {page}")
                    stagnant_page_count += 1
            else:
                print(f"[+] No URLs found on page {page}")
                stagnant_page_count += 1
            
            # Intelligent pagination stop conditions:
            
            # 1. Check if we've had too many consecutive pages with no new results
            if stagnant_page_count >= 3:
                print(f"[+] No new unique URLs found for {stagnant_page_count} consecutive pages, stopping pagination")
                break
                
            # 2. Check if we've reached the maximum page limit
            if args.max_pages > 0 and page >= args.max_pages:
                print(f"[+] Reached maximum page limit ({args.max_pages} pages)")
                break
                
            # 3. Check if pagination is disabled or if we've reached the end
            if args.no_pagination or not has_more:
                break
                
            # 4. Reset stagnant page counter if we found new URLs
            if found_new_urls:
                stagnant_page_count = 0
            
            # 5. If auto-paginate is enabled, continue without asking
            if args.auto_paginate:
                page += 1
                print(f"[+] Automatically proceeding to page {page}...")
                continue
                
            # 6. Ask for more results
            if not ask_for_more():
                break
                
            # Move to next page
            page += 1
            
        print(f"\n[+] Search complete. Processed {page} page(s) with {len(processed_urls)} unique URLs.")
        
    elif target and not args.no_crawl:
        # If no query but a target was provided, crawl that target
        run_photon_on_single_target(target, photon_args, depth, args.debug)

if __name__ == '__main__':
    main()

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/sumsearch/ app.py
========================================

#!/usr/bin/env python3

import os
import re
import subprocess
import webbrowser
from flask import Flask, jsonify, request, send_from_directory
from flask_cors import CORS

# Change this path to the directory where you want to look for summary.txt files
BASE_DIRECTORY = "/home/jarvis/photon_results"

app = Flask(__name__, static_folder='static')
CORS(app)  # Enable CORS for all routes

# Regular expression pattern to match URLs
URL_PATTERN = r'https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+'

def gather_summary_data(base_path):
    """
    Recursively walk through 'base_path' and gather information about all 'summary.txt' files.
    Returns a list of dictionaries with id, folder_path and file_path.
    """
    all_summaries = []
    id_counter = 1

    for root, dirs, files in os.walk(base_path):
        for file in files:
            if file.lower() == "summary.txt":
                filepath = os.path.join(root, file)
                try:
                    relative_folder = os.path.relpath(root, base_path)
                    all_summaries.append({
                        "id": id_counter,
                        "folder": relative_folder,
                        "filepath": filepath
                    })
                    id_counter += 1
                except Exception as e:
                    print(f"Error processing {filepath}: {e}")

    return all_summaries

def get_summary_content(filepath):
    """Read and return the content of a summary file"""
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            content = f.read()
        return content
    except Exception as e:
        print(f"Error reading {filepath}: {e}")
        return f"Error reading file: {str(e)}"

def open_file_location(filepath):
    """Open the file's directory in the system file explorer"""
    directory = os.path.dirname(filepath)
    
    # Platform-specific commands to open the file explorer
    try:
        if os.name == 'nt':  # Windows
            os.startfile(directory)
        elif os.name == 'posix':  # Linux, macOS
            if os.system(f'xdg-open "{directory}"') != 0:  # Try Linux first
                os.system(f'open "{directory}"')  # Try macOS
        return True
    except Exception as e:
        print(f"Error opening location: {e}")
        return False

# API Routes
@app.route('/')
def serve_index():
    return send_from_directory('static', 'index.html')

@app.route('/api/summaries')
def get_summaries():
    """API endpoint to get all summaries"""
    try:
        summaries = gather_summary_data(BASE_DIRECTORY)
        return jsonify(summaries)
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/summary/<int:summary_id>')
def get_summary(summary_id):
    """API endpoint to get a specific summary's content"""
    try:
        summaries = gather_summary_data(BASE_DIRECTORY)
        
        # Find the summary with the matching ID
        summary = next((s for s in summaries if s["id"] == summary_id), None)
        
        if not summary:
            return jsonify({"error": "Summary not found"}), 404
        
        content = get_summary_content(summary["filepath"])
        
        return jsonify({
            "summary": summary,
            "content": content
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/open-location', methods=['POST'])
def api_open_location():
    """API endpoint to open a file location"""
    try:
        data = request.json
        if not data or 'path' not in data:
            return jsonify({"error": "No path provided"}), 400
        
        filepath = data['path']
        success = open_file_location(filepath)
        
        if success:
            return jsonify({"status": "success"})
        else:
            return jsonify({"error": "Failed to open location"}), 500
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/open-url', methods=['POST'])
def api_open_url():
    """API endpoint to open a URL in the browser"""
    try:
        data = request.json
        if not data or 'url' not in data:
            return jsonify({"error": "No URL provided"}), 400
        
        url = data['url']
        # Validate URL
        if not re.match(URL_PATTERN, url):
            return jsonify({"error": "Invalid URL"}), 400
            
        webbrowser.open(url)
        return jsonify({"status": "success"})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == "__main__":
    # Create the static directory if it doesn't exist
    os.makedirs('static', exist_ok=True)
    
    # Copy the HTML file to the static directory (in a real app, you'd have a build process)
    # For now, assuming the HTML file is in the same directory as this script
    try:
        with open('frontend.html', 'r') as src, open('static/index.html', 'w') as dst:
            dst.write(src.read())
    except FileNotFoundError:
        print("Warning: frontend.html not found. Please place the HTML file in the static directory.")
    
    # Start the Flask server
    print(f"Starting server on http://localhost:5000")
    app.run(debug=True, host='0.0.0.0', port=5000)

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/proton.py
========================================

#!/usr/bin/env python3
import argparse
import subprocess
import sys
import logging
import shutil
import traceback
import json
import os
import re
import time
import textwrap
from collections import OrderedDict

__version__ = "1.8"

# Path to your virtual environment activation script and Photon script
VENV_PATH = os.path.expanduser("~/newvenv/bin/activate")
PHOTON_SCRIPT = os.path.join(os.path.dirname(os.path.realpath(__file__)), "photon.py")

def check_command(command):
    """Verify if a command is available in the system PATH."""
    if shutil.which(command) is None:
        logging.error(f"'{command}' is not installed. Please install it to proceed.")
        sys.exit(1)
    logging.debug(f"'{command}' is available.")

def pretty_print_json(output):
    """Attempt to parse and pretty-print JSON output; if it fails, return raw output."""
    try:
        data = json.loads(output)
        return json.dumps(data, indent=2)
    except json.JSONDecodeError:
        return output

def run_command(command, label, debug=False, use_venv=False, ignore_errors=False):
    """
    Execute a command, optionally within a virtual environment.
    Returns the command output as a string.
    """
    try:
        if use_venv:
            # For commands like Photon that must run inside your virtualenv,
            # we assume python3 is available.
            check_command("python3")
        else:
            check_command(command[0])
        logging.debug(f"Executing command: {' '.join(command)}")
        
        if use_venv:
            # Combine the activation command with the target command
            full_command = f"source {VENV_PATH} && " + " ".join(command)
            result = subprocess.run(full_command, shell=True, capture_output=True, text=True, executable="/bin/bash")
        else:
            result = subprocess.run(command, capture_output=True, text=True)

        if result.returncode != 0:
            logging.error(f"Error running {label}: {result.stderr.strip()}")
            if debug:
                logging.debug("Full stderr: " + result.stderr)
            if ignore_errors:
                # Return empty result but don't fail completely
                return ""
            return None
        logging.debug(f"{label} output: {result.stdout.strip()}")
        return result.stdout.strip()
    except Exception as exc:
        logging.error(f"Exception running {label}: {exc}")
        if debug:
            traceback.print_exc()
        return None

def calculate_resource_values(depth):
    """
    Calculate appropriate resource values based on the depth parameter (1-100).
    Returns a dictionary with calibrated values for different tools.
    
    Depth scale:
    1-20: Light/Quick search
    21-50: Medium search
    51-80: Thorough search
    81-100: Deep/Extensive search
    """
    # Ensure depth is within valid range (now 1-100)
    depth = max(1, min(100, depth))
    
    # Calculate ddgr results - maintain a reasonable page size
    # We'll use constant page size of 10 for better UX, but adjust total results
    ddgr_page_size = 10
    
    # Calculate ddgr total results based on depth
    if depth <= 20:
        ddgr_results = 10  # Light search
    elif depth <= 50:
        ddgr_results = 15  # Medium search
    elif depth <= 80:
        ddgr_results = 20  # Thorough search
    else:
        ddgr_results = 25  # Deep search (max for ddgr)
    
    # Calculate Photon crawl parameters
    # Level: 1 to 5 (Photon's internal crawl depth)
    photon_level = 1 + int((depth - 1) / 20)  # Ranges from 1-5
    
    # Threads: 2 to 50 (threading for parallel processing)
    photon_threads = 2 + int((depth - 1) * 0.48)  # Linear scaling
    
    # Timeout: 5s to 30s (longer for deeper crawls)
    photon_timeout = 5 + int((depth - 1) * 0.25)  # Linear scaling
    
    # URLs to crawl per page: proportional to depth
    if depth <= 20:
        urls_percent = 0.3  # Crawl 30% of results for light search
    elif depth <= 50:
        urls_percent = 0.5  # Crawl 50% of results for medium search
    elif depth <= 80:
        urls_percent = 0.7  # Crawl 70% of results for thorough search
    else:
        urls_percent = 1.0  # Crawl 100% of results for deep search
    
    urls_to_crawl = max(1, min(ddgr_page_size, round(ddgr_page_size * urls_percent)))
    
    return {
        "ddgr_results": ddgr_results,
        "ddgr_page_size": ddgr_page_size,
        "photon_threads": photon_threads,
        "photon_level": photon_level,
        "photon_timeout": photon_timeout,
        "urls_to_crawl": urls_to_crawl
    }

def build_ddgr_command(query, ddgr_args, page_size=10):
    """Build the ddgr command with all user-supplied arguments."""
    command = ['ddgr', '--json']
    
    # Ensure we're using a proper page size for ddgr
    command.extend(['--num', str(page_size)])
    
    # Add all ddgr-specific arguments
    if ddgr_args.get('region'):
        command.extend(['--reg', ddgr_args.get('region')])
    if ddgr_args.get('colorize'):
        command.extend(['--colorize', ddgr_args.get('colorize')])
    if ddgr_args.get('nocolor'):
        command.append('--nocolor')
    if ddgr_args.get('colors'):
        command.extend(['--colors', ddgr_args.get('colors')])
    if ddgr_args.get('time'):
        command.extend(['--time', ddgr_args.get('time')])
    if ddgr_args.get('site'):
        for site in ddgr_args.get('site'):
            command.extend(['--site', site])
    if ddgr_args.get('expand'):
        command.append('--expand')
    if ddgr_args.get('proxy'):
        command.extend(['--proxy', ddgr_args.get('proxy')])
    if ddgr_args.get('unsafe'):
        command.append('--unsafe')
    if ddgr_args.get('noua'):
        command.append('--noua')
    
    # Add the query
    command.append(query)
    
    return command

def run_ddgr_with_pagination(query, ddgr_args, depth=30, debug=False, page=1):
    """
    Run DuckDuckGo search using ddgr with proper pagination support.
    This function uses ddgr's interactive mode and simulates pagination commands.
    
    Parameters:
    - query: The search query string
    - ddgr_args: Dictionary of ddgr-specific arguments
    - depth: Depth parameter (1-100)
    - debug: Whether to enable debug output
    - page: Page number (1-based for user display)
    
    Returns:
    - List of URLs from search results
    - Boolean indicating if there might be more results
    """
    resources = calculate_resource_values(depth)
    page_size = resources["ddgr_page_size"]
    
    # For first page, search normally
    if page == 1:
        print(f"\n[+] DuckDuckGo (ddgr) search results for: {query}")
        print(f"[+] Search depth: {depth}/100 (Page {page})\n")
        command = build_ddgr_command(query, ddgr_args, page_size)
    else:
        print(f"\n[+] DuckDuckGo (ddgr) search results - Page {page} for: {query}")
        print(f"[+] Search depth: {depth}/100\n")
        
        # For subsequent pages, we need to simulate pagination:
        # 1. Run ddgr in non-interactive mode first to get initial results
        # 2. Then run multiple "next page" commands to get to the desired page
        
        # First, run the initial query with more results to move through pages faster
        # Use the max limit of 25 to reduce the number of pagination steps needed
        initial_results = 25
        command = ['ddgr', '--json', '--num', str(initial_results), query]
        # Add other args
        if ddgr_args.get('region'):
            command.extend(['--reg', ddgr_args.get('region')])
        if ddgr_args.get('time'):
            command.extend(['--time', ddgr_args.get('time')])
        if ddgr_args.get('site'):
            for site in ddgr_args.get('site'):
                command.extend(['--site', site])
        if ddgr_args.get('unsafe'):
            command.append('--unsafe')
    
    output = run_command(command, "ddgr", debug)
    if not output:
        print("[-] No results from ddgr search")
        return [], False
    
    urls = []
    try:
        results = json.loads(output)
        
        # If we're on a later page, we need to extract the correct subset of results
        result_offset = 0
        if page > 1:
            # For page 2, we want results 10-19 (assuming page_size=10)
            # For page 3, we want results 20-29, etc.
            result_offset = (page - 1) * page_size
            
            # If offset is beyond available results, no more results
            if result_offset >= len(results):
                print("[-] No more results available.")
                return [], False
            
            # Get the slice of results for this page
            end_offset = min(result_offset + page_size, len(results))
            page_results = results[result_offset:end_offset]
        else:
            # First page, just take the first page_size results
            page_results = results[:page_size]
        
        # Process and display the results
        for i, result in enumerate(page_results, 1):
            title = result.get("title", "No Title")
            url = result.get("url", "No URL")
            abstract = result.get("abstract", "")
            
            # Add URLs to the list for Photon crawling
            urls.append(url)
            
            # Display result with correct global index
            global_index = result_offset + i
            print(f"{global_index}. {title}")
            print(f"   {url}")
            if abstract:
                wrapped_abstract = textwrap.fill(abstract, width=80, initial_indent="   ", subsequent_indent="   ")
                print(f"{wrapped_abstract}\n")
            else:
                print()  # Empty line for spacing
        
        # Determine if there might be more results
        # We consider there are more if:
        # 1. We got a full page of results, or
        # 2. We know there are more results in our fetched batch
        has_more = (len(page_results) == page_size) or (result_offset + len(page_results) < len(results))
        
        return urls, has_more
    
    except json.JSONDecodeError:
        logging.error("Failed to parse ddgr JSON output.")
        print(output)
        return [], False

def build_photon_command(target, photon_args, output_dir):
    """Build the Photon command with all user-supplied arguments."""
    command = [
        'python3', PHOTON_SCRIPT,
        '-u', target,
        '-o', output_dir
    ]
    
    # Add all photon-specific arguments
    if photon_args.get('level') is not None:
        command.extend(['-l', str(photon_args.get('level'))])
    if photon_args.get('threads') is not None:
        command.extend(['-t', str(photon_args.get('threads'))])
    if photon_args.get('delay') is not None:
        command.extend(['-d', str(photon_args.get('delay'))])
    if photon_args.get('timeout') is not None:
        command.extend(['--timeout', str(photon_args.get('timeout'))])
    if photon_args.get('cookie'):
        command.extend(['-c', photon_args.get('cookie')])
    if photon_args.get('regex'):
        command.extend(['-r', photon_args.get('regex')])
    if photon_args.get('export'):
        command.extend(['-e', photon_args.get('export')])
    if photon_args.get('seeds'):
        command.extend(['-s'] + photon_args.get('seeds'))
    if photon_args.get('user_agent'):
        command.extend(['--user-agent', photon_args.get('user_agent')])
    if photon_args.get('exclude'):
        command.extend(['--exclude', photon_args.get('exclude')])
    if photon_args.get('proxy'):
        command.extend(['-p', photon_args.get('proxy')])
    
    # Add boolean flags
    if photon_args.get('verbose'):
        command.append('-v')
    if photon_args.get('headers'):
        command.append('--headers')
    if photon_args.get('dns'):
        command.append('--dns')
    if photon_args.get('keys'):
        command.append('--keys')
    if photon_args.get('only_urls'):
        command.append('--only-urls')
    if photon_args.get('wayback'):
        command.append('--wayback')
    
    return command

def run_photon_on_single_target(target, photon_args, depth=30, debug=False, index=None, total=None):
    """Run Photon OSINT on a single target URL with user-specified arguments."""
    resources = calculate_resource_values(depth)
    
    # Use calculated resources if not provided in photon_args
    level = photon_args.get('level') or resources["photon_level"]
    threads = photon_args.get('threads') or resources["photon_threads"]
    timeout = photon_args.get('timeout') or resources["photon_timeout"]
    
    # Create a progress indicator if we're processing multiple URLs
    progress_str = ""
    if index is not None and total is not None:
        progress_str = f"[{index}/{total}] "
    
    print(f"\n[+] {progress_str}Photon crawling target: {target}")
    print(f"[+] Crawl depth: {depth}/100 (level: {level}, threads: {threads}, timeout: {timeout}s)")
    
    if not os.path.isfile(PHOTON_SCRIPT):
        logging.error(f"Photon script not found at {PHOTON_SCRIPT}. Please ensure photon.py is available.")
        return None
    
    # Create organized output folder structure
    main_output_dir = "photon_results"
    if not os.path.exists(main_output_dir):
        os.makedirs(main_output_dir)
    
    # Get domain name for subfolder
    domain = target.replace("https://", "").replace("http://", "").split("/")[0]
    domain_safe = re.sub(r'[^\w\-_]', '_', domain)  # Make domain name safe for filesystem
    
    # Create a unique subfolder for this target
    timestamp = int(time.time())
    target_dir = f"{domain_safe}_{timestamp}"
    output_dir = os.path.join(main_output_dir, target_dir)
    
    # Build the Photon command
    command = build_photon_command(target, photon_args, output_dir)
    
    # Run the command with the proper settings and handle failures gracefully
    output = run_command(command, f"Photon OSINT on {target}", debug, use_venv=True, ignore_errors=True)
    
    # Create the output directory even if the command failed
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        
    if output is not None:
        print(f"[+] Completed scanning {target}")
        print(f"[+] Results saved to {output_dir}/")
        return output_dir
    
    # If the output is None, it means the command completely failed
    print(f"[!] Issues encountered while scanning {target}, but continuing...")
    print(f"[+] Partial results may be available in {output_dir}/")
    return output_dir

def run_photon_on_multiple_targets(targets, photon_args, depth=30, debug=False):
    """Run Photon OSINT on multiple target URLs with user-specified arguments."""
    if not targets:
        print("[-] No targets to crawl with Photon.")
        return
    
    resources = calculate_resource_values(depth)
    max_targets = photon_args.get('max_targets') or resources["urls_to_crawl"]
    
    # Deduplicate targets while preserving order
    unique_targets = list(OrderedDict.fromkeys(targets))
    
    if len(unique_targets) > max_targets:
        print(f"[*] Limiting Photon crawl to top {max_targets} targets based on depth setting {depth}/100")
        targets_to_crawl = unique_targets[:max_targets]
    else:
        targets_to_crawl = unique_targets
    
    print(f"\n[+] Starting Photon crawler on {len(targets_to_crawl)} targets")
    
    results = []
    for i, target in enumerate(targets_to_crawl, 1):
        result_dir = run_photon_on_single_target(
            target, photon_args, depth, debug, i, len(targets_to_crawl)
        )
        if result_dir:
            results.append((target, result_dir))
    
    if results:
        print("\n[+] Photon crawling complete. Summary:")
        print(f"[+] All results saved to the 'photon_results/' directory")
        for target, output_dir in results:
            print(f"  - {target} -> {os.path.basename(output_dir)}/")
    else:
        print("\n[-] No successful Photon crawls.")

def ask_for_more():
    """Ask the user if they want more results."""
    while True:
        answer = input("\nMore results? (y/n): ").lower().strip()
        if answer in ['y', 'yes']:
            return True
        elif answer in ['n', 'no']:
            return False
        else:
            print("Please answer 'y' or 'n'.")

def is_url(text):
    """Check if the given text is a URL."""
    url_pattern = re.compile(r'^https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+')
    return bool(url_pattern.match(text))

def main():
    # Create argument parser with extensive options for both ddgr and photon
    parser = argparse.ArgumentParser(
        description="Enhanced OSINT tool using Photon OSINT and ddgr with pagination support.",
        epilog=("Examples:\n"
                "  python3 proton.py \"search keywords\" 25                          # Search and crawl results with medium depth\n"
                "  python3 proton.py \"search keywords\" \"https://example.com\" 45    # Search, crawl specific site and search results\n"
                "  python3 proton.py --query 'osint tools' --depth 75 --no-crawl   # Search only, no crawling\n"
                "  python3 proton.py --target 'https://example.com' --depth 100     # Crawl only a specific site deeply\n"
                "\nDepth Values (1-100):\n"
                "  1-20:   Quick/light crawl (fewer results, shallow depth, faster)\n"
                "  21-50:  Medium crawl (moderate results and depth)\n"
                "  51-80:  Thorough crawl (more results, deeper level)\n"
                "  81-100: Deep crawl (maximum results and depth level, longer runtime)"),
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    # Basic options
    basic_group = parser.add_argument_group('Basic Options')
    basic_group.add_argument('--query', type=str, 
                    help="Search query for DuckDuckGo (ddgr)")
    basic_group.add_argument('--target', type=str, 
                    help="Target URL for Photon OSINT scraping")
    basic_group.add_argument('--depth', type=int, default=30, 
                    help="Search depth (1-100): 1=quickest, 100=deepest. Controls number of results, crawl depth, and threads")
    basic_group.add_argument('--debug', action='store_true', 
                    help="Enable debug mode for detailed output and error messages")
    basic_group.add_argument('--no-crawl', action='store_true',
                    help="Disable automatic crawling of search results (search only)")
    basic_group.add_argument('--no-pagination', action='store_true',
                    help="Disable pagination ('more' prompt at the end)")
    basic_group.add_argument('--version', action='version', 
                    version=f"%(prog)s {__version__}")
    
    # ddgr-specific options
    ddgr_group = parser.add_argument_group('DuckDuckGo (ddgr) Options')
    ddgr_group.add_argument('--ddgr-region', type=str, metavar='REG', default='us-en',
                    help="region-specific search e.g. 'us-en' for US (default)")
    ddgr_group.add_argument('--ddgr-colorize', type=str, choices=['auto', 'always', 'never'], default='auto',
                    help="whether to colorize output")
    ddgr_group.add_argument('--ddgr-nocolor', action='store_true',
                    help="equivalent to --ddgr-colorize=never")
    ddgr_group.add_argument('--ddgr-colors', type=str, metavar='COLORS',
                    help="set output colors")
    ddgr_group.add_argument('--ddgr-time', type=str, metavar='SPAN', choices=('d', 'w', 'm', 'y'),
                    help="time limit search [d (1 day), w (1 wk), m (1 month), y (1 year)]")
    ddgr_group.add_argument('--ddgr-site', type=str, metavar='SITE', action='append',
                    help="search sites using DuckDuckGo")
    ddgr_group.add_argument('--ddgr-expand', action='store_true',
                    help="Show complete url in search results")
    ddgr_group.add_argument('--ddgr-proxy', type=str, metavar='URI',
                    help="tunnel traffic through an HTTPS proxy; URI format: [http[s]://][user:pwd@]host[:port]")
    ddgr_group.add_argument('--ddgr-unsafe', action='store_true',
                    help="disable safe search")
    ddgr_group.add_argument('--ddgr-noua', action='store_true',
                    help="disable user agent")
    
    # Photon-specific options
    photon_group = parser.add_argument_group('Photon OSINT Options')
    photon_group.add_argument('--photon-level', type=int, metavar='LEVEL', 
                    help="levels to crawl (1-5)")
    photon_group.add_argument('--photon-threads', type=int, metavar='THREADS',
                    help="number of threads")
    photon_group.add_argument('--photon-delay', type=float, metavar='DELAY',
                    help="delay between requests")
    photon_group.add_argument('--photon-timeout', type=float, metavar='TIMEOUT',
                    help="http request timeout")
    photon_group.add_argument('--photon-cookie', type=str, metavar='COOKIE',
                    help="cookie")
    photon_group.add_argument('--photon-regex', type=str, metavar='REGEX',
                    help="regex pattern")
    photon_group.add_argument('--photon-export', type=str, metavar='FORMAT', choices=['csv', 'json'],
                    help="export format (csv, json)")
    photon_group.add_argument('--photon-seeds', type=str, metavar='SEEDS', action='append',
                    help="additional seed URLs")
    photon_group.add_argument('--photon-user-agent', type=str, metavar='UA',
                    help="custom user agent(s)")
    photon_group.add_argument('--photon-exclude', type=str, metavar='REGEX',
                    help="exclude URLs matching this regex")
    photon_group.add_argument('--photon-proxy', type=str, metavar='PROXY',
                    help="Proxy server IP:PORT or DOMAIN:PORT")
    photon_group.add_argument('--photon-verbose', action='store_true',
                    help="verbose output")
    photon_group.add_argument('--photon-headers', action='store_true',
                    help="add headers")
    photon_group.add_argument('--photon-dns', action='store_true',
                    help="enumerate subdomains and DNS data")
    photon_group.add_argument('--photon-keys', action='store_true',
                    help="find secret keys")
    photon_group.add_argument('--photon-only-urls', action='store_true',
                    help="only extract URLs")
    photon_group.add_argument('--photon-wayback', action='store_true',
                    help="fetch URLs from archive.org as seeds")
    photon_group.add_argument('--photon-max-targets', type=int, metavar='N',
                    help="maximum number of targets to crawl (overrides automatic scaling)")
    
    # Positional arguments (for simpler command line usage)
    parser.add_argument('keywords', type=str, nargs='?', 
                    help="Search keywords (e.g., \"George Washington\")")
    parser.add_argument('url_or_depth', type=str, nargs='?',
                    help="Either a URL to crawl (e.g., \"https://example.com\") or depth value (e.g., \"45\")")
    parser.add_argument('positional_depth', type=str, nargs='?',
                    help="Depth value when URL is provided (e.g., \"45\" when using format: \"keywords URL depth\")")
    
    args = parser.parse_args()

    # Configure logging based on the debug flag
    logging_level = logging.DEBUG if args.debug else logging.INFO
    logging.basicConfig(level=logging_level, format='[%(levelname)s] %(message)s')
    
    if args.debug:
        logging.debug("Debug mode enabled.")

    # Process positional arguments if provided
    if args.keywords:
        # We have at least the keywords argument
        query = args.keywords
        target = None
        depth = args.depth  # Default to named argument depth
        
        if args.url_or_depth:
            # Second argument could be either a URL or depth
            if is_url(args.url_or_depth):
                # It's a URL
                target = args.url_or_depth
                # If we have a third positional argument, it's the depth
                if args.positional_depth and args.positional_depth.isdigit():
                    depth = int(args.positional_depth)
            elif args.url_or_depth.isdigit():
                # It's a depth value
                depth = int(args.url_or_depth)
    else:
        # Use named arguments
        query = args.query
        target = args.target
        depth = args.depth
    
    # Ensure depth is within valid range
    if depth is not None:
        depth = max(1, min(100, depth))
    
    # Ensure we have at least one action to perform
    if not query and not target:
        parser.error("You must provide either keywords, a URL, or use --query/--target arguments.")
    
    # Initialize list of URLs to crawl with Photon
    urls_to_crawl = []
    
    # Add specifically provided target URL if any
    if target:
        urls_to_crawl.append(target)
    
    # Prepare ddgr arguments
    ddgr_args = {
        'region': args.ddgr_region,
        'colorize': args.ddgr_colorize,
        'nocolor': args.ddgr_nocolor,
        'colors': args.ddgr_colors,
        'time': args.ddgr_time,
        'site': args.ddgr_site,
        'expand': args.ddgr_expand,
        'proxy': args.ddgr_proxy,
        'unsafe': args.ddgr_unsafe,
        'noua': args.ddgr_noua
    }
    
    # Prepare photon arguments
    photon_args = {
        'level': args.photon_level,
        'threads': args.photon_threads,
        'delay': args.photon_delay,
        'timeout': args.photon_timeout,
        'cookie': args.photon_cookie,
        'regex': args.photon_regex,
        'export': args.photon_export,
        'seeds': args.photon_seeds,
        'user_agent': args.photon_user_agent,
        'exclude': args.photon_exclude,
        'proxy': args.photon_proxy,
        'verbose': args.photon_verbose,
        'headers': args.photon_headers,
        'dns': args.photon_dns,
        'keys': args.photon_keys,
        'only_urls': args.photon_only_urls,
        'wayback': args.photon_wayback,
        'max_targets': args.photon_max_targets
    }
    
    # Run ddgr search if query is provided
    if query:
        page = 1  # Start at page 1 (1-based for user display)
        has_more = True
        
        while has_more:
            # Run search for current page with proper pagination
            search_urls, has_more = run_ddgr_with_pagination(query, ddgr_args, depth, args.debug, page)
            
            # Add search results to crawl list if auto-crawl is enabled
            if not args.no_crawl and search_urls:
                # Run Photon on collected URLs for this page
                run_photon_on_multiple_targets(search_urls, photon_args, depth, args.debug)
            
            # Check if pagination is disabled or if we've reached the end
            if args.no_pagination or not has_more:
                break
                
            # Ask for more results
            if not ask_for_more():
                break
                
            # Move to next page
            page += 1
            
    elif target and not args.no_crawl:
        # If no query but a target was provided, crawl that target
        run_photon_on_single_target(target, photon_args, depth, args.debug)

if __name__ == '__main__':
    main()

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/photon.py
========================================

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""The Photon main part."""
from __future__ import print_function

import argparse
import os
import re
import requests
import sys
import time
import warnings
import random

from core.colors import good, info, run, green, red, white, end, bad

# Just a fancy ass banner
print('''%s      ____  __          __
     / %s__%s \/ /_  ____  / /_____  ____
    / %s/_/%s / __ \/ %s__%s \/ __/ %s__%s \/ __ \\
   / ____/ / / / %s/_/%s / /_/ %s/_/%s / / / /
  /_/   /_/ /_/\____/\__/\____/_/ /_/ %sv1.3.2%s\n''' %
      (red, white, red, white, red, white, red, white, red, white, red, white,
       red, white, end))

try:
    from urllib.parse import urlparse  # For Python 3
except ImportError:
    print('%s Photon runs only on Python 3.2 and above.' % info)
    quit()

import core.config
from core.config import INTELS
from core.flash import flash
from core.mirror import mirror
from core.prompt import prompt
from core.requester import requester
from core.updater import updater
from core.utils import (luhn,
                        proxy_type,
                        is_good_proxy,
                        top_level,
                        extract_headers,
                        verb, is_link,
                        entropy, regxy,
                        remove_regex,
                        timer,
                        writer)
from core.regex import rintels, rendpoint, rhref, rscript, rentropy

from core.zap import zap

# Disable SSL related warnings
warnings.filterwarnings('ignore')

# Processing command line arguments
parser = argparse.ArgumentParser()
# Options
parser.add_argument('-u', '--url', help='root url', dest='root')
parser.add_argument('-c', '--cookie', help='cookie', dest='cook')
parser.add_argument('-r', '--regex', help='regex pattern', dest='regex')
parser.add_argument('-e', '--export', help='export format', dest='export', choices=['csv', 'json'])
parser.add_argument('-o', '--output', help='output directory', dest='output')
parser.add_argument('-l', '--level', help='levels to crawl', dest='level',
                    type=int)
parser.add_argument('-t', '--threads', help='number of threads', dest='threads',
                    type=int)
parser.add_argument('-d', '--delay', help='delay between requests',
                    dest='delay', type=float)
parser.add_argument('-v', '--verbose', help='verbose output', dest='verbose',
                    action='store_true')
parser.add_argument('-s', '--seeds', help='additional seed URLs', dest='seeds',
                    nargs="+", default=[])
parser.add_argument('--stdout', help='send variables to stdout', dest='std')
parser.add_argument('--user-agent', help='custom user agent(s)',
                    dest='user_agent')
parser.add_argument('--exclude', help='exclude URLs matching this regex',
                    dest='exclude')
parser.add_argument('--timeout', help='http request timeout', dest='timeout',
                    type=float)
parser.add_argument('-p', '--proxy', help='Proxy server IP:PORT or DOMAIN:PORT', dest='proxies',
                    type=proxy_type)

# Switches
parser.add_argument('--clone', help='clone the website locally', dest='clone',
                    action='store_true')
parser.add_argument('--headers', help='add headers', dest='headers',
                    action='store_true')
parser.add_argument('--dns', help='enumerate subdomains and DNS data',
                    dest='dns', action='store_true')
parser.add_argument('--keys', help='find secret keys', dest='api',
                    action='store_true')
parser.add_argument('--update', help='update photon', dest='update',
                    action='store_true')
parser.add_argument('--only-urls', help='only extract URLs', dest='only_urls',
                    action='store_true')
parser.add_argument('--wayback', help='fetch URLs from archive.org as seeds',
                    dest='archive', action='store_true')
args = parser.parse_args()


# If the user has supplied --update argument
if args.update:
    updater()
    quit()

# If the user has supplied a URL
if args.root:
    main_inp = args.root
    if main_inp.endswith('/'):
        # We will remove it as it can cause problems later in the code
        main_inp = main_inp[:-1]
# If the user hasn't supplied an URL
else:
    print('\n' + parser.format_help().lower())
    quit()

clone = args.clone
headers = args.headers  # prompt for headers
verbose = args.verbose  # verbose output
delay = args.delay or 0  # Delay between requests
timeout = args.timeout or 6  # HTTP request timeout
cook = args.cook or None  # Cookie
api = bool(args.api)  # Extract high entropy strings i.e. API keys and stuff

proxies = []
if args.proxies:
    print("%s Testing proxies, can take a while..." % info)
    for proxy in args.proxies:
        if is_good_proxy(proxy):
            proxies.append(proxy)
        else:
            print("%s Proxy %s doesn't seem to work or timedout" %
                  (bad, proxy['http']))
    print("%s Done" % info)
    if not proxies:
        print("%s no working proxies, quitting!" % bad)
        exit()
else:
    proxies.append(None)

crawl_level = args.level or 2  # Crawling level
thread_count = args.threads or 2  # Number of threads
only_urls = bool(args.only_urls)  # Only URLs mode is off by default

# Variables we are gonna use later to store stuff
keys = set()  # High entropy strings, prolly secret keys
files = set()  # The pdf, css, png, etc files.
intel = set()  # The email addresses, website accounts, AWS buckets etc.
robots = set()  # The entries of robots.txt
custom = set()  # Strings extracted by custom regex pattern
failed = set()  # URLs that photon failed to crawl
scripts = set()  # THe Javascript files
external = set()  # URLs that don't belong to the target i.e. out-of-scope
# URLs that have get params in them e.g. example.com/page.php?id=2
fuzzable = set()
endpoints = set()  # URLs found from javascript files
processed = set(['dummy'])  # URLs that have been crawled
# URLs that belong to the target i.e. in-scope
internal = set(args.seeds)

everything = []
bad_scripts = set()  # Unclean javascript file urls
bad_intel = set() # needed for intel filtering

core.config.verbose = verbose

if headers:
    try:
        prompt = prompt()
    except FileNotFoundError as e:
        print('Could not load headers prompt: {}'.format(e))
        quit()
    headers = extract_headers(prompt)

# If the user hasn't supplied the root URL with http(s), we will handle it
if main_inp.startswith('http'):
    main_url = main_inp
else:
    try:
        requests.get('https://' + main_inp, proxies=random.choice(proxies))
        main_url = 'https://' + main_inp
    except:
        main_url = 'http://' + main_inp

schema = main_url.split('//')[0] # https: or http:?
# Adding the root URL to internal for crawling
internal.add(main_url)
# Extracts host out of the URL
host = urlparse(main_url).netloc

output_dir = args.output or host

try:
    domain = top_level(main_url)
except:
    domain = host

if args.user_agent:
    user_agents = args.user_agent.split(',')
else:
    with open(sys.path[0] + '/core/user-agents.txt', 'r') as uas:
        user_agents = [agent.strip('\n') for agent in uas]


supress_regex = False

def intel_extractor(url, response):
    """Extract intel from the response body."""
    for rintel in rintels:
        res = re.sub(r'<(script).*?</\1>(?s)', '', response)
        res = re.sub(r'<[^<]+?>', '', res)
        matches = rintel[0].findall(res)
        if matches:
            for match in matches:
                verb('Intel', match)
                bad_intel.add((match, rintel[1], url))


def js_extractor(response):
    """Extract js files from the response body"""
    # Extract .js files
    matches = rscript.findall(response)
    for match in matches:
        match = match[2].replace('\'', '').replace('"', '')
        verb('JS file', match)
        bad_scripts.add(match)

def remove_file(url):
    if url.count('/') > 2:
        replacable = re.search(r'/[^/]*?$', url).group()
        if replacable != '/':
            return url.replace(replacable, '')
        else:
            return url
    else:
        return url

def extractor(url):
    """Extract details from the response body."""
    response = requester(url, main_url, delay, cook, headers, timeout, host, proxies, user_agents, failed, processed)
    if clone:
        mirror(url, response)
    matches = rhref.findall(response)
    for link in matches:
        # Remove everything after a "#" to deal with in-page anchors
        link = link[1].replace('\'', '').replace('"', '').split('#')[0]
        # Checks if the URLs should be crawled
        if is_link(link, processed, files):
            if link[:4] == 'http':
                if link.startswith(main_url):
                    verb('Internal page', link)
                    internal.add(link)
                else:
                    verb('External page', link)
                    external.add(link)
            elif link[:2] == '//':
                if link.split('/')[2].startswith(host):
                    verb('Internal page', link)
                    internal.add(schema + '://' + link)
                else:
                    verb('External page', link)
                    external.add(link)
            elif link[:1] == '/':
                verb('Internal page', link)
                internal.add(remove_file(url) + link)
            else:
                verb('Internal page', link)
                usable_url = remove_file(url)
                if usable_url.endswith('/'):
                    internal.add(usable_url + link)
                elif link.startswith('/'):
                    internal.add(usable_url + link)
                else:
                    internal.add(usable_url + '/' + link)

    if not only_urls:
        intel_extractor(url, response)
        js_extractor(response)
    if args.regex and not supress_regex:
        regxy(args.regex, response, supress_regex, custom)
    if api:
        matches = rentropy.findall(response)
        for match in matches:
            if entropy(match) >= 4:
                verb('Key', match)
                keys.add(url + ': ' + match)


def jscanner(url):
    """Extract endpoints from JavaScript code."""
    response = requester(url, main_url, delay, cook, headers, timeout, host, proxies, user_agents, failed, processed)
    # Extract URLs/endpoints
    matches = rendpoint.findall(response)
    # Iterate over the matches, match is a tuple
    for match in matches:
        # Combining the items because one of them is always empty
        match = match[0] + match[1]
        # Making sure it's not some JavaScript code
        if not re.search(r'[}{><"\']', match) and not match == '/':
            verb('JS endpoint', match)
            endpoints.add(match)


# Records the time at which crawling started
then = time.time()

# Step 1. Extract urls from robots.txt & sitemap.xml
zap(main_url, args.archive, domain, host, internal, robots, proxies)

# This is so the level 1 emails are parsed as well
internal = set(remove_regex(internal, args.exclude))

# Step 2. Crawl recursively to the limit specified in "crawl_level"
for level in range(crawl_level):
    # Links to crawl = (all links - already crawled links) - links not to crawl
    links = remove_regex(internal - processed, args.exclude)
    # If links to crawl are 0 i.e. all links have been crawled
    if not links:
        break
    # if crawled links are somehow more than all links. Possible? ;/
    elif len(internal) <= len(processed):
        if len(internal) > 2 + len(args.seeds):
            break
    print('%s Level %i: %i URLs' % (run, level + 1, len(links)))
    try:
        flash(extractor, links, thread_count)
    except KeyboardInterrupt:
        print('')
        break

if not only_urls:
    for match in bad_scripts:
        if match.startswith(main_url):
            scripts.add(match)
        elif match.startswith('/') and not match.startswith('//'):
            scripts.add(main_url + match)
        elif not match.startswith('http') and not match.startswith('//'):
            scripts.add(main_url + '/' + match)
    # Step 3. Scan the JavaScript files for endpoints
    print('%s Crawling %i JavaScript files' % (run, len(scripts)))
    flash(jscanner, scripts, thread_count)

    for url in internal:
        if '=' in url:
            fuzzable.add(url)

    for match, intel_name, url in bad_intel:
        if isinstance(match, tuple):
            for x in match:  # Because "match" is a tuple
                if x != '':  # If the value isn't empty
                    if intel_name == "CREDIT_CARD":
                        if not luhn(match):
                            # garbage number
                            continue
                    intel.add("%s:%s" % (intel_name, x))
        else:
            if intel_name == "CREDIT_CARD":
                if not luhn(match):
                    # garbage number
                    continue
            intel.add("%s:%s:%s" % (url, intel_name, match))
        for url in external:
            try:
                if top_level(url, fix_protocol=True) in INTELS:
                    intel.add(url)
            except:
                pass

# Records the time at which crawling stopped
now = time.time()
# Finds total time taken
diff = (now - then)
minutes, seconds, time_per_request = timer(diff, processed)

# Step 4. Save the results
if not os.path.exists(output_dir): # if the directory doesn't exist
    os.mkdir(output_dir) # create a new directory

datasets = [files, intel, robots, custom, failed, internal, scripts,
            external, fuzzable, endpoints, keys]
dataset_names = ['files', 'intel', 'robots', 'custom', 'failed', 'internal',
                 'scripts', 'external', 'fuzzable', 'endpoints', 'keys']

writer(datasets, dataset_names, output_dir)
# Printing out results
print(('%s-%s' % (red, end)) * 50)
for dataset, dataset_name in zip(datasets, dataset_names):
    if dataset:
        print('%s %s: %s' % (good, dataset_name.capitalize(), len(dataset)))
print(('%s-%s' % (red, end)) * 50)

print('%s Total requests made: %i' % (info, len(processed)))
print('%s Total time taken: %i minutes %i seconds' % (info, minutes, seconds))
print('%s Requests per second: %i' % (info, int(len(processed) / diff)))

datasets = {
    'files': list(files), 'intel': list(intel), 'robots': list(robots),
    'custom': list(custom), 'failed': list(failed), 'internal': list(internal),
    'scripts': list(scripts), 'external': list(external),
    'fuzzable': list(fuzzable), 'endpoints': list(endpoints),
    'keys': list(keys)
}

if args.dns:
    print('%s Enumerating subdomains' % run)
    from plugins.find_subdomains import find_subdomains
    subdomains = find_subdomains(domain)
    print('%s %i subdomains found' % (info, len(subdomains)))
    writer([subdomains], ['subdomains'], output_dir)
    datasets['subdomains'] = subdomains
    from plugins.dnsdumpster import dnsdumpster
    print('%s Generating DNS map' % run)
    dnsdumpster(domain, output_dir)

if args.export:
    from plugins.exporter import exporter
    # exporter(directory, format, datasets)
    exporter(output_dir, args.export, datasets)

print('%s Results saved in %s%s%s directory' % (good, green, output_dir, end))

if args.std:
    for string in datasets[args.std]:
        sys.stdout.write(string + '\n')

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/summary_collector.py
========================================

#!/usr/bin/env python3

import os
import glob

# Define the path to search
search_path = "/home/jarvis/photon_results"
# Define the output file
output_file = "summary_collection.txt"

# Find all summary.txt files
summary_files = glob.glob(os.path.join(search_path, "**", "summary.txt"), recursive=True)

# Open the output file for writing
with open(output_file, "w") as outfile:
    for file_path in summary_files:
        outfile.write(f"## Contents from: {file_path}\n")
        
        # Read and write the contents of each summary file
        try:
            with open(file_path, "r") as infile:
                outfile.write(infile.read())
        except UnicodeDecodeError:
            outfile.write("Error: Could not read file (possibly binary data)\n")
        
        outfile.write("\n\n")

print(f"Summary collection has been created at {output_file}")

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/protonizer.py
========================================

#!/usr/bin/env python3
"""
WebSum - Photon Results Summarizer

This script:
1. Traverses through /home/jarvis/photon_results and its subfolders
2. Extracts URLs from text files in these folders
3. Fetches and extracts content from these URLs using BeautifulSoup
4. Summarizes content using various methods:
   - Built-in TextRank summarizer (no API required)
   - Ollama models (local)
   - ChatGPT (OpenAI API)
   - Hugging Face models
5. Processes URLs in batches, showing individual summaries and an overall batch summary
6. Asks user if they want to continue after each batch
7. Saves the summaries to a summary.txt file in each subfolder
"""

import os
import re
import json
import time
import argparse
import requests
import sys
import logging
import math
import heapq
import string
import nltk
from bs4 import BeautifulSoup
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import Counter

# Try to download nltk data if it's not already available
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    print("Downloading NLTK punkt tokenizer...")
    nltk.download('punkt', quiet=True)

__version__ = "1.1"

# Default configuration
PHOTON_ROOT = "/home/jarvis/photon_results"
BATCH_SIZE = 10
DEFAULT_OLLAMA_API_URL = "http://localhost:11434/api/generate"
DEFAULT_OPENAI_API_URL = "https://api.openai.com/v1/chat/completions"
DEFAULT_HF_API_URL = "https://api-inference.huggingface.co/models/"
DEFAULT_TIMEOUT = 90  # seconds
MAX_CONTENT_LENGTH = 8000  # characters

# Default API keys (can be overridden by environment variables or user input)
DEFAULT_OPENAI_API_KEY = ""
DEFAULT_HF_API_KEY = ""

# Regular expression for extracting URLs
URL_PATTERN = re.compile(r'https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+(?:/[^)\s]*)?')

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class TextRankSummarizer:
    """Implementation of TextRank algorithm for text summarization"""

    def __init__(self):
        self.stop_words = set([
            'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from',
            'has', 'he', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the',
            'to', 'was', 'were', 'will', 'with'
        ])

    def _clean_text(self, text):
        """Remove special characters and convert to lowercase"""
        text = re.sub(r'\s+', ' ', text)  # Replace multiple spaces with single space
        text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
        return text.lower()

    def _extract_sentences(self, text):
        """Extract sentences from text"""
        return nltk.sent_tokenize(text)

    def _compute_word_frequencies(self, text):
        """Compute word frequencies in the text"""
        word_freq = Counter()
        clean_text = self._clean_text(text)

        for word in clean_text.split():
            if word not in self.stop_words:
                word_freq[word] += 1

        # Normalize frequencies
        max_freq = max(word_freq.values()) if word_freq else 1
        for word in word_freq:
            word_freq[word] = word_freq[word] / max_freq

        return word_freq

    def _score_sentences(self, sentences, word_freq):
        """Score sentences based on word frequency"""
        sentence_scores = {}

        for i, sentence in enumerate(sentences):
            clean_sentence = self._clean_text(sentence)

            # Skip very short sentences (likely not meaningful content)
            if len(clean_sentence.split()) <= 3:
                continue

            for word in clean_sentence.split():
                if word in word_freq:
                    if i not in sentence_scores:
                        sentence_scores[i] = 0
                    sentence_scores[i] += word_freq[word]

        return sentence_scores

    def summarize(self, text, ratio=0.2):
        """Generate a summary using the TextRank algorithm"""
        if not text or len(text.strip()) == 0:
            return ""

        # Extract sentences
        sentences = self._extract_sentences(text)

        # Handle very short texts
        if len(sentences) <= 3:
            return text

        # Calculate word frequencies
        word_freq = self._compute_word_frequencies(text)

        # Score sentences
        sentence_scores = self._score_sentences(sentences, word_freq)

        # Calculate number of sentences for the summary
        num_sentences = max(1, math.ceil(len(sentences) * ratio))

        # Get top-scoring sentence indices
        top_sentence_indices = heapq.nlargest(num_sentences,
                                           sentence_scores,
                                           key=sentence_scores.get)

        # Sort indices to maintain original order
        top_sentence_indices = sorted(top_sentence_indices)

        # Construct summary
        summary = ' '.join([sentences[i] for i in top_sentence_indices])

        return summary

    def extract_keywords(self, text, ratio=0.1):
        """Extract keywords from the text"""
        word_freq = self._compute_word_frequencies(text)

        # Calculate number of keywords
        num_keywords = max(5, math.ceil(len(word_freq) * ratio))

        # Get top words
        keywords = heapq.nlargest(num_keywords, word_freq, key=word_freq.get)

        return ', '.join(keywords)

def setup_argparse():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Extract, fetch, and summarize URLs from Photon results",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument("--root-dir", type=str, default=PHOTON_ROOT,
                        help="Root directory containing Photon results")
    parser.add_argument("--batch-size", type=int, default=BATCH_SIZE,
                        help="Number of URLs to process in each batch")
    parser.add_argument("--timeout", type=int, default=DEFAULT_TIMEOUT,
                        help="Timeout for HTTP requests and API calls (seconds)")
    parser.add_argument("--ollama-url", type=str, default=DEFAULT_OLLAMA_API_URL,
                        help="Ollama API endpoint URL")
    parser.add_argument("--max-content", type=int, default=MAX_CONTENT_LENGTH,
                        help="Maximum content length to send for summarization")
    parser.add_argument("--verbose", action="store_true",
                        help="Enable verbose logging")
    parser.add_argument("--debug", action="store_true",
                        help="Enable debug mode with additional information")
    parser.add_argument("--auto", action="store_true",
                        help="Start in automatic mode (no prompts between batches)")

    return parser.parse_args()

def configure_logging(verbose, debug):
    """Configure logging level based on command line arguments."""
    if debug:
        logger.setLevel(logging.DEBUG)
    elif verbose:
        logger.setLevel(logging.INFO)
    else:
        logger.setLevel(logging.WARNING)

def print_header(text):
    """Print a formatted header."""
    try:
        width = min(os.get_terminal_size().columns, 80)
    except OSError:
        width = 80
    print(f"\n\033[1;36m{'=' * width}\n{text.center(width)}\n{'=' * width}\033[0m\n")

def extract_urls_from_file(file_path):
    """Extract URLs from a file using regex."""
    try:
        with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
            content = f.read()

        # Find all URLs in the content
        urls = URL_PATTERN.findall(content)

        # Remove duplicates while preserving order
        clean_urls = []
        seen = set()
        for url in urls:
            # Clean up the URL to remove trailing punctuation
            url = url.rstrip('.,;:\'\"!?)')

            # Skip duplicates
            if url not in seen:
                seen.add(url)
                clean_urls.append(url)

        return clean_urls
    except Exception as e:
        logger.error(f"Error reading {file_path}: {e}")
        return []

def get_domain(url):
    """Extract domain from URL."""
    parsed = urlparse(url)
    domain = parsed.netloc
    if domain.startswith('www.'):
        domain = domain[4:]
    return domain

def fetch_webpage_content(url, timeout=DEFAULT_TIMEOUT):
    """Fetch and extract text content from a webpage."""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }

        # Make the request but don't raise for status yet
        response = requests.get(url, timeout=timeout, headers=headers)

        # Check specifically for 404 status
        if response.status_code == 404:
            logger.warning(f"404 Not Found: {url}")
            return {
                "title": "404 Not Found",
                "text": "",
                "url": url,
                "status_code": 404,
                "error": True
            }

        # Check for other error status codes
        if response.status_code >= 400:
            logger.warning(f"HTTP Error {response.status_code}: {url}")
            return {
                "title": f"HTTP Error {response.status_code}",
                "text": "",
                "url": url,
                "status_code": response.status_code,
                "error": True
            }

        # If we got here, status code is good, raise for other errors
        response.raise_for_status()

        # Check content type
        content_type = response.headers.get('Content-Type', '').lower()
        if 'text/html' not in content_type and 'application/json' not in content_type:
            return {
                "title": f"Unsupported content type: {content_type}",
                "text": "",
                "url": url,
                "status_code": response.status_code,
                "error": True
            }

        soup = BeautifulSoup(response.content, "html.parser")

        # Extract title
        title_tag = soup.find('title')
        title = title_tag.get_text(strip=True) if title_tag else "No title found"

        # Remove script, style, nav, etc.
        for tag in soup(["script", "style", "nav", "footer", "header"]):
            tag.extract()

        # Extract text from paragraph tags first
        paragraphs = soup.find_all("p")
        if paragraphs:
            page_text = "\n\n".join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
        else:
            # If no paragraphs, try other common content containers
            content_containers = soup.select("article, .content, .main, #content, #main")
            if content_containers:
                page_text = "\n\n".join([el.get_text(strip=True) for el in content_containers])
            else:
                # Fallback to all text
                page_text = soup.get_text(separator="\n\n", strip=True)

        # Clean up text
        page_text = re.sub(r'\n{3,}', '\n\n', page_text)
        page_text = re.sub(r'\s{2,}', ' ', page_text)

        # Return content with success status
        return {
            "title": title,
            "text": page_text,
            "url": url,
            "status_code": response.status_code,
            "error": False
        }
    except Exception as e:
        logger.error(f"Error fetching {url}: {str(e)}")
        return {
            "title": f"Error: {str(e)[:100]}",
            "text": "",
            "url": url,
            "status_code": 0,  # Use 0 to indicate a connection/non-HTTP error
            "error": True
        }

def check_ollama_connection(url, timeout=5):
    """Check if Ollama server is running and reachable."""
    try:
        base_url = url
        if "/api/generate" in url:
            base_url = url.split("/api/generate")[0]

        response = requests.get(f"{base_url}/api/tags", timeout=timeout)
        response.raise_for_status()

        # Check if we got a valid response with models
        data = response.json()
        if "models" in data and len(data["models"]) > 0:
            logger.debug(f"Available Ollama models: {[m['name'] for m in data['models']]}")
            return True
        else:
            logger.warning("No models found in Ollama")
            return False
    except Exception as e:
        logger.error(f"Ollama connection failed: {e}")
        return False

def create_spinner():
    """Create a simple spinner to indicate progress during API calls."""
    import itertools
    import threading

    spinner_active = [True]
    spinner_thread = None

    def spin():
        for c in itertools.cycle('|/-\\'):
            if not spinner_active[0]:
                break
            sys.stdout.write(f"\r{c} ")
            sys.stdout.flush()
            time.sleep(0.1)
        sys.stdout.write('\r')
        sys.stdout.flush()

    spinner_thread = threading.Thread(target=spin)
    spinner_thread.daemon = True
    spinner_thread.start()

    def stop_spinner():
        spinner_active[0] = False
        if spinner_thread:
            spinner_thread.join(0.5)

    return stop_spinner

def summarize_with_textrank(content, depth, timeout=DEFAULT_TIMEOUT):
    """Summarize content using TextRank algorithm."""
    text = content["text"]
    if not text:
        return "No content to summarize."

    print("Generating summary with TextRank...", end="", flush=True)
    stop_spinner = create_spinner()

    try:
        # Create TextRank summarizer instance
        summarizer = TextRankSummarizer()

        # Adjust ratio based on depth parameter
        if depth == "short":
            ratio = 0.05  # Very short summary
        elif depth == "medium":
            ratio = 0.1   # Medium summary
        else:  # detailed
            ratio = 0.3   # Detailed summary

        # Apply summarization
        summary = summarizer.summarize(text, ratio=ratio)

        # If summary is empty (happens with short texts), return the original text
        if not summary:
            if len(text) > 200:
                summary = text[:200] + "..."
            else:
                summary = text

        # Add keywords if the depth is medium or detailed
        if depth in ["medium", "detailed"]:
            try:
                key_terms = summarizer.extract_keywords(text, ratio=0.01)
                if key_terms:
                    summary += f"\n\nKey terms: {key_terms}"
            except Exception as e:
                logger.warning(f"Error extracting keywords: {e}")

        stop_spinner()
        print(" Done!")
        return summary
    except Exception as e:
        stop_spinner()
        print(f" Error: {str(e)[:100]}")
        logger.error(f"TextRank summarization error: {e}")
        return f"Error generating summary: {str(e)[:100]}"

def summarize_batch_with_textrank(summaries):
    """Generate an overall summary of a batch using TextRank summarization."""
    # Combine all the individual summaries into one text
    combined_text = "\n\n".join([f"Title: {s['title']}\nSummary: {s['summary']}"
                                for s in summaries
                                if s["summary"] and not s["summary"].startswith("Error")])

    if not combined_text:
        return "No valid summaries to create an overall summary."

    try:
        # Use the TextRank algorithm to extract the key points
        summarizer = TextRankSummarizer()
        overall_summary = summarizer.summarize(combined_text, ratio=0.3)

        # If summary is empty, use a default message
        if not overall_summary:
            overall_summary = "The batch contains varied content without clear unifying themes."

        # Add keywords to provide additional context
        try:
            key_terms = summarizer.extract_keywords(combined_text, ratio=0.02)
            if key_terms:
                overall_summary += f"\n\nCommon themes across this batch: {key_terms}"
        except Exception:
            pass  # Ignore keyword errors in batch summary

        return overall_summary
    except Exception as e:
        logger.error(f"Error generating batch summary with TextRank: {e}")
        return "Could not generate an overall batch summary."

def summarize_with_ollama(content, model, depth, api_url=DEFAULT_OLLAMA_API_URL, timeout=DEFAULT_TIMEOUT):
    """Summarize content using Ollama API."""
    # Truncate content if too long
    text = content["text"]
    if len(text) > MAX_CONTENT_LENGTH:
        text = text[:MAX_CONTENT_LENGTH] + "... [truncated]"

    # Build prompt based on depth
    if depth == "short":
        prompt = f"Summarize the following content in 1-2 concise sentences. Provide any additional context or knowledge you have that's relevant to the topic:\n\n{text}"
    elif depth == "medium":
        prompt = f"Summarize the following content in one paragraph (3-5 sentences). Include important details and provide any additional context or knowledge you have that's relevant to the topic:\n\n{text}"
    else:  # detailed
        prompt = f"Provide a detailed summary of the following content, including important details, relevant facts, and any additional context or knowledge you have that's relevant to the topic:\n\n{text}"

    print("Generating summary with Ollama...", end="", flush=True)
    stop_spinner = create_spinner()

    try:
        response = requests.post(
            api_url,
            json={"model": model, "prompt": prompt, "stream": False},
            timeout=timeout
        )
        response.raise_for_status()
        result = response.json()
        summary = result.get("response", "No summary generated.")

        stop_spinner()
        print(" Done!")
        return summary.strip()
    except Exception as e:
        stop_spinner()
        print(f" Error: {str(e)[:100]}")
        logger.error(f"Ollama API error: {e}")
        return f"Error generating summary: {str(e)[:100]}"

def summarize_with_openai(content, api_key, depth, timeout=DEFAULT_TIMEOUT):
    """Summarize content using OpenAI API."""
    text = content["text"]
    if len(text) > MAX_CONTENT_LENGTH:
        text = text[:MAX_CONTENT_LENGTH] + "... [truncated]"

    if depth == "short":
        system_prompt = "You are a skilled summarizer. Create a 1-2 sentence summary capturing the key points. Include any additional context or knowledge you have that's relevant to the topic."
    elif depth == "medium":
        system_prompt = "You are a skilled summarizer. Create a single paragraph summary (3-5 sentences) capturing the main ideas. Include any additional context or knowledge you have that's relevant to the topic."
    else:  # detailed
        system_prompt = "You are a skilled summarizer. Create a detailed summary capturing important details and relevant facts. Include any additional context or knowledge you have that's relevant to the topic."

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }

    data = {
        "model": "gpt-3.5-turbo",
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": text}
        ]
    }

    print("Generating summary with ChatGPT...", end="", flush=True)
    stop_spinner = create_spinner()

    try:
        response = requests.post(
            DEFAULT_OPENAI_API_URL,
            headers=headers,
            json=data,
            timeout=timeout
        )
        response.raise_for_status()
        result = response.json()
        summary = result["choices"][0]["message"]["content"]

        stop_spinner()
        print(" Done!")
        return summary.strip()
    except Exception as e:
        stop_spinner()
        print(f" Error: {str(e)[:100]}")
        logger.error(f"OpenAI API error: {e}")
        return f"Error generating summary: {str(e)[:100]}"

def summarize_with_huggingface(content, api_key, model=None, depth="medium", timeout=DEFAULT_TIMEOUT):
    """Summarize content using Hugging Face API."""
    if not model:
        model = "facebook/bart-large-cnn"

    text = content["text"]
    if len(text) > MAX_CONTENT_LENGTH:
        text = text[:MAX_CONTENT_LENGTH] + "... [truncated]"

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    # Adjust parameters based on depth
    parameters = {}
    if depth == "short":
        parameters = {"max_length": 50, "min_length": 20}
    elif depth == "medium":
        parameters = {"max_length": 100, "min_length": 50}
    else:  # detailed
        parameters = {"max_length": 200, "min_length": 100}

    payload = {
        "inputs": text,
        "parameters": parameters
    }

    print(f"Generating summary with Hugging Face ({model})...", end="", flush=True)
    stop_spinner = create_spinner()

    try:
        response = requests.post(
            f"{DEFAULT_HF_API_URL}{model}",
            headers=headers,
            json=payload,
            timeout=timeout
        )
        response.raise_for_status()
        result = response.json()

        stop_spinner()
        print(" Done!")

        if isinstance(result, list) and len(result) > 0:
            return result[0].get("summary_text", "No summary generated.").strip()
        else:
            return "No summary generated."
    except Exception as e:
        stop_spinner()
        print(f" Error: {str(e)[:100]}")
        logger.error(f"Hugging Face API error: {e}")
        return f"Error generating summary: {str(e)[:100]}"

def summarize_content(content, provider, model, depth, api_keys, api_urls, timeout=DEFAULT_TIMEOUT):
    """Summarize content using the selected provider."""
    if not content["text"]:
        return f"No content available for {content['url']}"

    if provider == "textrank":
        return summarize_with_textrank(content, depth, timeout)
    elif provider == "ollama":
        return summarize_with_ollama(content, model, depth, api_urls["ollama"], timeout)
    elif provider == "openai":
        if not api_keys["openai"]:
            return "Error: OpenAI API key not provided."
        return summarize_with_openai(content, api_keys["openai"], depth, timeout)
    elif provider == "huggingface":
        if not api_keys["huggingface"]:
            return "Error: Hugging Face API key not provided."
        return summarize_with_huggingface(content, api_keys["huggingface"], model, depth, timeout)
    else:
        return f"Error: Unknown provider '{provider}'."

def process_batch(urls, provider, model, depth, api_keys, api_urls, timeout=DEFAULT_TIMEOUT, automatic_mode=False):
    """Process a batch of URLs: fetch, summarize, and display results."""
    valid_contents = []
    invalid_urls = []
    summaries = []
    all_404 = True  # Flag to track if all URLs result in 404 errors

    print(f"\nProcessing {len(urls)} URLs...")

    # Fetch content from URLs
    contents = []
    with ThreadPoolExecutor(max_workers=min(len(urls), 5)) as executor:
        future_to_url = {executor.submit(fetch_webpage_content, url, timeout): url for url in urls}
        for i, future in enumerate(as_completed(future_to_url), 1):
            url = future_to_url[future]
            try:
                content = future.result()
                # Print appropriate message based on content status
                if content["error"]:
                    if content["status_code"] == 404:
                        # In automatic mode, don't show 404 messages
                        if not automatic_mode:
                            print(f"[{i}/{len(urls)}] Skipping 404 Not Found: {url}")
                        invalid_urls.append(url)
                    else:
                        print(f"[{i}/{len(urls)}] Error: {content['title']} for {url}")
                        contents.append(content)
                        all_404 = False  # Found an error that's not a 404
                else:
                    print(f"[{i}/{len(urls)}] Fetched: {url} ({len(content['text'])} chars)")
                    contents.append(content)
                    valid_contents.append(content)
                    all_404 = False  # Found at least one valid URL
            except Exception as e:
                logger.error(f"Error processing {url}: {e}")
                print(f"[{i}/{len(urls)}] Error fetching {url}: {str(e)[:100]}")
                contents.append({
                    "title": f"Error fetching {url}",
                    "text": "",
                    "url": url,
                    "status_code": 0,
                    "error": True
                })

    # Log the number of URLs being skipped due to 404
    if invalid_urls:
        logger.info(f"Skipping {len(invalid_urls)} URLs due to 404 errors")
        if not automatic_mode:  # Only show in non-automatic mode
            print(f"\n\033[1;33mSkipping {len(invalid_urls)} URLs due to 404 errors\033[0m")

    # Summarize content (excluding 404 pages)
    for i, content in enumerate([c for c in contents if not (c.get("status_code") == 404)], 1):
        if not content["text"]:
            print(f"\n\033[1;33m[{i}/{len(contents) - len(invalid_urls)}] Error: Could not extract content from {content['url']}\033[0m")
            summaries.append({
                "title": content["title"],
                "url": content["url"],
                "summary": "No content could be extracted from this URL."
            })
            continue

        print(f"\n\033[1;33m[{i}/{len(contents) - len(invalid_urls)}] Summarizing: {content['title']}\033[0m")
        summary = summarize_content(content, provider, model, depth, api_keys, api_urls, timeout)

        domain = get_domain(content["url"])
        print(f"\n\033[1;32m[{i}/{len(contents) - len(invalid_urls)}] Summary for {content['title']}\033[0m")
        print(f"\033[0;36m{content['url']}\033[0m \033[0;90m({domain})\033[0m")
        print(f"{summary}")
        print("-" * 60)

        summaries.append({
            "title": content["title"],
            "url": content["url"],
            "summary": summary
        })

    # Generate overall summary for the batch (only for valid contents)
    if valid_contents and summaries and any(s["summary"] and not s["summary"].startswith("Error") for s in summaries):
        print("\nGenerating overall summary for this batch...")

        # Use TextRank's specialized batch summarization if provider is textrank
        if provider == "textrank":
            overall_summary = summarize_batch_with_textrank(summaries)
        else:
            # Create a prompt with all individual summaries for other providers
            overall_prompt = {
                "title": f"Overall summary for batch of {len(summaries)} websites",
                "text": "Based on the following summaries, provide an overall summary that highlights common themes and important points:\n\n" +
                       "\n\n".join([f"Title: {s['title']}\nSummary: {s['summary']}" for s in summaries if s["summary"] and not s["summary"].startswith("Error")])
            }

            overall_summary = summarize_content(overall_prompt, provider, model, depth, api_keys, api_urls, timeout)

        print("\n\033[1;32mOVERALL BATCH SUMMARY:\033[0m")
        print(overall_summary)
        print("=" * 60)

        return summaries, overall_summary, all_404

    return summaries, "No valid summaries generated for this batch.", all_404

def collect_urls_from_folders(root_dir):
    """Collect URLs from all subfolders in the root directory."""
    folder_urls = {}

    if not os.path.isdir(root_dir):
        logger.error(f"Root directory {root_dir} does not exist.")
        return folder_urls

    # List all subfolders
    subfolders = [f for f in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, f))]
    if not subfolders:
        logger.warning(f"No subfolders found in {root_dir}.")
        return folder_urls

    # Process each subfolder
    for subfolder in subfolders:
        subfolder_path = os.path.join(root_dir, subfolder)
        urls = []

        # Get all text files in the subfolder
        text_files = [f for f in os.listdir(subfolder_path)
                      if f.endswith('.txt') and f != 'summary.txt' and os.path.isfile(os.path.join(subfolder_path, f))]

        # Extract URLs from each text file
        for text_file in text_files:
            file_path = os.path.join(subfolder_path, text_file)
            file_urls = extract_urls_from_file(file_path)
            urls.extend(file_urls)

        # Remove duplicates while preserving order
        unique_urls = []
        seen = set()
        for url in urls:
            if url not in seen:
                seen.add(url)
                unique_urls.append(url)

        if unique_urls:
            folder_urls[subfolder] = unique_urls

    return folder_urls

def choose_provider():
    """Let user choose the summarization provider."""
    print("\nChoose a summarization provider:")
    print("1) TextRank (built-in, no API required)")
    print("2) Ollama (local)")
    print("3) OpenAI (ChatGPT)")
    print("4) Hugging Face")

    while True:
        choice = input("Enter your choice (1-4): ").strip()
        if choice == "1":
            return "textrank"
        elif choice == "2":
            return "ollama"
        elif choice == "3":
            return "openai"
        elif choice == "4":
            return "huggingface"
        else:
            print("Invalid choice. Please enter 1, 2, 3, or 4.")

def choose_model(provider):
    """Let user choose the model for the selected provider."""
    if provider == "textrank":
        # TextRank has no model options in this implementation
        return "textrank"

    elif provider == "ollama":
        print("\nChoose an Ollama model:")
        print("1) gemma3:12b")
        print("2) llama3:latest")
        print("3) deepseek-r1:8b")
        print("4) Other (enter name)")

        choice = input("Enter your choice (1-4): ").strip()
        if choice == "1":
            return "gemma3:12b"
        elif choice == "2":
            return "llama3:latest"
        elif choice == "3":
            return "deepseek-r1:8b"
        elif choice == "4":
            return input("Enter the model name: ").strip()
        else:
            print("Invalid choice. Using gemma3:12b.")
            return "gemma3:12b"

    elif provider == "openai":
        print("\nChoose an OpenAI model:")
        print("1) gpt-3.5-turbo")
        print("2) gpt-4 (if available)")

        choice = input("Enter your choice (1-2): ").strip()
        if choice == "1":
            return "gpt-3.5-turbo"
        elif choice == "2":
            return "gpt-4"
        else:
            print("Invalid choice. Using gpt-3.5-turbo.")
            return "gpt-3.5-turbo"

    elif provider == "huggingface":
        print("\nChoose a Hugging Face model:")
        print("1) facebook/bart-large-cnn (summarization)")
        print("2) sshleifer/distilbart-cnn-12-6 (faster summarization)")
        print("3) Other (enter name)")

        choice = input("Enter your choice (1-3): ").strip()
        if choice == "1":
            return "facebook/bart-large-cnn"
        elif choice == "2":
            return "sshleifer/distilbart-cnn-12-6"
        elif choice == "3":
            return input("Enter the model name: ").strip()
        else:
            print("Invalid choice. Using facebook/bart-large-cnn.")
            return "facebook/bart-large-cnn"

    return None

def choose_depth():
    """Let user choose the summarization depth."""
    print("\nChoose summarization depth:")
    print("1) Short (1-2 sentences)")
    print("2) Medium (one paragraph)")
    print("3) Detailed (comprehensive)")

    while True:
        choice = input("Enter your choice (1-3): ").strip()
        if choice == "1":
            return "short"
        elif choice == "2":
            return "medium"
        elif choice == "3":
            return "detailed"
        else:
            print("Invalid choice. Please enter 1, 2, or 3.")

def get_api_keys(provider):
    """Get API keys for the selected provider."""
    api_keys = {
        "openai": DEFAULT_OPENAI_API_KEY,
        "huggingface": DEFAULT_HF_API_KEY
    }

    # No API keys needed for textrank
    if provider == "textrank":
        return api_keys

    if provider == "openai":
        key_input = input(f"\nEnter your OpenAI API key (or press Enter to use default/env var): ").strip()
        if key_input:
            api_keys["openai"] = key_input
        elif os.environ.get("OPENAI_API_KEY"):
            api_keys["openai"] = os.environ.get("OPENAI_API_KEY")

    elif provider == "huggingface":
        key_input = input(f"\nEnter your Hugging Face API key (or press Enter to use default/env var): ").strip()
        if key_input:
            api_keys["huggingface"] = key_input
        elif os.environ.get("HF_API_KEY"):
            api_keys["huggingface"] = os.environ.get("HF_API_KEY")

    return api_keys

def save_summary_to_file(folder_path, batch_summaries, batch_overall_summaries):
    """Save summaries to a file in the folder."""
    summary_path = os.path.join(folder_path, "summary.txt")

    try:
        with open(summary_path, "w", encoding="utf-8") as f:
            f.write("# WEBSITE SUMMARIES\n\n")

            for batch_idx, (summaries, overall) in enumerate(zip(batch_summaries, batch_overall_summaries), 1):
                f.write(f"## Batch {batch_idx}\n\n")

                for i, summary in enumerate(summaries, 1):
                    f.write(f"### {i}. {summary['title']}\n")
                    f.write(f"URL: {summary['url']}\n\n")
                    f.write(f"{summary['summary']}\n\n")

                f.write("### Overall Batch Summary\n\n")
                f.write(f"{overall}\n\n")
                f.write("-" * 40 + "\n\n")

            f.write("# COMPLETE FOLDER SUMMARY\n\n")

            # Create a summary of all batch overall summaries
            if batch_overall_summaries:
                f.write("Based on all batches processed, the websites in this folder cover:\n\n")
                for batch_idx, overall in enumerate(batch_overall_summaries, 1):
                    f.write(f"Batch {batch_idx}: {overall}\n\n")

        print(f"\nSummary saved to: {summary_path}")
        return True
    except Exception as e:
        logger.error(f"Error saving summary to {summary_path}: {e}")
        print(f"Error saving summary: {str(e)}")
        return False

def check_for_key_press():
    """Check if a key has been pressed without blocking execution."""
    try:
        # Check if there's any input ready (non-blocking)
        import select
        ready, _, _ = select.select([sys.stdin], [], [], 0)
        if ready:
            # There's input ready, read it
            key = sys.stdin.readline().strip().lower()
            return key
        return None
    except Exception:
        return None

def main():
    """Main function."""
    args = setup_argparse()
    configure_logging(args.verbose, args.debug)

    print_header("WEBSUMMARIZER - PHOTON RESULTS SUMMARIZER")
    print(f"Version: {__version__}")

    # Choose summarization provider, model, and depth
    provider = choose_provider()
    model = choose_model(provider)
    depth = choose_depth()
    api_keys = get_api_keys(provider)

    api_urls = {
        "ollama": args.ollama_url,
        "openai": DEFAULT_OPENAI_API_URL,
        "huggingface": DEFAULT_HF_API_URL
    }

    # Check Ollama connection if needed
    if provider == "ollama" and not check_ollama_connection(args.ollama_url):
        print("\nError: Cannot connect to Ollama. Make sure Ollama is running.")
        print("You can start Ollama with: ollama serve")
        print(f"And pull the model with: ollama pull {model}")
        return 1

    # Collect URLs from folders
    print(f"\nCollecting URLs from subfolders in {args.root_dir}...")
    folder_urls = collect_urls_from_folders(args.root_dir)

    if not folder_urls:
        print("No URLs found in any subfolder.")
        return 1

    print(f"\nFound {len(folder_urls)} subfolders with URLs:")
    for folder, urls in folder_urls.items():
        print(f"- {folder}: {len(urls)} URLs")

    # Track automatic mode state
    automatic_mode = args.auto
    if automatic_mode:
        print("\nAutomatic mode enabled. Processing will continue until complete or interrupted.")
        print(f"Using batch size of 20 for automatic mode (overrides default of {args.batch_size}).")
        print("Press 'n' then Enter at any time to exit automatic mode.")
        print("Or press Ctrl+C at any time to stop processing and save results.")

    # Process each subfolder
    for folder, urls in folder_urls.items():
        folder_path = os.path.join(args.root_dir, folder)
        print_header(f"Processing folder: {folder}")
        print(f"Found {len(urls)} URLs to process")

        # Process URLs in batches
        batch_summaries = []
        batch_overall_summaries = []

        i = 0
        while i < len(urls):
            # Use batch size of 20 when in automatic mode, otherwise use the specified batch size
            current_batch_size = 20 if automatic_mode else args.batch_size
            batch = urls[i:i + current_batch_size]

            batch_number = i // current_batch_size + 1
            print_header(f"Processing batch {batch_number} ({len(batch)} URLs)")

            summaries, overall_summary, all_404 = process_batch(
                batch, provider, model, depth, api_keys, api_urls, args.timeout, automatic_mode
            )

            batch_summaries.append(summaries)
            batch_overall_summaries.append(overall_summary)

            # Move to next batch
            i += current_batch_size

            # Check if we should continue to the next batch
            if i < len(urls):
                if all_404:
                    # Automatically continue to next batch if all URLs were 404s
                    if not automatic_mode:  # Only show in non-automatic mode
                        print("\nAll URLs in this batch resulted in 404 errors. Automatically continuing to next batch...")
                    continue
                elif automatic_mode:
                    # In fully automatic mode, just continue without prompting
                    # Just print a brief status message
                    print("\nAutomatic mode active - continuing to next batch automatically.")
                    print("(Press 'n' then Enter to exit automatic mode)")
                    # Brief pause to allow for reading messages
                    time.sleep(1.0)

                    # Non-blocking check if 'n' was pressed to exit automatic mode
                    key = check_for_key_press()
                    if key == 'n':
                        automatic_mode = False
                        print("Automatic mode disabled. Will prompt before each batch.")
                        # FIX: Make sure to clearly ask about continuing and enforce input validation
                        while True:
                            choice = input("Continue with next batch? (y/n): ").strip().lower()
                            if choice == 'y':
                                break
                            elif choice == 'n':
                                print("Stopping at user request.")
                                i = len(urls)  # This will exit the outer while loop
                                break
                            else:
                                print("Please enter 'y' or 'n'.")
                else:
                    # FIX: Implement better input validation to ensure only valid inputs are accepted
                    valid_response = False
                    while not valid_response:
                        choice = input("\nContinue with next batch? (y/n/a for automatic mode): ").strip().lower()
                        if choice == 'a':
                            automatic_mode = True
                            print("Automatic mode enabled. Processing will continue until complete or interrupted.")
                            print("Using batch size of 20 for remaining batches.")
                            print("Press 'n' then Enter at any time to exit automatic mode.")
                            valid_response = True
                        elif choice == 'y':
                            valid_response = True
                        elif choice == 'n':
                            print("Stopping at user request.")
                            i = len(urls)  # This will exit the outer while loop
                            valid_response = True
                        else:
                            print("Invalid input. Please enter 'y', 'n', or 'a'.")

        # Save summaries to file
        save_summary_to_file(folder_path, batch_summaries, batch_overall_summaries)

    print_header("SUMMARIZATION COMPLETE")
    return 0

if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        print("\n\nOperation interrupted by user.")
        print("Saving any completed summaries...")
        # Note: The summaries for the completed batches are already saved by the main loop
        print("Summary files have been saved for completed batches.")
        sys.exit(1)

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/wetails.py
========================================

import requests
from bs4 import BeautifulSoup

def extract_website_info(url):
    try:
        response = requests.get(url)
        response.raise_for_status()  # Raises an HTTPError if the HTTP request returned an unsuccessful status code
    except Exception as e:
        print(f"Error fetching the URL: {e}")
        return None

    # Parse the HTML content with BeautifulSoup
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # Extract the title of the page
    title = soup.title.string if soup.title else "No title found"

    # Extract the meta description (if available)
    meta_description = soup.find("meta", attrs={"name": "description"})
    description = meta_description['content'] if meta_description and meta_description.has_attr('content') else "No description available"

    # Extract text from all paragraph elements
    paragraphs = [p.get_text(strip=True) for p in soup.find_all("p")]
    
    return {
        "title": title,
        "description": description,
        "paragraphs": paragraphs,
    }

def main():
    url = input("Enter a website URL: ")
    info = extract_website_info(url)
    if info:
        print("\nWebsite Information:")
        print("Title:", info["title"])
        print("Meta Description:", info["description"])
        print("\nExtracted Paragraphs:\n")
        for p in info["paragraphs"]:
            print(p)
            print("-" * 80)

if __name__ == "__main__":
    main()

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/runner.py
========================================

#!/usr/bin/env python3
"""
OSINT Suite Runner

This script automates running the entire OSINT workflow:
1. Running proton.py to search and crawl websites
2. Running protonizer.py to analyze and summarize the results
3. Starting the app.py web server
4. Opening the frontend in a web browser

Usage:
    python3 osint_runner.py "search keywords" "example.com" 50
"""

import argparse
import subprocess
import time
import webbrowser
import os
import sys
import signal
from pathlib import Path

def parse_arguments():
    parser = argparse.ArgumentParser(description="Run the complete OSINT workflow")
    parser.add_argument("keyword", help="Search keyword for proton.py")
    parser.add_argument("site", nargs='?', default=None, help="Site to search/crawl (optional)")
    parser.add_argument("depth", type=int, help="Depth value (1-300)")
    parser.add_argument("--skip-proton", action="store_true", 
                        help="Skip running proton.py (use existing results)")
    parser.add_argument("--skip-protonizer", action="store_true", 
                        help="Skip running protonizer.py")
    parser.add_argument("--port", type=int, default=5000, 
                        help="Port for the web server (default: 5000)")
    
    # Handle the case where user provides only keyword and depth
    args = parser.parse_args()
    
    # If user provided just two arguments, the second is actually the depth
    if len(sys.argv) == 3 and args.site and args.site.isdigit() and args.depth is None:
        args.depth = int(args.site)
        args.site = None
    
    return args

def run_command(cmd, description):
    """Run a command and handle any errors."""
    print(f"\n[+] {description}...")
    try:
        print(f"[*] Running: {' '.join(cmd)}")
        subprocess.run(cmd, check=True)
        print(f"[+] Command completed successfully")
        return True
    except subprocess.CalledProcessError as e:
        print(f"[!] Error: {e}")
        return False
    except Exception as e:
        print(f"[!] Unexpected error: {e}")
        return False

def start_web_server(port=5000):
    """Start the Flask web server in the background."""
    print(f"\n[+] Starting web server on port {port}...")
    try:
        # Define the correct static directory path
        static_dir = Path("/home/jarvis/Proton/static")
        
        # Make sure the static directory exists
        static_dir.mkdir(exist_ok=True, parents=True)
        print(f"[*] Ensuring static directory exists: {static_dir}")
        
        # Copy frontend.html to the static directory if needed
        frontend_path = Path("frontend.html")
        static_index_path = static_dir / "index.html"
        
        if frontend_path.exists():
            print(f"[*] Copying frontend.html to {static_index_path}")
            with open(frontend_path, "r") as src, open(static_index_path, "w") as dst:
                dst.write(src.read())
        else:
            print(f"[!] Warning: frontend.html not found in current directory")
            # Check if it's already in the destination
            if not static_index_path.exists():
                print(f"[!] Warning: {static_index_path} doesn't exist either!")
                print(f"[*] The web interface may not function correctly")
        
        # Start the server as a background process
        cmd = ["python3", "app.py"]
        server_process = subprocess.Popen(cmd, 
                                         stdout=subprocess.PIPE,
                                         stderr=subprocess.PIPE)
        
        # Give the server a moment to start up
        time.sleep(2)
        
        # Check if the process is still running
        if server_process.poll() is None:
            print(f"[+] Web server started successfully (PID: {server_process.pid})")
            return server_process
        else:
            stdout, stderr = server_process.communicate()
            print(f"[!] Web server failed to start: {stderr.decode()}")
            return None
    except Exception as e:
        print(f"[!] Error starting web server: {e}")
        return None

def open_web_browser(port=5000):
    """Open the web browser to view the interface."""
    url = f"http://localhost:{port}"
    print(f"\n[+] Opening web browser to {url}...")
    try:
        webbrowser.open(url)
        print(f"[+] Browser opened successfully")
        return True
    except Exception as e:
        print(f"[!] Failed to open browser: {e}")
        print(f"[+] Please manually navigate to {url}")
        return False

def main():
    args = parse_arguments()
    
    # Run proton.py if not skipped
    if not args.skip_proton:
        # Build the command based on whether a site was provided
        if args.site:
            cmd = ["python3", "proton.py", args.keyword, args.site, str(args.depth)]
            description = f"Running proton.py for '{args.keyword}' on site '{args.site}' with depth {args.depth}"
        else:
            cmd = ["python3", "proton.py", args.keyword, str(args.depth)]
            description = f"Running proton.py for '{args.keyword}' with depth {args.depth}"
        
        success = run_command(cmd, description)
        if not success:
            choice = input("[!] proton.py encountered errors. Continue anyway? (y/n): ").lower()
            if choice != 'y':
                return
    else:
        print("\n[*] Skipping proton.py as requested")
    
    # Run protonizer.py if not skipped
    if not args.skip_protonizer:
        success = run_command(
            ["python3", "protonizer.py"],
            "Running protonizer.py to analyze and summarize the results"
        )
        if not success:
            choice = input("[!] protonizer.py encountered errors. Continue anyway? (y/n): ").lower()
            if choice != 'y':
                return
    else:
        print("\n[*] Skipping protonizer.py as requested")
    
    # Start the web server
    server_process = start_web_server(args.port)
    if not server_process:
        print("[!] Failed to start web server. Exiting.")
        return
    
    # Open the web browser
    open_web_browser(args.port)
    
    print("\n[+] OSINT suite is now running")
    print(f"[+] The web interface is available at http://localhost:{args.port}")
    print("[+] Press Ctrl+C to stop the server and exit")
    
    try:
        # Keep the script running until the user interrupts
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("\n[+] Stopping web server...")
        os.kill(server_process.pid, signal.SIGTERM)
        time.sleep(1)
        print("[+] Done. Goodbye!")

if __name__ == "__main__":
    main()

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/sumsearch.py
========================================

#!/usr/bin/env python3

import os
import re
import tkinter as tk
from tkinter import ttk, messagebox
import webbrowser
import threading
from functools import partial

# Change this path to the directory where you want to look for summary.txt files
BASE_DIRECTORY = "/home/jarvis/photon_results"

# Regular expression pattern to match URLs
URL_PATTERN = r'https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+'

def gather_summary_data(base_path):
    """
    Recursively walk through 'base_path' and gather text from all 'summary.txt' files.
    Returns a list of tuples (folder_path, file_contents, file_path).
    """
    all_summaries = []

    for root, dirs, files in os.walk(base_path):
        for file in files:
            if file.lower() == "summary.txt":
                filepath = os.path.join(root, file)
                try:
                    with open(filepath, "r", encoding="utf-8") as f:
                        content = f.read()
                    relative_folder = os.path.relpath(root, base_path)
                    all_summaries.append((relative_folder, content, filepath))
                except Exception as e:
                    print(f"Error reading {filepath}: {e}")

    return all_summaries

def open_file_location(filepath):
    """Open the file's directory in the system file explorer"""
    directory = os.path.dirname(filepath)
    
    # Platform-specific commands to open the file explorer
    if os.name == 'nt':  # Windows
        os.startfile(directory)
    elif os.name == 'posix':  # Linux, macOS
        try:
            # Try using xdg-open for Linux
            os.system(f'xdg-open "{directory}"')
        except:
            try:
                # Try using open for macOS
                os.system(f'open "{directory}"')
            except:
                messagebox.showinfo("Information", f"Path: {directory}")

class SummaryApp:
    def __init__(self, root):
        self.root = root
        self.root.title("Hacker Information Board")
        self.root.geometry("1100x750")
        
        # Define color scheme - hacker theme
        self.colors = {
            'bg_dark': "#0D0D0D",
            'bg_light': "#1A1A1A",
            'accent': "#00FF41",      # Matrix green
            'text_light': "#00FF41",  # Matrix green
            'text_dark': "#0D0D0D",
            'highlight': "#FFD700",   # Gold for highlights
            'error': "#FF3131"        # Red for errors
        }
        
        # Configure styles
        self.configure_styles()
        
        # Create UI elements
        self.create_ui()
        
        # Initialize variables
        self.summaries_data = {}
        self.search_results = []
        self.current_match_index = tk.IntVar(value=-1)
        
        # Load data
        self.load_summaries()
    
    def configure_styles(self):
        """Configure the ttk styles for the application"""
        style = ttk.Style()
        style.theme_use("clam")
        
        # Main frames
        style.configure(
            "Custom.TFrame", 
            background=self.colors['bg_dark']
        )
        style.configure(
            "Search.TFrame", 
            background=self.colors['bg_light']
        )
        
        # Labels
        style.configure(
            "Custom.TLabel", 
            background=self.colors['bg_dark'], 
            foreground=self.colors['text_light'], 
            font=("Courier New", 14, "bold")
        )
        style.configure(
            "Search.TLabel", 
            background=self.colors['bg_light'], 
            foreground=self.colors['text_light'], 
            font=("Courier New", 10)
        )
        
        # Buttons
        style.configure(
            "Custom.TButton",
            background=self.colors['accent'],
            foreground=self.colors['text_dark'],
            padding=6,
            font=("Courier New", 10, "bold")
        )
        style.map(
            "Custom.TButton",
            background=[('active', self.colors['highlight'])]
        )
        
        # Scrollbars
        style.configure(
            "Custom.Vertical.TScrollbar",
            troughcolor=self.colors['bg_light'],
            gripcount=0,
            background=self.colors['accent'],
            bordercolor=self.colors['bg_dark'],
            arrowcolor=self.colors['text_dark']
        )
        style.configure(
            "Custom.Horizontal.TScrollbar",
            troughcolor=self.colors['bg_light'],
            gripcount=0,
            background=self.colors['accent'],
            bordercolor=self.colors['bg_dark'],
            arrowcolor=self.colors['text_dark']
        )
        
        # Entry fields
        style.configure(
            "Custom.TEntry",
            fieldbackground=self.colors['bg_light'],
            foreground=self.colors['text_light'],
            bordercolor=self.colors['accent'],
            lightcolor=self.colors['accent'],
            darkcolor=self.colors['accent'],
        )
        
        # Checkbutton
        style.configure(
            "Search.TCheckbutton", 
            background=self.colors['bg_light'], 
            foreground=self.colors['text_light']
        )
        
        # Treeview
        style.configure(
            "Treeview", 
            background=self.colors['bg_light'], 
            foreground=self.colors['text_light'], 
            fieldbackground=self.colors['bg_light'],
            bordercolor=self.colors['bg_dark']
        )
        style.configure(
            "Treeview.Heading", 
            background=self.colors['bg_dark'], 
            foreground=self.colors['text_light'], 
            bordercolor=self.colors['bg_dark'],
            font=("Courier New", 10, "bold")
        )
        style.map(
            "Treeview", 
            background=[('selected', self.colors['accent'])],
            foreground=[('selected', self.colors['text_dark'])]
        )
    
    def create_ui(self):
        """Create the user interface"""
        # Main frame
        self.main_frame = ttk.Frame(self.root, style="Custom.TFrame")
        self.main_frame.pack(fill=tk.BOTH, expand=True)
        
        # Header
        self.create_header()
        
        # Search bar
        self.create_search_bar()
        
        # Split pane for list and content
        self.create_content_panes()
        
        # Status bar
        self.create_status_bar()
        
        # Bind keyboard shortcuts
        self.bind_shortcuts()
    
    def create_header(self):
        """Create the header section"""
        header_frame = ttk.Frame(self.main_frame, style="Custom.TFrame")
        header_frame.pack(fill=tk.X, padx=10, pady=10)
        
        title_label = ttk.Label(
            header_frame, 
            text="[ ACCESSING COMPROMISED DATA ARCHIVE ]", 
            style="Custom.TLabel"
        )
        title_label.pack(side=tk.LEFT, padx=5)
    
    def create_search_bar(self):
        """Create the search bar"""
        search_frame = ttk.Frame(self.main_frame, style="Search.TFrame")
        search_frame.pack(fill=tk.X, padx=10, pady=5)
        
        # Search label
        search_label = ttk.Label(search_frame, text="[ SEARCH ]:", style="Search.TLabel")
        search_label.pack(side=tk.LEFT, padx=5)
        
        # Search entry
        self.search_var = tk.StringVar()
        self.search_entry = ttk.Entry(search_frame, textvariable=self.search_var, width=40, style="Custom.TEntry")
        self.search_entry.pack(side=tk.LEFT, padx=5)
        
        # Case-sensitive search option
        self.case_sensitive_var = tk.BooleanVar(value=False)
        case_check = ttk.Checkbutton(
            search_frame, 
            text="Case Sensitive", 
            variable=self.case_sensitive_var,
            style="Search.TCheckbutton"
        )
        case_check.pack(side=tk.LEFT, padx=10)
        
        # Search count display
        self.search_count_var = tk.StringVar(value="")
        search_count_label = ttk.Label(
            search_frame, 
            textvariable=self.search_count_var, 
            style="Search.TLabel"
        )
        search_count_label.pack(side=tk.RIGHT, padx=10)
        
        # Navigation buttons
        self.search_prev_button = ttk.Button(
            search_frame, 
            text=" Prev", 
            style="Custom.TButton",
            command=self.prev_match
        )
        self.search_prev_button.pack(side=tk.RIGHT, padx=2)
        
        self.search_next_button = ttk.Button(
            search_frame, 
            text="Next ", 
            style="Custom.TButton",
            command=self.next_match
        )
        self.search_next_button.pack(side=tk.RIGHT, padx=2)
        
        self.search_button = ttk.Button(
            search_frame, 
            text="Scan", 
            style="Custom.TButton",
            command=self.perform_search
        )
        self.search_button.pack(side=tk.RIGHT, padx=5)
    
    def create_content_panes(self):
        """Create the split pane for list and content"""
        # Create a paned window
        self.paned_window = ttk.PanedWindow(self.main_frame, orient=tk.HORIZONTAL)
        self.paned_window.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)
        
        # Left panel for summary list
        self.list_frame = ttk.Frame(self.paned_window, style="Custom.TFrame")
        
        # Create a treeview for the list of summaries
        list_columns = ('index', 'folder')
        self.summary_tree = ttk.Treeview(self.list_frame, columns=list_columns, show='headings')
        self.summary_tree.heading('index', text='#')
        self.summary_tree.column('index', width=40, anchor=tk.CENTER)
        self.summary_tree.heading('folder', text='Source Location')
        
        # Add scrollbar for the treeview
        list_scrollbar = ttk.Scrollbar(
            self.list_frame, 
            orient=tk.VERTICAL, 
            command=self.summary_tree.yview,
            style="Custom.Vertical.TScrollbar"
        )
        self.summary_tree.configure(yscrollcommand=list_scrollbar.set)
        
        # Pack the treeview and scrollbar
        self.summary_tree.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        list_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        
        # Right panel for summary content
        self.content_frame = ttk.Frame(self.paned_window, style="Custom.TFrame")
        
        # Add the panels to the paned window
        self.paned_window.add(self.list_frame, weight=1)
        self.paned_window.add(self.content_frame, weight=3)
        
        # Create a text widget with scrollbars for the content
        text_container = ttk.Frame(self.content_frame, style="Custom.TFrame")
        text_container.pack(fill=tk.BOTH, expand=True)
        
        # Vertical scrollbar
        v_scrollbar = ttk.Scrollbar(
            text_container, 
            orient=tk.VERTICAL, 
            style="Custom.Vertical.TScrollbar"
        )
        v_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        
        # Horizontal scrollbar
        h_scrollbar = ttk.Scrollbar(
            text_container, 
            orient=tk.HORIZONTAL, 
            style="Custom.Horizontal.TScrollbar"
        )
        h_scrollbar.pack(side=tk.BOTTOM, fill=tk.X)
        
        # Text widget with custom cursor
        self.text_widget = tk.Text(
            text_container,
            wrap="none",  # Allow horizontal scrolling
            yscrollcommand=v_scrollbar.set,
            xscrollcommand=h_scrollbar.set,
            bg=self.colors['bg_light'],
            fg=self.colors['text_light'],
            font=("Courier New", 12),
            padx=5,
            pady=5,
            insertbackground=self.colors['accent'],  # Cursor color
            selectbackground=self.colors['accent'],
            selectforeground=self.colors['text_dark'],
            cursor="arrow"
        )
        self.text_widget.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        
        # Configure scrollbars
        v_scrollbar.config(command=self.text_widget.yview)
        h_scrollbar.config(command=self.text_widget.xview)
        
        # Create text tags for formatting
        self.text_widget.tag_configure("title", font=("Courier New", 14, "bold"), foreground=self.colors['accent'])
        self.text_widget.tag_configure("location", font=("Courier New", 12, "italic"), foreground=self.colors['text_light'])
        self.text_widget.tag_configure("hyperlink", foreground=self.colors['accent'], underline=1)
        self.text_widget.tag_configure("search_match", background=self.colors['highlight'], foreground=self.colors['text_dark'])
        self.text_widget.tag_configure("current_match", background=self.colors['accent'], foreground=self.colors['text_dark'])
    
    def create_status_bar(self):
        """Create the status bar"""
        status_frame = ttk.Frame(self.main_frame, style="Search.TFrame")
        status_frame.pack(fill=tk.X, side=tk.BOTTOM)
        
        self.status_var = tk.StringVar(value="[ SYSTEM READY ]")
        status_label = ttk.Label(
            status_frame, 
            textvariable=self.status_var, 
            style="Search.TLabel"
        )
        status_label.pack(side=tk.LEFT, padx=10, pady=3)

        # Add font size controls
        font_frame = ttk.Frame(status_frame, style="Search.TFrame")
        font_frame.pack(side=tk.RIGHT, padx=10)
        
        font_label = ttk.Label(font_frame, text="Font Size:", style="Search.TLabel")
        font_label.pack(side=tk.LEFT, padx=(0, 5))
        
        font_decrease = ttk.Button(font_frame, text="-", width=2, style="Custom.TButton", command=self.decrease_font)
        font_decrease.pack(side=tk.LEFT, padx=2)
        
        font_increase = ttk.Button(font_frame, text="+", width=2, style="Custom.TButton", command=self.increase_font)
        font_increase.pack(side=tk.LEFT, padx=2)
        
        # Add word wrap toggle
        self.wrap_var = tk.BooleanVar(value=False)
        wrap_check = ttk.Checkbutton(
            status_frame, 
            text="Word Wrap", 
            variable=self.wrap_var,
            style="Search.TCheckbutton",
            command=self.toggle_word_wrap
        )
        wrap_check.pack(side=tk.RIGHT, padx=10)
    
    def bind_shortcuts(self):
        """Bind keyboard shortcuts"""
        self.search_entry.bind("<Key>", self.search_key_handler)
        self.summary_tree.bind("<<TreeviewSelect>>", self.display_summary)
        self.summary_tree.bind("<Button-3>", self.context_menu_for_tree)
        self.root.bind("<Control-f>", lambda e: self.search_entry.focus())
        self.root.bind("<F3>", lambda e: self.next_match())
        self.root.bind("<Shift-F3>", lambda e: self.prev_match())
    
    def load_summaries(self):
        """Load summary data in a background thread"""
        self.status_var.set("[ SCANNING FILE SYSTEM ]")
        
        def background_load():
            summaries = gather_summary_data(BASE_DIRECTORY)
            
            # Update the UI in the main thread
            self.root.after(0, lambda: self.populate_summary_list(summaries))
        
        # Start background thread
        thread = threading.Thread(target=background_load)
        thread.daemon = True
        thread.start()
    
    def populate_summary_list(self, summaries):
        """Populate the summary list with data"""
        if not summaries:
            self.text_widget.config(state=tk.NORMAL)
            self.text_widget.insert(tk.END, "[ NO DATA FOUND ]\n")
            self.text_widget.config(state=tk.DISABLED)
            self.status_var.set("[ NO DATA FOUND IN TARGET DIRECTORY ]")
            return
        
        # Clear existing items
        for item in self.summary_tree.get_children():
            self.summary_tree.delete(item)
        
        # Add new items
        for index, (subfolder, content, filepath) in enumerate(summaries, start=1):
            item_id = self.summary_tree.insert(
                '', 'end', 
                values=(index, subfolder)
            )
            # Store the summary data for reference
            self.summaries_data[item_id] = (subfolder, content, filepath)
        
        # Select first item
        if self.summary_tree.get_children():
            first_item = self.summary_tree.get_children()[0]
            self.summary_tree.selection_set(first_item)
            self.summary_tree.focus(first_item)
            self.display_summary()
        
        self.status_var.set(f"[ {len(summaries)} DATA ARCHIVES IDENTIFIED ]")
    
    def find_urls_in_text(self, text):
        """Find all URLs in the given text and return a list of (start, end, url) tuples"""
        urls = []
        for match in re.finditer(URL_PATTERN, text):
            start, end = match.span()
            url = match.group(0)
            urls.append((start, end, url))
        return urls
    
    def open_url(self, url):
        """Open a URL in the default web browser"""
        try:
            webbrowser.open(url)
            self.status_var.set(f"[ ACCESSING EXTERNAL RESOURCE: {url} ]")
        except Exception as e:
            self.status_var.set(f"[ CONNECTION ERROR: {str(e)} ]")
    
    def display_summary(self, event=None):
        """Display the selected summary in the text widget"""
        selected_items = self.summary_tree.selection()
        if not selected_items:
            return
        
        item_id = selected_items[0]
        item_data = self.summaries_data.get(item_id)
        if not item_data:
            return
        
        folder_path, content, full_path = item_data
        
        # Clear the text widget
        self.text_widget.config(state=tk.NORMAL)
        self.text_widget.delete(1.0, tk.END)
        
        # Insert title
        self.text_widget.insert(tk.END, f"[ DATA FROM: {os.path.basename(folder_path)} ]\n", "title")
        
        # Insert location with a button to open it
        location_text = f"Location: {folder_path}"
        self.text_widget.insert(tk.END, location_text + "\n\n", "location")
        
        # Make the location clickable
        location_start = "1.0 + 10c"
        location_end = f"1.0 + {10 + len(folder_path)}c"
        location_tag = "location_link"
        self.text_widget.tag_configure(location_tag, underline=1)
        self.text_widget.tag_bind(location_tag, "<Button-1>", lambda e: open_file_location(full_path))
        self.text_widget.tag_add(location_tag, location_start, location_end)
        
        # Insert content and process URLs
        start_pos = self.text_widget.index(tk.END)
        self.text_widget.insert(tk.END, content)
        
        # Find and mark URLs
        all_content = self.text_widget.get(start_pos, tk.END)
        url_positions = self.find_urls_in_text(all_content)
        
        for start, end, url in url_positions:
            # Calculate positions in the text widget
            start_index = f"{start_pos} + {start} chars"
            end_index = f"{start_pos} + {end} chars"
            
            # Apply hyperlink tag
            self.text_widget.tag_add("hyperlink", start_index, end_index)
            
            # Bind click event
            tag_name = f"url_{start}_{end}"
            self.text_widget.tag_configure(tag_name)
            self.text_widget.tag_bind(tag_name, "<Button-1>", lambda e, u=url: self.open_url(u))
            self.text_widget.tag_add(tag_name, start_index, end_index)
        
        self.text_widget.config(state=tk.DISABLED)
        
        # Update status
        self.status_var.set(f"[ DISPLAYING: {folder_path} ]")
        
        # Clear search highlights when changing summaries
        self.search_results.clear()
        self.current_match_index.set(-1)
        self.search_count_var.set("")
    
    def perform_search(self):
        """Search for text in the current summary"""
        search_text = self.search_var.get()
        if not search_text:
            self.status_var.set("[ ENTER SEARCH TERM ]")
            return
        
        # Clear previous search highlighting
        self.text_widget.tag_remove("search_match", "1.0", tk.END)
        self.text_widget.tag_remove("current_match", "1.0", tk.END)
        
        # Find all matches
        self.search_results.clear()
        self.current_match_index.set(-1)
        
        start_idx = "1.0"
        count_var = tk.StringVar()
        
        while True:
            # Find the next match
            match_pos = self.text_widget.search(
                search_text, start_idx, tk.END, 
                count=count_var,
                nocase=not self.case_sensitive_var.get()
            )
            
            if not match_pos:
                break
                
            # Calculate end index
            match_length = int(count_var.get())
            match_end = f"{match_pos}+{match_length}c"
            
            # Add to results
            self.search_results.append((match_pos, match_end))
            
            # Highlight match
            self.text_widget.tag_add("search_match", match_pos, match_end)
            
            # Move to next starting position
            start_idx = match_end
        
        # Update search count
        num_results = len(self.search_results)
        if num_results > 0:
            self.search_count_var.set(f"[ {num_results} MATCHES ]")
            self.go_to_match(0)  # Go to first match
        else:
            self.search_count_var.set("[ NO MATCHES ]")
            self.status_var.set(f"[ NO RESULTS FOR: '{search_text}' ]")
    
    def go_to_match(self, index):
        """Go to a specific search match by index"""
        if not self.search_results:
            return
        
        # Make sure index is in bounds
        num_results = len(self.search_results)
        if index < 0:
            index = num_results - 1
        elif index >= num_results:
            index = 0
        
        # Remove current match highlight
        self.text_widget.tag_remove("current_match", "1.0", tk.END)
        
        # Highlight current match
        match_pos, match_end = self.search_results[index]
        self.text_widget.tag_add("current_match", match_pos, match_end)
        
        # Ensure visibility
        self.text_widget.see(match_pos)
        
        # Update current match index
        self.current_match_index.set(index)
        
        # Update status
        self.status_var.set(f"[ MATCH {index + 1} OF {num_results} ]")
        self.search_count_var.set(f"[ {index + 1} / {num_results} ]")
    
    def next_match(self):
        """Go to the next search match"""
        current = self.current_match_index.get()
        if current >= 0:
            self.go_to_match(current + 1)
    
    def prev_match(self):
        """Go to the previous search match"""
        current = self.current_match_index.get()
        if current >= 0:
            self.go_to_match(current - 1)
    
    def search_key_handler(self, event):
        """Handle key events in the search entry"""
        if event.keysym == "Return":
            self.perform_search()
            return "break"
        elif event.keysym == "Escape":
            # Clear search
            self.search_var.set("")
            self.text_widget.tag_remove("search_match", "1.0", tk.END)
            self.text_widget.tag_remove("current_match", "1.0", tk.END)
            self.search_results.clear()
            self.current_match_index.set(-1)
            self.search_count_var.set("")
            return "break"
    
    def context_menu_for_tree(self, event):
        """Show context menu for treeview items"""
        # Get the item under the mouse
        item = self.summary_tree.identify('item', event.x, event.y)
        if item:
            # Select the item
            self.summary_tree.selection_set(item)
            
            # Create a context menu
            context_menu = tk.Menu(self.root, tearoff=0, bg=self.colors['bg_light'], fg=self.colors['text_light'])
            
            # Get the item data
            item_data = self.summaries_data.get(item)
            if item_data:
                subfolder, content, filepath = item_data
                
                # Add menu items
                context_menu.add_command(
                    label="Open File Location", 
                    command=lambda: open_file_location(filepath)
                )
                
                # Show the menu
                context_menu.post(event.x_root, event.y_root)
    
    def increase_font(self):
        """Increase the font size in the text widget"""
        current_font = self.text_widget['font']
        if isinstance(current_font, str):
            # Parse font string
            parts = current_font.split()
            family = parts[0]
            size = int(parts[1])
        else:
            # Font is a tuple
            family, size = current_font
        
        new_size = min(size + 2, 36)  # Maximum size 36
        self.text_widget.configure(font=(family, new_size))
    
    def decrease_font(self):
        """Decrease the font size in the text widget"""
        current_font = self.text_widget['font']
        if isinstance(current_font, str):
            # Parse font string
            parts = current_font.split()
            family = parts[0]
            size = int(parts[1])
        else:
            # Font is a tuple
            family, size = current_font
        
        new_size = max(size - 2, 8)  # Minimum size 8
        self.text_widget.configure(font=(family, new_size))
    
    def toggle_word_wrap(self):
        """Toggle word wrap in the text widget"""
        if self.wrap_var.get():
            self.text_widget.configure(wrap="word")
        else:
            self.text_widget.configure(wrap="none")


def create_ui():
    """Create the main application UI"""
    root = tk.Tk()
    app = SummaryApp(root)
    root.mainloop()


if __name__ == "__main__":
    create_ui()

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/miniproton.py
========================================

#!/usr/bin/env python3
import argparse
import subprocess
import sys
import logging
import shutil
import traceback
import json
import os

VENV_PATH = os.path.expanduser("~/newvenv/bin/activate")  # Adjust virtualenv path as needed

def check_command(command):
    """Verify if a command is available in the system PATH."""
    if shutil.which(command) is None:
        logging.error(f"'{command}' is not installed. Please install it to proceed.")
        sys.exit(1)
    logging.debug(f"'{command}' is available.")

def run_command(command, label, debug=False, use_venv=False):
    """Execute a command, optionally using a virtual environment."""
    try:
        check_command(command[0])
        logging.debug(f"Executing command: {' '.join(command)}")

        # If running Photon, ensure it is within the virtual environment
        if use_venv:
            command = f"source {VENV_PATH} && " + " ".join(command)
            result = subprocess.run(command, shell=True, capture_output=True, text=True, executable="/bin/bash")
        else:
            result = subprocess.run(command, capture_output=True, text=True)

        if result.returncode != 0:
            logging.error(f"Error running {label}: {result.stderr.strip()}")
            if debug:
                logging.debug("Full stderr: " + result.stderr)
            return None

        logging.debug(f"{label} output: {result.stdout.strip()}")
        return result.stdout.strip()

    except Exception as exc:
        logging.error(f"Exception running {label}: {exc}")
        if debug:
            traceback.print_exc()
        return None

def run_ddgr(query, debug=False):
    """Run ddgr with the given search query and output formatted results."""
    print(f"\n[+] DuckDuckGo (ddgr) search results for: {query}\n")
    output = run_command(['ddgr', '--json', query], "ddgr", debug)
    
    if output:
        try:
            results = json.loads(output)
            for result in results[:10]:  # Limit output to 10 results
                print(f"- {result.get('title', 'No Title')}\n  {result.get('url', 'No URL')}\n")
        except json.JSONDecodeError:
            logging.error("Failed to parse ddgr JSON output.")

def run_photon(target, debug=False):
    """Run Photon OSINT tool on the given target URL."""
    print(f"\n[+] Photon OSINT results for: {target}\n")
    output = run_command(['python3', 'photon.py', '-u', target, '-o', 'json'], "Photon OSINT", debug, use_venv=True)
    
    if output:
        print(output)

def main():
    parser = argparse.ArgumentParser(description="OSINT tool using Photon OSINT and ddgr.")
    parser.add_argument('--query', type=str, help="Search query for DuckDuckGo (ddgr)")
    parser.add_argument('--target', type=str, help="Target URL for Photon OSINT scraping")
    parser.add_argument('--debug', action='store_true', help="Enable debug mode for detailed output")
    args = parser.parse_args()

    # Configure logging based on the debug flag.
    logging_level = logging.DEBUG if args.debug else logging.INFO
    logging.basicConfig(level=logging_level, format='[%(levelname)s] %(message)s')

    if args.debug:
        logging.debug("Debug mode enabled.")

    if not args.query and not args.target:
        parser.error("You must provide at least --query or --target (or both).")

    if args.query:
        run_ddgr(args.query, args.debug)

    if args.target:
        run_photon(args.target, args.debug)

if __name__ == '__main__':
    main()

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/ app.py
========================================

#!/usr/bin/env python3

import os
import re
import subprocess
import webbrowser
from flask import Flask, jsonify, request, send_from_directory
from flask_cors import CORS

# Change this path to the directory where you want to look for summary.txt files
BASE_DIRECTORY = "/home/jarvis/photon_results"

app = Flask(__name__, static_folder='static')
CORS(app)  # Enable CORS for all routes

# Regular expression pattern to match URLs
URL_PATTERN = r'https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+'

def gather_summary_data(base_path):
    """
    Recursively walk through 'base_path' and gather information about all 'summary.txt' files.
    Returns a list of dictionaries with id, folder_path and file_path.
    """
    all_summaries = []
    id_counter = 1

    for root, dirs, files in os.walk(base_path):
        for file in files:
            if file.lower() == "summary.txt":
                filepath = os.path.join(root, file)
                try:
                    relative_folder = os.path.relpath(root, base_path)
                    all_summaries.append({
                        "id": id_counter,
                        "folder": relative_folder,
                        "filepath": filepath
                    })
                    id_counter += 1
                except Exception as e:
                    print(f"Error processing {filepath}: {e}")

    return all_summaries

def get_summary_content(filepath):
    """Read and return the content of a summary file"""
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            content = f.read()
        return content
    except Exception as e:
        print(f"Error reading {filepath}: {e}")
        return f"Error reading file: {str(e)}"

def open_file_location(filepath):
    """Open the file's directory in the system file explorer"""
    directory = os.path.dirname(filepath)
    
    # Platform-specific commands to open the file explorer
    try:
        if os.name == 'nt':  # Windows
            os.startfile(directory)
        elif os.name == 'posix':  # Linux, macOS
            if os.system(f'xdg-open "{directory}"') != 0:  # Try Linux first
                os.system(f'open "{directory}"')  # Try macOS
        return True
    except Exception as e:
        print(f"Error opening location: {e}")
        return False

# API Routes
@app.route('/')
def serve_index():
    return send_from_directory('static', 'index.html')

@app.route('/api/summaries')
def get_summaries():
    """API endpoint to get all summaries"""
    try:
        summaries = gather_summary_data(BASE_DIRECTORY)
        return jsonify(summaries)
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/summary/<int:summary_id>')
def get_summary(summary_id):
    """API endpoint to get a specific summary's content"""
    try:
        summaries = gather_summary_data(BASE_DIRECTORY)
        
        # Find the summary with the matching ID
        summary = next((s for s in summaries if s["id"] == summary_id), None)
        
        if not summary:
            return jsonify({"error": "Summary not found"}), 404
        
        content = get_summary_content(summary["filepath"])
        
        return jsonify({
            "summary": summary,
            "content": content
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/open-location', methods=['POST'])
def api_open_location():
    """API endpoint to open a file location"""
    try:
        data = request.json
        if not data or 'path' not in data:
            return jsonify({"error": "No path provided"}), 400
        
        filepath = data['path']
        success = open_file_location(filepath)
        
        if success:
            return jsonify({"status": "success"})
        else:
            return jsonify({"error": "Failed to open location"}), 500
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/open-url', methods=['POST'])
def api_open_url():
    """API endpoint to open a URL in the browser"""
    try:
        data = request.json
        if not data or 'url' not in data:
            return jsonify({"error": "No URL provided"}), 400
        
        url = data['url']
        # Validate URL
        if not re.match(URL_PATTERN, url):
            return jsonify({"error": "Invalid URL"}), 400
            
        webbrowser.open(url)
        return jsonify({"status": "success"})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == "__main__":
    # Create the static directory if it doesn't exist
    os.makedirs('static', exist_ok=True)
    
    # Copy the HTML file to the static directory (in a real app, you'd have a build process)
    # For now, assuming the HTML file is in the same directory as this script
    try:
        with open('frontend.html', 'r') as src, open('static/index.html', 'w') as dst:
            dst.write(src.read())
    except FileNotFoundError:
        print("Warning: frontend.html not found. Please place the HTML file in the static directory.")
    
    # Start the Flask server
    print(f"Starting server on http://localhost:5000")
    app.run(debug=True, host='0.0.0.0', port=5000)

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/deepsearch-proton.py
========================================

#!/usr/bin/env python3
import argparse
import subprocess
import sys
import logging
import shutil
import traceback
import json
import os
import re
import time
import textwrap
from collections import OrderedDict

__version__ = "2.0"

# Path to your virtual environment activation script and Photon script
VENV_PATH = os.path.expanduser("~/newvenv/bin/activate")
PHOTON_SCRIPT = os.path.join(os.path.dirname(os.path.realpath(__file__)), "photon.py")

def check_command(command):
    """Verify if a command is available in the system PATH."""
    if shutil.which(command) is None:
        logging.error(f"'{command}' is not installed. Please install it to proceed.")
        sys.exit(1)
    logging.debug(f"'{command}' is available.")

def pretty_print_json(output):
    """Attempt to parse and pretty-print JSON output; if it fails, return raw output."""
    try:
        data = json.loads(output)
        return json.dumps(data, indent=2)
    except json.JSONDecodeError:
        return output

def run_command(command, label, debug=False, use_venv=False, ignore_errors=False):
    """
    Execute a command, optionally within a virtual environment.
    Returns the command output as a string.
    """
    try:
        if use_venv:
            # For commands like Photon that must run inside your virtualenv,
            # we assume python3 is available.
            check_command("python3")
        else:
            check_command(command[0])
        logging.debug(f"Executing command: {' '.join(command)}")
        
        if use_venv:
            # Combine the activation command with the target command
            full_command = f"source {VENV_PATH} && " + " ".join(command)
            result = subprocess.run(full_command, shell=True, capture_output=True, text=True, executable="/bin/bash")
        else:
            result = subprocess.run(command, capture_output=True, text=True)

        if result.returncode != 0:
            logging.error(f"Error running {label}: {result.stderr.strip()}")
            if debug:
                logging.debug("Full stderr: " + result.stderr)
            if ignore_errors:
                # Return empty result but don't fail completely
                return ""
            return None
        logging.debug(f"{label} output: {result.stdout.strip()}")
        return result.stdout.strip()
    except Exception as exc:
        logging.error(f"Exception running {label}: {exc}")
        if debug:
            traceback.print_exc()
        return None

def calculate_resource_values(depth):
    """
    Calculate appropriate resource values based on the depth parameter (1-100).
    Returns a dictionary with calibrated values for different tools.
    
    Depth scale:
    1-20: Light/Quick search
    21-50: Medium search
    51-80: Thorough search
    81-100: Deep/Extensive search
    """
    # Ensure depth is within valid range (1-100)
    depth = max(1, min(100, depth))
    
    # Always use maximum page size for ddgr (25 results)
    ddgr_page_size = 25
    ddgr_results = 25
    
    # Calculate Photon crawl parameters
    # Level: 1 to 5 (Photon's internal crawl depth)
    photon_level = 1 + int((depth - 1) / 20)  # Ranges from 1-5
    
    # Threads: 2 to 50 (threading for parallel processing)
    photon_threads = 2 + int((depth - 1) * 0.48)  # Linear scaling
    
    # Timeout: 5s to 30s (longer for deeper crawls)
    photon_timeout = 5 + int((depth - 1) * 0.25)  # Linear scaling
    
    # URLs to crawl per page: proportional to depth
    if depth <= 20:
        urls_percent = 0.6  # Crawl 60% of results for light search
    elif depth <= 50:
        urls_percent = 0.8  # Crawl 80% of results for medium search
    elif depth <= 80:
        urls_percent = 0.9  # Crawl 90% of results for thorough search
    else:
        urls_percent = 1.0  # Crawl 100% of results for deep search
    
    urls_to_crawl = max(1, min(ddgr_page_size, round(ddgr_page_size * urls_percent)))
    
    return {
        "ddgr_results": ddgr_results,
        "ddgr_page_size": ddgr_page_size,
        "photon_threads": photon_threads,
        "photon_level": photon_level,
        "photon_timeout": photon_timeout,
        "urls_to_crawl": urls_to_crawl
    }

def deduplicate_urls(new_urls, existing_urls):
    """
    Add new URLs to the existing set while avoiding duplicates.
    
    Parameters:
    - new_urls: List of new URLs to add
    - existing_urls: Set of already processed URLs
    
    Returns:
    - List of URLs that aren't already in existing_urls
    """
    unique_new_urls = []
    for url in new_urls:
        if url not in existing_urls:
            existing_urls.add(url)
            unique_new_urls.append(url)
    
    return unique_new_urls

def build_ddgr_command(query, ddgr_args, page_size=25):
    """Build the ddgr command with all user-supplied arguments."""
    command = ['ddgr', '--json']
    
    # Ensure we're using maximum page size for ddgr
    command.extend(['--num', str(page_size)])
    
    # Add all ddgr-specific arguments
    if ddgr_args.get('region'):
        command.extend(['--reg', ddgr_args.get('region')])
    if ddgr_args.get('colorize'):
        command.extend(['--colorize', ddgr_args.get('colorize')])
    if ddgr_args.get('nocolor'):
        command.append('--nocolor')
    if ddgr_args.get('colors'):
        command.extend(['--colors', ddgr_args.get('colors')])
    if ddgr_args.get('time'):
        command.extend(['--time', ddgr_args.get('time')])
    if ddgr_args.get('site'):
        for site in ddgr_args.get('site'):
            command.extend(['--site', site])
    if ddgr_args.get('expand'):
        command.append('--expand')
    if ddgr_args.get('proxy'):
        command.extend(['--proxy', ddgr_args.get('proxy')])
    if ddgr_args.get('unsafe'):
        command.append('--unsafe')
    if ddgr_args.get('noua'):
        command.append('--noua')
    
    # Add the query
    command.append(query)
    
    return command

def run_ddgr_with_pagination(query, ddgr_args, depth=30, debug=False, page=1):
    """
    Run DuckDuckGo search using ddgr with proper pagination support.
    This function uses ddgr's interactive mode and simulates pagination commands.
    
    Parameters:
    - query: The search query string
    - ddgr_args: Dictionary of ddgr-specific arguments
    - depth: Depth parameter (1-100)
    - debug: Whether to enable debug output
    - page: Page number (1-based for user display)
    
    Returns:
    - List of URLs from search results
    - Boolean indicating if there might be more results
    """
    resources = calculate_resource_values(depth)
    page_size = resources["ddgr_page_size"]  # Now this will always be 25
    
    # For first page, search normally
    if page == 1:
        print(f"\n[+] DuckDuckGo (ddgr) search results for: {query}")
        print(f"[+] Search depth: {depth}/100 (Page {page}, retrieving {page_size} results)")
        command = build_ddgr_command(query, ddgr_args, page_size)
    else:
        print(f"\n[+] DuckDuckGo (ddgr) search results - Page {page} for: {query}")
        print(f"[+] Search depth: {depth}/100 (retrieving {page_size} results per page)")
        
        # For subsequent pages, we need to simulate pagination
        # Always use maximum results (25) to reduce pagination steps
        initial_results = 25  # This is the maximum for ddgr
        command = ['ddgr', '--json', '--num', str(initial_results), query]
        # Add other args
        if ddgr_args.get('region'):
            command.extend(['--reg', ddgr_args.get('region')])
        if ddgr_args.get('time'):
            command.extend(['--time', ddgr_args.get('time')])
        if ddgr_args.get('site'):
            for site in ddgr_args.get('site'):
                command.extend(['--site', site])
        if ddgr_args.get('unsafe'):
            command.append('--unsafe')
    
    output = run_command(command, "ddgr", debug)
    if not output:
        print("[-] No results from ddgr search")
        return [], False
    
    urls = []
    try:
        results = json.loads(output)
        
        # If we're on a later page, we need to extract the correct subset of results
        result_offset = 0
        if page > 1:
            # Calculate the offset for the current page
            # For page 2, we want results 25-49 (assuming page_size=25)
            # For page 3, we want results 50-74, etc.
            result_offset = (page - 1) * page_size
            
            # If offset is beyond available results, no more results
            if result_offset >= len(results):
                print("[-] No more results available.")
                return [], False
            
            # Get the slice of results for this page
            end_offset = min(result_offset + page_size, len(results))
            page_results = results[result_offset:end_offset]
        else:
            # First page, just take the first page_size results
            page_results = results[:page_size]
        
        # Process and display the results
        for i, result in enumerate(page_results, 1):
            title = result.get("title", "No Title")
            url = result.get("url", "No URL")
            abstract = result.get("abstract", "")
            
            # Add URLs to the list for Photon crawling
            urls.append(url)
            
            # Display result with correct global index
            global_index = result_offset + i
            print(f"{global_index}. {title}")
            print(f"   {url}")
            if abstract:
                wrapped_abstract = textwrap.fill(abstract, width=80, initial_indent="   ", subsequent_indent="   ")
                print(f"{wrapped_abstract}\n")
            else:
                print()  # Empty line for spacing
        
        # Determine if there might be more results
        # We consider there are more if:
        # 1. We got a full page of results, or
        # 2. We know there are more results in our fetched batch
        has_more = (len(page_results) == page_size) or (result_offset + len(page_results) < len(results))
        
        return urls, has_more
    
    except json.JSONDecodeError:
        logging.error("Failed to parse ddgr JSON output.")
        print(output)
        return [], False

def build_photon_command(target, photon_args, output_dir):
    """Build the Photon command with all user-supplied arguments."""
    command = [
        'python3', PHOTON_SCRIPT,
        '-u', target,
        '-o', output_dir
    ]
    
    # Add all photon-specific arguments
    if photon_args.get('level') is not None:
        command.extend(['-l', str(photon_args.get('level'))])
    if photon_args.get('threads') is not None:
        command.extend(['-t', str(photon_args.get('threads'))])
    if photon_args.get('delay') is not None:
        command.extend(['-d', str(photon_args.get('delay'))])
    if photon_args.get('timeout') is not None:
        command.extend(['--timeout', str(photon_args.get('timeout'))])
    if photon_args.get('cookie'):
        command.extend(['-c', photon_args.get('cookie')])
    if photon_args.get('regex'):
        command.extend(['-r', photon_args.get('regex')])
    if photon_args.get('export'):
        command.extend(['-e', photon_args.get('export')])
    if photon_args.get('seeds'):
        command.extend(['-s'] + photon_args.get('seeds'))
    if photon_args.get('user_agent'):
        command.extend(['--user-agent', photon_args.get('user_agent')])
    if photon_args.get('exclude'):
        command.extend(['--exclude', photon_args.get('exclude')])
    if photon_args.get('proxy'):
        command.extend(['-p', photon_args.get('proxy')])
    
    # Add boolean flags
    if photon_args.get('verbose'):
        command.append('-v')
    if photon_args.get('headers'):
        command.append('--headers')
    if photon_args.get('dns'):
        command.append('--dns')
    if photon_args.get('keys'):
        command.append('--keys')
    if photon_args.get('only_urls'):
        command.append('--only-urls')
    if photon_args.get('wayback'):
        command.append('--wayback')
    
    return command

def run_photon_on_single_target(target, photon_args, depth=30, debug=False, index=None, total=None):
    """Run Photon OSINT on a single target URL with user-specified arguments."""
    resources = calculate_resource_values(depth)
    
    # Use calculated resources if not provided in photon_args
    level = photon_args.get('level') or resources["photon_level"]
    threads = photon_args.get('threads') or resources["photon_threads"]
    timeout = photon_args.get('timeout') or resources["photon_timeout"]
    
    # Create a progress indicator if we're processing multiple URLs
    progress_str = ""
    if index is not None and total is not None:
        progress_str = f"[{index}/{total}] "
    
    print(f"\n[+] {progress_str}Photon crawling target: {target}")
    print(f"[+] Crawl depth: {depth}/100 (level: {level}, threads: {threads}, timeout: {timeout}s)")
    
    if not os.path.isfile(PHOTON_SCRIPT):
        logging.error(f"Photon script not found at {PHOTON_SCRIPT}. Please ensure photon.py is available.")
        return None
    
    # Create organized output folder structure
    main_output_dir = "photon_results"
    if not os.path.exists(main_output_dir):
        os.makedirs(main_output_dir)
    
    # Get domain name for subfolder
    domain = target.replace("https://", "").replace("http://", "").split("/")[0]
    domain_safe = re.sub(r'[^\w\-_]', '_', domain)  # Make domain name safe for filesystem
    
    # Create a unique subfolder for this target
    timestamp = int(time.time())
    target_dir = f"{domain_safe}_{timestamp}"
    output_dir = os.path.join(main_output_dir, target_dir)
    
    # Build the Photon command
    command = build_photon_command(target, photon_args, output_dir)
    
    # Run the command with the proper settings and handle failures gracefully
    output = run_command(command, f"Photon OSINT on {target}", debug, use_venv=True, ignore_errors=True)
    
    # Create the output directory even if the command failed
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        
    if output is not None:
        print(f"[+] Completed scanning {target}")
        print(f"[+] Results saved to {output_dir}/")
        return output_dir
    
    # If the output is None, it means the command completely failed
    print(f"[!] Issues encountered while scanning {target}, but continuing...")
    print(f"[+] Partial results may be available in {output_dir}/")
    return output_dir

def run_photon_on_multiple_targets(targets, photon_args, depth=30, debug=False):
    """Run Photon OSINT on multiple target URLs with user-specified arguments."""
    if not targets:
        print("[-] No targets to crawl with Photon.")
        return
    
    resources = calculate_resource_values(depth)
    max_targets = photon_args.get('max_targets') or resources["urls_to_crawl"]
    
    # Deduplicate targets while preserving order
    unique_targets = list(OrderedDict.fromkeys(targets))
    
    if len(unique_targets) > max_targets:
        print(f"[*] Limiting Photon crawl to top {max_targets} targets based on depth setting {depth}/100")
        targets_to_crawl = unique_targets[:max_targets]
    else:
        targets_to_crawl = unique_targets
    
    print(f"\n[+] Starting Photon crawler on {len(targets_to_crawl)} targets")
    
    results = []
    for i, target in enumerate(targets_to_crawl, 1):
        result_dir = run_photon_on_single_target(
            target, photon_args, depth, debug, i, len(targets_to_crawl)
        )
        if result_dir:
            results.append((target, result_dir))
    
    if results:
        print("\n[+] Photon crawling complete. Summary:")
        print(f"[+] All results saved to the 'photon_results/' directory")
        for target, output_dir in results:
            print(f"  - {target} -> {os.path.basename(output_dir)}/")
    else:
        print("\n[-] No successful Photon crawls.")

def ask_for_more():
    """Ask the user if they want more results."""
    while True:
        answer = input("\nMore results? (y/n): ").lower().strip()
        if answer in ['y', 'yes']:
            return True
        elif answer in ['n', 'no']:
            return False
        else:
            print("Please answer 'y' or 'n'.")

def is_url(text):
    """Check if the given text is a URL."""
    url_pattern = re.compile(r'^https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+')
    return bool(url_pattern.match(text))

def main():
    # Create argument parser with extensive options for both ddgr and photon
    parser = argparse.ArgumentParser(
        description="Enhanced Deep Search OSINT tool using Photon OSINT and ddgr with maximum results mode.",
        epilog=("Examples:\n"
                "  python3 deepsearch-proton.py \"search keywords\" 25                          # Search and crawl results with medium depth\n"
                "  python3 deepsearch-proton.py \"search keywords\" \"https://example.com\" 45    # Search, crawl specific site and search results\n"
                "  python3 deepsearch-proton.py --query 'osint tools' --depth 75 --no-crawl   # Search only, no crawling\n"
                "  python3 deepsearch-proton.py --target 'https://example.com' --depth 100     # Crawl only a specific site deeply\n"
                "  python3 deepsearch-proton.py \"keywords\" --max-pages 10 --auto-paginate     # Auto-paginate through 10 pages of results\n"
                "\nDepth Values (1-100):\n"
                "  1-20:   Quick/light crawl (fewer results, shallow depth, faster)\n"
                "  21-50:  Medium crawl (moderate results and depth)\n"
                "  51-80:  Thorough crawl (more results, deeper level)\n"
                "  81-100: Deep crawl (maximum results and depth level, longer runtime)"),
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    # Basic options
    basic_group = parser.add_argument_group('Basic Options')
    basic_group.add_argument('--query', type=str, 
                    help="Search query for DuckDuckGo (ddgr)")
    basic_group.add_argument('--target', type=str, 
                    help="Target URL for Photon OSINT scraping")
    basic_group.add_argument('--depth', type=int, default=30, 
                    help="Search depth (1-100): 1=quickest, 100=deepest. Controls number of results, crawl depth, and threads")
    basic_group.add_argument('--debug', action='store_true', 
                    help="Enable debug mode for detailed output and error messages")
    basic_group.add_argument('--no-crawl', action='store_true',
                    help="Disable automatic crawling of search results (search only)")
    basic_group.add_argument('--no-pagination', action='store_true',
                    help="Disable pagination ('more' prompt at the end)")
    basic_group.add_argument('--max-pages', type=int, metavar='N', default=0,
                    help="Maximum number of search result pages to fetch (0 for unlimited)")
    basic_group.add_argument('--auto-paginate', action='store_true',
                    help="Automatically fetch all pages without prompting")
    basic_group.add_argument('--version', action='version', 
                    version=f"%(prog)s {__version__}")
    
    # ddgr-specific options
    ddgr_group = parser.add_argument_group('DuckDuckGo (ddgr) Options')
    ddgr_group.add_argument('--ddgr-region', type=str, metavar='REG', default='us-en',
                    help="region-specific search e.g. 'us-en' for US (default)")
    ddgr_group.add_argument('--ddgr-colorize', type=str, choices=['auto', 'always', 'never'], default='auto',
                    help="whether to colorize output")
    ddgr_group.add_argument('--ddgr-nocolor', action='store_true',
                    help="equivalent to --ddgr-colorize=never")
    ddgr_group.add_argument('--ddgr-colors', type=str, metavar='COLORS',
                    help="set output colors")
    ddgr_group.add_argument('--ddgr-time', type=str, metavar='SPAN', choices=('d', 'w', 'm', 'y'),
                    help="time limit search [d (1 day), w (1 wk), m (1 month), y (1 year)]")
    ddgr_group.add_argument('--ddgr-site', type=str, metavar='SITE', action='append',
                    help="search sites using DuckDuckGo")
    ddgr_group.add_argument('--ddgr-expand', action='store_true',
                    help="Show complete url in search results")
    ddgr_group.add_argument('--ddgr-proxy', type=str, metavar='URI',
                    help="tunnel traffic through an HTTPS proxy; URI format: [http[s]://][user:pwd@]host[:port]")
    ddgr_group.add_argument('--ddgr-unsafe', action='store_true',
                    help="disable safe search")
    ddgr_group.add_argument('--ddgr-noua', action='store_true',
                    help="disable user agent")
    
    # Photon-specific options
    photon_group = parser.add_argument_group('Photon OSINT Options')
    photon_group.add_argument('--photon-level', type=int, metavar='LEVEL', 
                    help="levels to crawl (1-5)")
    photon_group.add_argument('--photon-threads', type=int, metavar='THREADS',
                    help="number of threads")
    photon_group.add_argument('--photon-delay', type=float, metavar='DELAY',
                    help="delay between requests")
    photon_group.add_argument('--photon-timeout', type=float, metavar='TIMEOUT',
                    help="http request timeout")
    photon_group.add_argument('--photon-cookie', type=str, metavar='COOKIE',
                    help="cookie")
    photon_group.add_argument('--photon-regex', type=str, metavar='REGEX',
                    help="regex pattern")
    photon_group.add_argument('--photon-export', type=str, metavar='FORMAT', choices=['csv', 'json'],
                    help="export format (csv, json)")
    photon_group.add_argument('--photon-seeds', type=str, metavar='SEEDS', action='append',
                    help="additional seed URLs")
    photon_group.add_argument('--photon-user-agent', type=str, metavar='UA',
                    help="custom user agent(s)")
    photon_group.add_argument('--photon-exclude', type=str, metavar='REGEX',
                    help="exclude URLs matching this regex")
    photon_group.add_argument('--photon-proxy', type=str, metavar='PROXY',
                    help="Proxy server IP:PORT or DOMAIN:PORT")
    photon_group.add_argument('--photon-verbose', action='store_true',
                    help="verbose output")
    photon_group.add_argument('--photon-headers', action='store_true',
                    help="add headers")
    photon_group.add_argument('--photon-dns', action='store_true',
                    help="enumerate subdomains and DNS data")
    photon_group.add_argument('--photon-keys', action='store_true',
                    help="find secret keys")
    photon_group.add_argument('--photon-only-urls', action='store_true',
                    help="only extract URLs")
    photon_group.add_argument('--photon-wayback', action='store_true',
                    help="fetch URLs from archive.org as seeds")
    photon_group.add_argument('--photon-max-targets', type=int, metavar='N',
                    help="maximum number of targets to crawl (overrides automatic scaling)")
    
    # Positional arguments (for simpler command line usage)
    parser.add_argument('keywords', type=str, nargs='?', 
                    help="Search keywords (e.g., \"George Washington\")")
    parser.add_argument('url_or_depth', type=str, nargs='?',
                    help="Either a URL to crawl (e.g., \"https://example.com\") or depth value (e.g., \"45\")")
    parser.add_argument('positional_depth', type=str, nargs='?',
                    help="Depth value when URL is provided (e.g., \"45\" when using format: \"keywords URL depth\")")
    
    args = parser.parse_args()

    # Configure logging based on the debug flag
    logging_level = logging.DEBUG if args.debug else logging.INFO
    logging.basicConfig(level=logging_level, format='[%(levelname)s] %(message)s')
    
    if args.debug:
        logging.debug("Debug mode enabled.")

    # Process positional arguments if provided
    if args.keywords:
        # We have at least the keywords argument
        query = args.keywords
        target = None
        depth = args.depth  # Default to named argument depth
        
        if args.url_or_depth:
            # Second argument could be either a URL or depth
            if is_url(args.url_or_depth):
                # It's a URL
                target = args.url_or_depth
                # If we have a third positional argument, it's the depth
                if args.positional_depth and args.positional_depth.isdigit():
                    depth = int(args.positional_depth)
            elif args.url_or_depth.isdigit():
                # It's a depth value
                depth = int(args.url_or_depth)
    else:
        # Use named arguments
        query = args.query
        target = args.target
        depth = args.depth
    
    # Ensure depth is within valid range
    if depth is not None:
        depth = max(1, min(100, depth))
    
    # Ensure we have at least one action to perform
    if not query and not target:
        parser.error("You must provide either keywords, a URL, or use --query/--target arguments.")
    
    # Initialize list of URLs to crawl with Photon
    urls_to_crawl = []
    
    # Add specifically provided target URL if any
    if target:
        urls_to_crawl.append(target)
    
    # Prepare ddgr arguments
    ddgr_args = {
        'region': args.ddgr_region,
        'colorize': args.ddgr_colorize,
        'nocolor': args.ddgr_nocolor,
        'colors': args.ddgr_colors,
        'time': args.ddgr_time,
        'site': args.ddgr_site,
        'expand': args.ddgr_expand,
        'proxy': args.ddgr_proxy,
        'unsafe': args.ddgr_unsafe,
        'noua': args.ddgr_noua
    }
    
    # Prepare photon arguments
    photon_args = {
        'level': args.photon_level,
        'threads': args.photon_threads,
        'delay': args.photon_delay,
        'timeout': args.photon_timeout,
        'cookie': args.photon_cookie,
        'regex': args.photon_regex,
        'export': args.photon_export,
        'seeds': args.photon_seeds,
        'user_agent': args.photon_user_agent,
        'exclude': args.photon_exclude,
        'proxy': args.photon_proxy,
        'verbose': args.photon_verbose,
        'headers': args.photon_headers,
        'dns': args.photon_dns,
        'keys': args.photon_keys,
        'only_urls': args.photon_only_urls,
        'wayback': args.photon_wayback,
        'max_targets': args.photon_max_targets
    }
    
    # Run ddgr search if query is provided
    if query:
        page = 1  # Start at page 1 (1-based for user display)
        has_more = True
        processed_urls = set()  # Keep track of all URLs found (as a set to avoid duplicates)
        stagnant_page_count = 0  # Track how many consecutive pages yield no new URLs
        
        print(f"\n[+] Running enhanced deep search for: {query}")
        print(f"[+] Search depth: {depth}/100 (Maximum results mode enabled)")
        
        while has_more:
            # Run search for current page with proper pagination
            search_urls, has_more = run_ddgr_with_pagination(query, ddgr_args, depth, args.debug, page)
            
            # Track if we found new URLs in this page
            found_new_urls = False
            
            # Process search results and add to deduplication set
            if search_urls:
                # Get only unique new URLs
                unique_new_urls = deduplicate_urls(search_urls, processed_urls)
                
                if unique_new_urls:
                    found_new_urls = True
                    print(f"[+] Found {len(unique_new_urls)} new unique URLs on page {page}")
                    print(f"[+] Total unique URLs found so far: {len(processed_urls)}")
                    
                    # Add search results to crawl list if auto-crawl is enabled
                    if not args.no_crawl:
                        # Run Photon only on the unique new URLs
                        run_photon_on_multiple_targets(unique_new_urls, photon_args, depth, args.debug)
                else:
                    print(f"[+] No new unique URLs found on page {page}")
                    stagnant_page_count += 1
            else:
                print(f"[+] No URLs found on page {page}")
                stagnant_page_count += 1
            
            # Intelligent pagination stop conditions:
            
            # 1. Check if we've had too many consecutive pages with no new results
            if stagnant_page_count >= 3:
                print(f"[+] No new unique URLs found for {stagnant_page_count} consecutive pages, stopping pagination")
                break
                
            # 2. Check if we've reached the maximum page limit
            if args.max_pages > 0 and page >= args.max_pages:
                print(f"[+] Reached maximum page limit ({args.max_pages} pages)")
                break
                
            # 3. Check if pagination is disabled or if we've reached the end
            if args.no_pagination or not has_more:
                break
                
            # 4. Reset stagnant page counter if we found new URLs
            if found_new_urls:
                stagnant_page_count = 0
            
            # 5. If auto-paginate is enabled, continue without asking
            if args.auto_paginate:
                page += 1
                print(f"[+] Automatically proceeding to page {page}...")
                continue
                
            # 6. Ask for more results
            if not ask_for_more():
                break
                
            # Move to next page
            page += 1
            
        print(f"\n[+] Search complete. Processed {page} page(s) with {len(processed_urls)} unique URLs.")
        
    elif target and not args.no_crawl:
        # If no query but a target was provided, crawl that target
        run_photon_on_single_target(target, photon_args, depth, args.debug)

if __name__ == '__main__':
    main()

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/plugins/find_subdomains.py
========================================

"""Support for findsubdomains.com."""
from re import findall

from requests import get


def find_subdomains(domain):
    """Find subdomains according to the TLD."""
    result = set()
    response = get('https://findsubdomains.com/subdomains-of/' + domain).text
    matches = findall(r'(?s)<div class="domains js-domain-name">(.*?)</div>', response)
    for match in matches:
        result.add(match.replace(' ', '').replace('\n', ''))
    return list(result)

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/plugins/wayback.py
========================================

"""Support for archive.org."""
import datetime
import json

from requests import get


def time_machine(host, mode):
    """Query archive.org."""
    now = datetime.datetime.now()
    to = str(now.year) + str(now.day) + str(now.month)
    if now.month > 6:
    	fro = str(now.year) + str(now.day) + str(now.month - 6)
    else:
    	fro = str(now.year - 1) + str(now.day) + str(now.month + 6)
    url = "http://web.archive.org/cdx/search?url=%s&matchType=%s&collapse=urlkey&fl=original&filter=mimetype:text/html&filter=statuscode:200&output=json&from=%s&to=%s" % (host, mode, fro, to)
    response = get(url).text
    parsed = json.loads(response)[1:]
    urls = []
    for item in parsed:
        urls.append(item[0])
    return urls

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/plugins/exporter.py
========================================

"""Support for exporting the results."""
import csv
import json


def exporter(directory, method, datasets):
    """Export the results."""
    if method.lower() == 'json':
        # Convert json_dict to a JSON styled string
        json_string = json.dumps(datasets, indent=4)
        savefile = open('{}/exported.json'.format(directory), 'w+')
        savefile.write(json_string)
        savefile.close()

    if method.lower() == 'csv':
        with open('{}/exported.csv'.format(directory), 'w+') as csvfile:
            csv_writer = csv.writer(
                csvfile, delimiter=',', quoting=csv.QUOTE_MINIMAL)
            for key, values in datasets.items():
                if values is None:
                    csv_writer.writerow([key])
                else:
                    csv_writer.writerow([key] + values)
        csvfile.close()

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/plugins/__init__.py
========================================

"""Plugins for Photon."""

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/plugins/dnsdumpster.py
========================================

"""Support for dnsdumpster.com."""
import re

import requests


def dnsdumpster(domain, output_dir):
    """Query dnsdumpster.com."""
    response = requests.Session().get('https://dnsdumpster.com/').text
    csrf_token = re.search(
        r'name=\"csrfmiddlewaretoken\" value=\"(.*?)\"', response).group(1)

    cookies = {'csrftoken': csrf_token}
    headers = {'Referer': 'https://dnsdumpster.com/'}
    data = {'csrfmiddlewaretoken': csrf_token, 'targetip': domain}
    response = requests.Session().post(
        'https://dnsdumpster.com/', cookies=cookies, data=data, headers=headers)

    image = requests.get('https://dnsdumpster.com/static/map/%s.png' % domain)
    if image.status_code == 200:
        with open('%s/%s.png' % (output_dir, domain), 'wb') as f:
            f.write(image.content)

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/sumsearchweb/ app.py
========================================

#!/usr/bin/env python3

import os
import re
import subprocess
import webbrowser
from flask import Flask, jsonify, request, send_from_directory
from flask_cors import CORS

# Change this path to the directory where you want to look for summary.txt files
BASE_DIRECTORY = "/home/jarvis/photon_results"

app = Flask(__name__, static_folder='static')
CORS(app)  # Enable CORS for all routes

# Regular expression pattern to match URLs
URL_PATTERN = r'https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+'

def gather_summary_data(base_path):
    """
    Recursively walk through 'base_path' and gather information about all 'summary.txt' files.
    Returns a list of dictionaries with id, folder_path and file_path.
    """
    all_summaries = []
    id_counter = 1

    for root, dirs, files in os.walk(base_path):
        for file in files:
            if file.lower() == "summary.txt":
                filepath = os.path.join(root, file)
                try:
                    relative_folder = os.path.relpath(root, base_path)
                    all_summaries.append({
                        "id": id_counter,
                        "folder": relative_folder,
                        "filepath": filepath
                    })
                    id_counter += 1
                except Exception as e:
                    print(f"Error processing {filepath}: {e}")

    return all_summaries

def get_summary_content(filepath):
    """Read and return the content of a summary file"""
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            content = f.read()
        return content
    except Exception as e:
        print(f"Error reading {filepath}: {e}")
        return f"Error reading file: {str(e)}"

def open_file_location(filepath):
    """Open the file's directory in the system file explorer"""
    directory = os.path.dirname(filepath)
    
    # Platform-specific commands to open the file explorer
    try:
        if os.name == 'nt':  # Windows
            os.startfile(directory)
        elif os.name == 'posix':  # Linux, macOS
            if os.system(f'xdg-open "{directory}"') != 0:  # Try Linux first
                os.system(f'open "{directory}"')  # Try macOS
        return True
    except Exception as e:
        print(f"Error opening location: {e}")
        return False

# API Routes
@app.route('/')
def serve_index():
    return send_from_directory('static', 'index.html')

@app.route('/api/summaries')
def get_summaries():
    """API endpoint to get all summaries"""
    try:
        summaries = gather_summary_data(BASE_DIRECTORY)
        return jsonify(summaries)
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/summary/<int:summary_id>')
def get_summary(summary_id):
    """API endpoint to get a specific summary's content"""
    try:
        summaries = gather_summary_data(BASE_DIRECTORY)
        
        # Find the summary with the matching ID
        summary = next((s for s in summaries if s["id"] == summary_id), None)
        
        if not summary:
            return jsonify({"error": "Summary not found"}), 404
        
        content = get_summary_content(summary["filepath"])
        
        return jsonify({
            "summary": summary,
            "content": content
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/open-location', methods=['POST'])
def api_open_location():
    """API endpoint to open a file location"""
    try:
        data = request.json
        if not data or 'path' not in data:
            return jsonify({"error": "No path provided"}), 400
        
        filepath = data['path']
        success = open_file_location(filepath)
        
        if success:
            return jsonify({"status": "success"})
        else:
            return jsonify({"error": "Failed to open location"}), 500
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/open-url', methods=['POST'])
def api_open_url():
    """API endpoint to open a URL in the browser"""
    try:
        data = request.json
        if not data or 'url' not in data:
            return jsonify({"error": "No URL provided"}), 400
        
        url = data['url']
        # Validate URL
        if not re.match(URL_PATTERN, url):
            return jsonify({"error": "Invalid URL"}), 400
            
        webbrowser.open(url)
        return jsonify({"status": "success"})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == "__main__":
    # Create the static directory if it doesn't exist
    os.makedirs('static', exist_ok=True)
    
    # Copy the HTML file to the static directory (in a real app, you'd have a build process)
    # For now, assuming the HTML file is in the same directory as this script
    try:
        with open('frontend.html', 'r') as src, open('static/index.html', 'w') as dst:
            dst.write(src.read())
    except FileNotFoundError:
        print("Warning: frontend.html not found. Please place the HTML file in the static directory.")
    
    # Start the Flask server
    print(f"Starting server on http://localhost:5000")
    app.run(debug=True, host='0.0.0.0', port=5000)

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/core/mirror.py
========================================

import os


def mirror(url, response):
    if response != 'dummy':
        clean_url = url.replace('http://', '').replace('https://', '').rstrip('/')
        parts = clean_url.split('?')[0].split('/')
        root = parts[0]
        webpage = parts[-1]
        parts.remove(root)
        try:
            parts.remove(webpage)
        except ValueError:
            pass
        prefix = root + '_mirror'
        try:
            os.mkdir(prefix)
        except OSError:
            pass
        suffix = ''
        if parts:
            for directory in parts:
                suffix += directory + '/'
                try:
                    os.mkdir(prefix + '/' + suffix)
                except OSError:
                    pass
        path = prefix + '/' + suffix
        trail = ''
        if '.' not in webpage:
            trail += '.html'
        if webpage == root:
            name = 'index.html'
        else:
            name = webpage
        if len(url.split('?')) > 1:
            trail += '?' + url.split('?')[1]
        with open(path + name + trail, 'w+') as out_file:
            out_file.write(response.encode('utf-8'))

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/core/config.py
========================================

"""Configuration options for Photon."""

VERBOSE = False

INTELS = [
    'facebook.com',
    'github.com',
    'instagram.com',
    'youtube.com',
]

BAD_TYPES = (
    'bmp',
    'css',
    'csv',
    'docx',
    'ico',
    'jpeg',
    'jpg',
    'js',
    'json',
    'pdf',
    'png',
    'svg',
    'xls',
    'xml',
)

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/core/colors.py
========================================

import sys


if sys.platform.lower().startswith(('os', 'win', 'darwin', 'ios')):
    # Colors shouldn't be displayed on Mac and Windows
    end = red = white = green = yellow = run = bad = good = info = que = ''
else:
    white = '\033[97m'
    green = '\033[92m'
    red = '\033[91m'
    yellow = '\033[93m'
    end = '\033[0m'
    back = '\033[7;91m'
    info = '\033[93m[!]\033[0m'
    que = '\033[94m[?]\033[0m'
    bad = '\033[91m[-]\033[0m'
    good = '\033[92m[+]\033[0m'
    run = '\033[97m[~]\033[0m'

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/core/flash.py
========================================

from __future__ import print_function
import concurrent.futures

from core.colors import info

def flash(function, links, thread_count):
    """Process the URLs and uses a threadpool to execute a function."""
    # Convert links (set) to list
    links = list(links)
    threadpool = concurrent.futures.ThreadPoolExecutor(
            max_workers=thread_count)
    futures = (threadpool.submit(function, link) for link in links)
    for i, _ in enumerate(concurrent.futures.as_completed(futures)):
        if i + 1 == len(links) or (i + 1) % thread_count == 0:
            print('%s Progress: %i/%i' % (info, i + 1, len(links)),
                    end='\r')
    print('')

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/core/prompt.py
========================================

"""Support for an input prompt."""
import os
import tempfile


def prompt(default=None):
    """Present the user a prompt."""
    editor = 'nano'
    with tempfile.NamedTemporaryFile(mode='r+') as tmpfile:
        if default:
            tmpfile.write(default)
            tmpfile.flush()

        child_pid = os.fork()
        is_child = child_pid == 0

        if is_child:
            os.execvp(editor, [editor, tmpfile.name])
        else:
            os.waitpid(child_pid, 0)
            tmpfile.seek(0)
            return tmpfile.read().strip()

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/core/requester.py
========================================

import random
import time

import requests
from requests.exceptions import TooManyRedirects


SESSION = requests.Session()
SESSION.max_redirects = 3

def requester(
        url,
        main_url=None,
        delay=0,
        cook=None,
        headers=None,
        timeout=10,
        host=None,
        proxies=[None],
        user_agents=[None],
        failed=None,
        processed=None
    ):
    """Handle the requests and return the response body."""
    cook = cook or set()
    headers = headers or set()
    user_agents = user_agents or ['Photon']
    failed = failed or set()
    processed = processed or set()
    # Mark the URL as crawled
    processed.add(url)
    # Pause/sleep the program for specified time
    time.sleep(delay)

    def make_request(url):
        """Default request"""
        final_headers = headers or {
            'Host': host,
            # Selecting a random user-agent
            'User-Agent': random.choice(user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip',
            'DNT': '1',
            'Connection': 'close',
        }
        try:
            response = SESSION.get(
                url,
                cookies=cook,
                headers=final_headers,
                verify=False,
                timeout=timeout,
                stream=True,
                proxies=random.choice(proxies)
            )
        except TooManyRedirects:
            return 'dummy'

        if 'text/html' in response.headers['content-type'] or \
           'text/plain' in response.headers['content-type']:
            if response.status_code != '404':
                return response.text
            else:
                response.close()
                failed.add(url)
                return 'dummy'
        else:
            response.close()
            return 'dummy'

    return make_request(url)

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/core/updater.py
========================================

import os
import re

from core.colors import run, que, good, green, end, info
from core.requester import requester


def updater():
    """Update the current installation.

    git clones the latest version and merges it with the current directory.
    """
    print('%s Checking for updates' % run)
    # Changes must be separated by ;
    changes = '''major bug fixes;removed ninja mode;dropped python < 3.2 support;fixed unicode output;proxy support;more intels'''
    latest_commit = requester('https://raw.githubusercontent.com/s0md3v/Photon/master/core/updater.py', host='raw.githubusercontent.com')
    # Just a hack to see if a new version is available
    if changes not in latest_commit:
        changelog = re.search(r"changes = '''(.*?)'''", latest_commit)
        # Splitting the changes to form a list
        changelog = changelog.group(1).split(';')
        print('%s A new version of Photon is available.' % good)
        print('%s Changes:' % info)
        for change in changelog: # print changes
            print('%s>%s %s' % (green, end, change))

        current_path = os.getcwd().split('/') # if you know it, you know it
        folder = current_path[-1] # current directory name
        path = '/'.join(current_path) # current directory path
        choice = input('%s Would you like to update? [Y/n] ' % que).lower()

        if choice != 'n':
            print('%s Updating Photon' % run)
            os.system('git clone --quiet https://github.com/s0md3v/Photon %s'
                      % (folder))
            os.system('cp -r %s/%s/* %s && rm -r %s/%s/ 2>/dev/null'
                      % (path, folder, path, path, folder))
            print('%s Update successful!' % good)
    else:
        print('%s Photon is up to date!' % good)

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/core/zap.py
========================================

import re
import requests
import random

from core.utils import verb, xml_parser
from core.colors import run, good
from plugins.wayback import time_machine


def zap(input_url, archive, domain, host, internal, robots, proxies):
    """Extract links from robots.txt and sitemap.xml."""
    if archive:
        print('%s Fetching URLs from archive.org' % run)
        if False:
            archived_urls = time_machine(domain, 'domain')
        else:
            archived_urls = time_machine(host, 'host')
        print('%s Retrieved %i URLs from archive.org' % (
            good, len(archived_urls) - 1))
        for url in archived_urls:
            verb('Internal page', url)
            internal.add(url)
    # Makes request to robots.txt
    response = requests.get(input_url + '/robots.txt',
                            proxies=random.choice(proxies)).text
    # Making sure robots.txt isn't some fancy 404 page
    if '<body' not in response:
        # If you know it, you know it
        matches = re.findall(r'Allow: (.*)|Disallow: (.*)', response)
        if matches:
            # Iterating over the matches, match is a tuple here
            for match in matches:
                # One item in match will always be empty so will combine both
                # items
                match = ''.join(match)
                # If the URL doesn't use a wildcard
                if '*' not in match:
                    url = input_url + match
                    # Add the URL to internal list for crawling
                    internal.add(url)
                    # Add the URL to robots list
                    robots.add(url)
            print('%s URLs retrieved from robots.txt: %s' % (good, len(robots)))
    # Makes request to sitemap.xml
    response = requests.get(input_url + '/sitemap.xml',
                            proxies=random.choice(proxies)).text
    # Making sure robots.txt isn't some fancy 404 page
    if '<body' not in response:
        matches = xml_parser(response)
        if matches: # if there are any matches
            print('%s URLs retrieved from sitemap.xml: %s' % (
                good, len(matches)))
            for match in matches:
                verb('Internal page', match)
                # Cleaning up the URL and adding it to the internal list for
                # crawling
                internal.add(match)

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/core/__init__.py
========================================

"""The Photon core."""

========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/core/utils.py
========================================

import requests
import math
import os.path
import re
import argparse

import tld

from core.colors import info
from core.config import VERBOSE, BAD_TYPES

from urllib.parse import urlparse


def regxy(pattern, response, supress_regex, custom):
    """Extract a string based on regex pattern supplied by user."""
    try:
        matches = re.findall(r'%s' % pattern, response)
        for match in matches:
            verb('Custom regex', match)
            custom.add(match)
    except:
        supress_regex = True


def is_link(url, processed, files):
    """
    Determine whether or not a link should be crawled
    A url should not be crawled if it
        - Is a file
        - Has already been crawled

    Args:
        url: str Url to be processed
        processed: list[str] List of urls that have already been crawled

    Returns:
        bool If `url` should be crawled
    """
    if url not in processed:
        if url.startswith('#') or url.startswith('javascript:'):
            return False
        is_file = url.endswith(BAD_TYPES)
        if is_file:
            files.add(url)
            return False
        return True
    return False


def remove_regex(urls, regex):
    """
    Parse a list for non-matches to a regex.

    Args:
        urls: iterable of urls
        regex: string regex to be parsed for

    Returns:
        list of strings not matching regex
    """

    if not regex:
        return urls

    # To avoid iterating over the characters of a string
    if not isinstance(urls, (list, set, tuple)):
        urls = [urls]

    try:
        non_matching_urls = [url for url in urls if not re.search(regex, url)]
    except TypeError:
        return []

    return non_matching_urls


def writer(datasets, dataset_names, output_dir):
    """Write the results."""
    for dataset, dataset_name in zip(datasets, dataset_names):
        if dataset:
            filepath = output_dir + '/' + dataset_name + '.txt'
            with open(filepath, 'w+') as out_file:
                joined = '\n'.join(dataset)
                out_file.write(str(joined.encode('utf-8').decode('utf-8')))
                out_file.write('\n')


def timer(diff, processed):
    """Return the passed time."""
    # Changes seconds into minutes and seconds
    minutes, seconds = divmod(diff, 60)
    try:
        # Finds average time taken by requests
        time_per_request = diff / float(len(processed))
    except ZeroDivisionError:
        time_per_request = 0
    return minutes, seconds, time_per_request


def entropy(string):
    """Calculate the entropy of a string."""
    entropy = 0
    for number in range(256):
        result = float(string.encode('utf-8').count(
            chr(number))) / len(string.encode('utf-8'))
        if result != 0:
            entropy = entropy - result * math.log(result, 2)
    return entropy


def xml_parser(response):
    """Extract links from .xml files."""
    # Regex for extracting URLs
    return re.findall(r'<loc>(.*?)</loc>', response)


def verb(kind, string):
    """Enable verbose output."""
    if VERBOSE:
        print('%s %s: %s' % (info, kind, string))


def extract_headers(headers):
    """This function extracts valid headers from interactive input."""
    sorted_headers = {}
    matches = re.findall(r'(.*):\s(.*)', headers)
    for match in matches:
        header = match[0]
        value = match[1]
        try:
            if value[-1] == ',':
                value = value[:-1]
            sorted_headers[header] = value
        except IndexError:
            pass
    return sorted_headers


def top_level(url, fix_protocol=True):
    """Extract the top level domain from an URL."""
    ext = tld.get_tld(url, fix_protocol=fix_protocol)
    toplevel = '.'.join(urlparse(url).netloc.split('.')[-2:]).split(
        ext)[0] + ext
    return toplevel


def is_proxy_list(v, proxies):
    if os.path.isfile(v):
        with open(v, 'r') as _file:
            for line in _file:
                line = line.strip()
                if re.match(r"((http|socks5):\/\/.)?(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}):(\d{1,5})", line) or \
                   re.match(r"((http|socks5):\/\/.)?[-a-zA-Z0-9@:%._\+~#=]{2,256}\.[a-z]{2,6}:(\d{1,5})", line):
                    proxies.append({"http": line,
                                    "https": line})
                else:
                    print("%s ignored" % line)
        if proxies:
            return True
    return False


def proxy_type(v):
    """ Match IP:PORT or DOMAIN:PORT in a losse manner """
    proxies = []
    if re.match(r"((http|socks5):\/\/.)?(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}):(\d{1,5})", v):
        proxies.append({"http": v,
                        "https": v})
        return proxies
    elif re.match(r"((http|socks5):\/\/.)?[-a-zA-Z0-9@:%._\+~#=]{2,256}\.[a-z]{2,6}:(\d{1,5})", v):
        proxies.append({"http": v,
                        "https": v})
        return proxies
    elif is_proxy_list(v, proxies):
        return proxies
    else:
        raise argparse.ArgumentTypeError(
            "Proxy should follow IP:PORT or DOMAIN:PORT format")


def luhn(purported):

    # sum_of_digits (index * 2)
    LUHN_ODD_LOOKUP = (0, 2, 4, 6, 8, 1, 3, 5, 7, 9)

    if not isinstance(purported, str):
        purported = str(purported)
    try:
        evens = sum(int(p) for p in purported[-1::-2])
        odds = sum(LUHN_ODD_LOOKUP[int(p)] for p in purported[-2::-2])
        return (evens + odds) % 10 == 0
    except ValueError:  # Raised if an int conversion fails
        return False


def is_good_proxy(pip):
    try:
        requests.get('http://example.com', proxies=pip, timeout=3)
    except requests.exceptions.ConnectTimeout as e:
        return False
    except Exception as detail:
        return False

    return True


========================================
# File: /home/jarvis/Documents/GitHub/WhatNow-/Proton/core/regex.py
========================================

import re

# regex taken from https://github.com/InQuest/python-iocextract
# Reusable end punctuation regex.
END_PUNCTUATION = r"[\.\?>\"'\)!,}:;\u201d\u2019\uff1e\uff1c\]]*"

# Reusable regex for symbols commonly used to defang.
SEPARATOR_DEFANGS = r"[\(\)\[\]{}<>\\]"

# Split URLs on some characters that may be valid, but may also be garbage.
URL_SPLIT_STR = r"[>\"'\),};]"

# Get basic url format, including a few obfuscation techniques, main anchor is the uri scheme.
GENERIC_URL = re.compile(r"""
        (
            # Scheme.
            [fhstu]\S\S?[px]s?
            # One of these delimiters/defangs.
            (?:
                :\/\/|
                :\\\\|
                :?__
            )
            # Any number of defang characters.
            (?:
                \x20|
                """ + SEPARATOR_DEFANGS + r"""
            )*
            # Domain/path characters.
            \w
            \S+?
            # CISCO ESA style defangs followed by domain/path characters.
            (?:\x20[\/\.][^\.\/\s]\S*?)*
        )
    """ + END_PUNCTUATION + r"""
        (?=\s|$)
    """, re.IGNORECASE | re.VERBOSE | re.UNICODE)

# Get some obfuscated urls, main anchor is brackets around the period.
BRACKET_URL = re.compile(r"""
        \b
        (
            [\.\:\/\\\w\[\]\(\)-]+
            (?:
                \x20?
                [\(\[]
                \x20?
                \.
                \x20?
                [\]\)]
                \x20?
                \S*?
            )+
        )
    """ + END_PUNCTUATION + r"""
        (?=\s|$)
    """, re.VERBOSE | re.UNICODE)

# Get some obfuscated urls, main anchor is backslash before a period.
BACKSLASH_URL = re.compile(r"""
        \b
        (
            [\:\/\\\w\[\]\(\)-]+
            (?:
                \x20?
                \\?\.
                \x20?
                \S*?
            )*?
            (?:
                \x20?
                \\\.
                \x20?
                \S*?
            )
            (?:
                \x20?
                \\?\.
                \x20?
                \S*?
            )*
        )
    """ + END_PUNCTUATION + r"""
        (?=\s|$)
    """, re.VERBOSE | re.UNICODE)

# Get hex-encoded urls.
HEXENCODED_URL = re.compile(r"""
        (
            [46][86]
            (?:[57]4)?
            [57]4[57]0
            (?:[57]3)?
            3a2f2f
            (?:2[356def]|3[0-9adf]|[46][0-9a-f]|[57][0-9af])+
        )
        (?:[046]0|2[0-2489a-c]|3[bce]|[57][b-e]|[8-f][0-9a-f]|0a|0d|09|[
            \x5b-\x5d\x7b\x7d\x0a\x0d\x20
        ]|$)
    """, re.IGNORECASE | re.VERBOSE)

# Get urlencoded urls.
URLENCODED_URL = re.compile(r"""
        (s?[hf]t?tps?%3A%2F%2F\w[\w%-]*?)(?:[^\w%-]|$)
    """, re.IGNORECASE | re.VERBOSE)

# Get base64-encoded urls.
B64ENCODED_URL = re.compile(r"""
        (
            # b64re '([hH][tT][tT][pP][sS]|[hH][tT][tT][pP]|[fF][tT][pP])://'
            # Modified to ignore whitespace.
            (?:
                [\x2b\x2f-\x39A-Za-z]\s*[\x2b\x2f-\x39A-Za-z]\s*[\x31\x35\x39BFJNRVZdhlptx]\s*[Gm]\s*[Vd]\s*[FH]\s*[A]\s*\x36\s*L\s*y\s*[\x2b\x2f\x38-\x39]\s*|
                [\x2b\x2f-\x39A-Za-z]\s*[\x2b\x2f-\x39A-Za-z]\s*[\x31\x35\x39BFJNRVZdhlptx]\s*[Io]\s*[Vd]\s*[FH]\s*[R]\s*[Qw]\s*[O]\s*i\s*\x38\s*v\s*[\x2b\x2f-\x39A-Za-z]\s*|
                [\x2b\x2f-\x39A-Za-z]\s*[\x2b\x2f-\x39A-Za-z]\s*[\x31\x35\x39BFJNRVZdhlptx]\s*[Io]\s*[Vd]\s*[FH]\s*[R]\s*[Qw]\s*[Uc]\s*[z]\s*o\s*v\s*L\s*[\x2b\x2f-\x39w-z]\s*|
                [\x2b\x2f-\x39A-Za-z]\s*[\x30\x32EGUWkm]\s*[Z]\s*[\x30U]\s*[Uc]\s*[D]\s*o\s*v\s*L\s*[\x2b\x2f-\x39w-z]\s*|
                [\x2b\x2f-\x39A-Za-z]\s*[\x30\x32EGUWkm]\s*[h]\s*[\x30U]\s*[Vd]\s*[FH]\s*[A]\s*\x36\s*L\s*y\s*[\x2b\x2f\x38-\x39]\s*|
                [\x2b\x2f-\x39A-Za-z]\s*[\x30\x32EGUWkm]\s*[h]\s*[\x30U]\s*[Vd]\s*[FH]\s*[B]\s*[Tz]\s*[O]\s*i\s*\x38\s*v\s*[\x2b\x2f-\x39A-Za-z]\s*|
                [RZ]\s*[ln]\s*[R]\s*[Qw]\s*[O]\s*i\s*\x38\s*v\s*[\x2b\x2f-\x39A-Za-z]\s*|
                [Sa]\s*[FH]\s*[R]\s*[\x30U]\s*[Uc]\s*[D]\s*o\s*v\s*L\s*[\x2b\x2f-\x39w-z]\s*|
                [Sa]\s*[FH]\s*[R]\s*[\x30U]\s*[Uc]\s*[FH]\s*[M]\s*\x36\s*L\s*y\s*[\x2b\x2f\x38-\x39]\s*
            )
            # Up to 260 characters (pre-encoding, reasonable URL length).
            [A-Za-z0-9+/=\s]{1,357}
        )
        (?=[^A-Za-z0-9+/=\s]|$)
    """, re.VERBOSE)

# Get some valid obfuscated ip addresses.
IPV4 = re.compile(r"""
        (?:^|
            (?![^\d\.])
        )
        (?:
            (?:[1-9]?\d|1\d\d|2[0-4]\d|25[0-5])
            [\[\(\\]*?\.[\]\)]*?
        ){3}
        (?:[1-9]?\d|1\d\d|2[0-4]\d|25[0-5])
        (?:(?=[^\d\.])|$)
    """, re.VERBOSE)

# Experimental IPv6 regex, will not catch everything but should be sufficent for now.
IPV6 = re.compile(r"""
        \b(?:[a-f0-9]{1,4}:|:){2,7}(?:[a-f0-9]{1,4}|:)\b
    """, re.IGNORECASE | re.VERBOSE)

# Capture email addresses including common defangs.
EMAIL = re.compile(r"""
        (
            [a-z0-9_.+-]+
            [\(\[{\x20]*
            (?:@|\Wat\W)
            [\)\]}\x20]*
            [a-z0-9-]+
            (?:
                (?:
                    (?:
                        \x20*
                        """ + SEPARATOR_DEFANGS + r"""
                        \x20*
                    )*
                    \.
                    (?:
                        \x20*
                        """ + SEPARATOR_DEFANGS + r"""
                        \x20*
                    )*
                    |
                    \W+dot\W+
                )
                [a-z0-9-]+?
            )+
        )
    """ + END_PUNCTUATION + r"""
        (?=\s|$)
    """, re.IGNORECASE | re.VERBOSE | re.UNICODE)

MD5 = re.compile(r"(?:[^a-fA-F\d]|\b)([a-fA-F\d]{32})(?:[^a-fA-F\d]|\b)")
SHA1 = re.compile(r"(?:[^a-fA-F\d]|\b)([a-fA-F\d]{40})(?:[^a-fA-F\d]|\b)")
SHA256 = re.compile(r"(?:[^a-fA-F\d]|\b)([a-fA-F\d]{64})(?:[^a-fA-F\d]|\b)")
SHA512 = re.compile(
    r"(?:[^a-fA-F\d]|\b)([a-fA-F\d]{128})(?:[^a-fA-F\d]|\b)")

# YARA regex.
YARA_PARSE = re.compile(r"""
        (?:^|\s)
        (
            (?:
                \s*?import\s+?"[^\r\n]*?[\r\n]+|
                \s*?include\s+?"[^\r\n]*?[\r\n]+|
                \s*?//[^\r\n]*[\r\n]+|
                \s*?/\*.*?\*/\s*?
            )*
            (?:
                \s*?private\s+|
                \s*?global\s+
            )*
            rule\s*?
            \w+\s*?
            (?:
                :[\s\w]+
            )?
            \s+\{
            .*?
            condition\s*?:
            .*?
            \s*\}
        )
        (?:$|\s)
    """, re.MULTILINE | re.DOTALL | re.VERBOSE)

CREDIT_CARD = re.compile(r"[0-9]{4}[ ]?[-]?[0-9]{4}[ ]?[-]?[0-9]{4}[ ]?[-]?[0-9]{4}")

rintels = [(GENERIC_URL, "GENERIC_URL"),
           (BRACKET_URL, "BRACKET_URL"),
           (BACKSLASH_URL, "BACKSLASH_URL"),
           (HEXENCODED_URL, "HEXENCODED_URL"),
           (URLENCODED_URL, "URLENCODED_URL"),
           (B64ENCODED_URL, "B64ENCODED_URL"),
           (IPV4, "IPV4"),
           (IPV6, "IPV6"),
           (EMAIL, "EMAIL"),
           (MD5, "MD5"),
           (SHA1, "SHA1"),
           (SHA256, "SHA256"),
           (SHA512, "SHA512"),
           (YARA_PARSE, "YARA_PARSE"),
           (CREDIT_CARD, "CREDIT_CARD")]


rscript = re.compile(r'<(script|SCRIPT).*(src|SRC)=([^\s>]+)')
rhref = re.compile(r'<[aA].*(href|HREF)=([^\s>]+)')
rendpoint = re.compile(r'[\'"](/.*?)[\'"]|[\'"](http.*?)[\'"]')
rentropy = re.compile(r'[\w-]{16,45}')
