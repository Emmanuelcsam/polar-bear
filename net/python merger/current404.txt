
========================================
# File: /home/jarvis/Documents/GitHub/WhatNowAI/app.py
========================================

"""
WhatNowAI Flask Application

An intelligent activity recommendation system that helps users discover local events 
and activities based on their location, interests, and preferences. Features include:

- Multi-step onboarding with text-to-speech guidance
- Ticketmaster API integration for event discovery
- Interactive maps with event visualization
- Background research for personalized recommendations
"""
import logging.config
from flask import Flask

from routes import main_bp
from config.settings import FLASK_CONFIG, LOGGING_CONFIG, AUDIO_DIR, check_api_keys
from services.tts_service import TTSService

# Configure logging
logging.config.dictConfig(LOGGING_CONFIG)
logger = logging.getLogger(__name__)


def create_app() -> Flask:
    """
    Application factory function
    
    Returns:
        Configured Flask application instance
    """
    app = Flask(__name__)
    
    # Register blueprints
    app.register_blueprint(main_bp)
    
    # Initialize services
    tts_service = TTSService(str(AUDIO_DIR))
    
    # Cleanup old audio files on startup
    try:
        tts_service.cleanup_old_audio()
        logger.info("Audio cleanup completed")
    except Exception as e:
        logger.warning(f"Audio cleanup failed: {e}")
    
    logger.info("WhatNowAI application initialized successfully")
    return app


def main():
    """Main entry point"""
    # Check API keys on startup
    check_api_keys()
    
    app = create_app()
    
    logger.info(f"Starting WhatNowAI on {FLASK_CONFIG['HOST']}:{FLASK_CONFIG['PORT']}")
    app.run(
        debug=FLASK_CONFIG['DEBUG'],
        host=FLASK_CONFIG['HOST'],
        port=FLASK_CONFIG['PORT']
    )


if __name__ == '__main__':
    main()

========================================
# File: /home/jarvis/Documents/GitHub/WhatNowAI/routes.py
========================================

"""
Flask application routes for WhatNowAI

This module defines all the API endpoints for the WhatNowAI application, including:
- Onboarding flow with TTS integration
- Location services and geocoding
- Event discovery and mapping
- Background research and personalization
"""
from flask import Blueprint, render_template, request, jsonify, abort, send_file
import logging
from typing import Dict, Any

from services.tts_service import TTSService, get_introduction_text, INTRODUCTION_TEXTS
from services.geocoding_service import GeocodingService
from services.ticketmaster_service import TicketmasterService
from services.allevents_service import AllEventsService
from services.mapping_service import MappingService
from services.user_profiling_service import EnhancedUserProfilingService
from utils.helpers import validate_coordinates, generate_response_text
from config.settings import (AUDIO_DIR, DEFAULT_TTS_VOICE, TICKETMASTER_API_KEY, ALLEVENTS_API_KEY,
                           TICKETMASTER_CONFIG, ALLEVENTS_CONFIG, MAP_CONFIG)
from searchmethods.background_search import UserProfile, perform_background_search

logger = logging.getLogger(__name__)

# Create blueprint
main_bp = Blueprint('main', __name__)

# Initialize services
tts_service = TTSService(str(AUDIO_DIR), DEFAULT_TTS_VOICE)
geocoding_service = GeocodingService()
ticketmaster_service = TicketmasterService(TICKETMASTER_API_KEY, TICKETMASTER_CONFIG)
allevents_service = AllEventsService(ALLEVENTS_API_KEY, ALLEVENTS_CONFIG)
mapping_service = MappingService(MAP_CONFIG)
user_profiling_service = EnhancedUserProfilingService()


@main_bp.route('/')
def home():
    """Render the homepage with the form"""
    return render_template('home.html')


@main_bp.route('/tts/introduction/<step>', methods=['POST'])
def generate_introduction_tts(step: str):
    """Generate TTS for introduction steps"""
    try:
        # Get any location data from request for context
        data = request.get_json() if request.is_json else {}
        location_data = data.get('location')
        
        # Generate dynamic text based on time and location
        text = get_introduction_text(step, location_data)
        
        # Fallback to static text if dynamic generation fails
        if not text:
            text = INTRODUCTION_TEXTS.get(step)
            
        if not text:
            return jsonify({
                'success': False,
                'message': 'Invalid introduction step'
            }), 400
        
        audio_id, audio_path = tts_service.generate_audio_sync(text)
        
        if audio_id:
            return jsonify({
                'success': True,
                'audio_id': audio_id,
                'text': text
            })
        else:
            return jsonify({
                'success': False,
                'message': 'Failed to generate audio'
            }), 500
            
    except Exception as e:
        logger.error(f"Error generating introduction TTS: {e}")
        return jsonify({
            'success': False,
            'message': 'An error occurred while generating audio'
        }), 500


@main_bp.route('/submit', methods=['POST'])
def submit_info():
    """Handle form submission with user's name and activity"""
    try:
        data = request.get_json()
        name = data.get('name', '').strip()
        activity = data.get('activity', '').strip()
        social = data.get('social', {})
        
        if not name or not activity:
            return jsonify({
                'success': False,
                'message': 'Please provide both your name and what you want to do.'
            }), 400
        
        # Process the user input - start background processing
        response_message = f"Hello {name}! I'm processing your request to {activity}. Please wait while I work on this..."
        
        return jsonify({
            'success': True,
            'message': response_message,
            'name': name,
            'activity': activity,
            'social': social,
            'processing': True
        })
    
    except Exception as e:
        logger.error(f"Error in submit_info: {e}")
        return jsonify({
            'success': False,
            'message': 'An error occurred while processing your request.'
        }), 500


@main_bp.route('/chat', methods=['POST'])
def chat():
    """Handle chat messages"""
    try:
        data = request.get_json()
        message = data.get('message', '').strip()
        
        if not message:
            return jsonify({
                'success': False,
                'message': 'Please provide a message.'
            }), 400
        
        # Simple response logic (you can enhance this with AI)
        response = f"I received your message: '{message}'. How can I help you further?"
        
        return jsonify({
            'success': True,
            'response': response
        })
    
    except Exception as e:
        logger.error(f"Error in chat: {e}")
        return jsonify({
            'success': False,
            'message': 'An error occurred while processing your message.'
        }), 500


@main_bp.route('/process', methods=['POST'])
def process_request():
    """Handle background processing of user request"""
    try:
        data = request.get_json()
        name = data.get('name', '').strip()
        activity = data.get('activity', '').strip()
        location_data = data.get('location', {})
        social_data = data.get('social', {})
        
        if not name or not activity:
            return jsonify({
                'success': False,
                'message': 'Missing name or activity information.'
            }), 400
        
        # Perform background search
        logger.info(f"Starting background search for user: {name}")
        
        # Create user profile for search
        user_profile = UserProfile(
            name=name,
            location=location_data.get('city', '') + ', ' + location_data.get('country', ''),
            social_handles={
                'twitter': social_data.get('twitter', ''),
                'instagram': social_data.get('instagram', ''),
                'github': social_data.get('github', ''),
                'linkedin': social_data.get('linkedin', ''),
                'tiktok': social_data.get('tiktok', ''),
                'youtube': social_data.get('youtube', '')
            },
            activity=activity
        )
        
        # Perform background search (this may take some time)
        search_results = None
        search_summaries = None
        
        try:
            search_data = perform_background_search(user_profile)
            search_results = search_data.get('raw_results', {})
            search_summaries = search_data.get('summaries', {})
            logger.info(f"Background search completed. Found {search_data.get('total_results', 0)} total results")
        except Exception as search_error:
            logger.warning(f"Background search failed: {search_error}")
            search_summaries = {
                'general': 'Background search temporarily unavailable.',
                'social': 'Social media search temporarily unavailable.',
                'location': 'Location search temporarily unavailable.',
                'activity': 'Activity search temporarily unavailable.'
            }
        
        # Generate response text with search context
        result = generate_response_text(name, activity, location_data, social_data, search_summaries)
        
        # Create enhanced user profile using the new profiling service
        enhanced_user_profile = None
        try:
            enhanced_user_profile = user_profiling_service.create_enhanced_profile(
                name=name,
                location=location_data,
                activity=activity,
                social_data=social_data,
                search_results={
                    'search_results': search_results,
                    'search_summaries': search_summaries
                }
            )
            logger.info(f"Enhanced user profile created with {enhanced_user_profile.profile_completion:.1f}% completion")
            
            # Get recommendation context for events
            recommendation_context = user_profiling_service.get_recommendation_context(enhanced_user_profile)
            
        except Exception as profile_error:
            logger.warning(f"Enhanced user profiling failed: {profile_error}")
            recommendation_context = {}
        
        # Prepare personalization data for later use
        personalization_data = {
            'search_results': search_results,
            'search_summaries': search_summaries,
            'user_profile': {
                'name': name,
                'activity': activity,
                'location': location_data,
                'social': social_data
            },
            'enhanced_profile': recommendation_context,  # Include enhanced profile context
            'activity': activity  # Ensure activity is available at top level
        }
        
        return jsonify({
            'success': True,
            'result': result,
            'name': name,
            'activity': activity,
            'location': location_data,
            'social': social_data,
            'search_summaries': search_summaries,
            'personalization_data': personalization_data,  # Include personalization data
            'enhanced_profile_completion': enhanced_user_profile.profile_completion if enhanced_user_profile else 0,
            'total_search_results': len(search_results) if search_results else 0,
            'redirect_to_map': True,  # Signal frontend to redirect to map
            'map_url': '/map'
        })
    
    except Exception as e:
        logger.error(f"Error in process_request: {e}")
        return jsonify({
            'success': False,
            'message': 'An error occurred while processing your request.'
        }), 500


@main_bp.route('/geocode', methods=['POST'])
def reverse_geocode():
    """Reverse geocode latitude/longitude to get address information"""
    try:
        data = request.get_json()
        latitude = data.get('latitude')
        longitude = data.get('longitude')
        
        # Try to convert to float if they're strings
        try:
            if latitude is not None:
                latitude = float(latitude)
            if longitude is not None:
                longitude = float(longitude)
        except (ValueError, TypeError) as e:
            logger.error(f"Failed to convert coordinates to float in geocode: {e}")
            return jsonify({
                'success': False,
                'message': 'Invalid coordinate format. Coordinates must be numbers.'
            }), 400
        
        if not validate_coordinates(latitude, longitude):
            return jsonify({
                'success': False,
                'message': 'Invalid latitude or longitude coordinates.'
            }), 400
        
        location_info = geocoding_service.reverse_geocode(latitude, longitude)
        
        if location_info:
            return jsonify({
                'success': True,
                'location': location_info
            })
        else:
            return jsonify({
                'success': False,
                'message': 'Failed to geocode location.'
            }), 500
            
    except Exception as e:
        logger.error(f"Error in reverse_geocode: {e}")
        return jsonify({
            'success': False,
            'message': 'An error occurred while processing location.'
        }), 500


@main_bp.route('/audio/<audio_id>')
def serve_audio(audio_id: str):
    """Serve generated audio files"""
    try:
        if not tts_service.audio_exists(audio_id):
            abort(404)
        
        audio_path = tts_service.get_audio_path(audio_id)
        return send_file(audio_path, mimetype='audio/mpeg')
        
    except Exception as e:
        logger.error(f"Audio serve error: {e}")
        abort(500)


@main_bp.route('/map/events', methods=['POST'])
def get_map_events():
    """Get events and activities for map display"""
    try:
        data = request.get_json()
        location_data = data.get('location', {})
        user_interests = data.get('interests', [])
        user_activity = data.get('activity', '')
        personalization_data = data.get('personalization_data', {})  # Enhanced personalization data
        
        # Debug logging for incoming request
        logger.info(f"=== DEBUG: Incoming request data ===")
        logger.info(f"Location data: {location_data}")
        logger.info(f"User interests: {user_interests}")
        logger.info(f"User activity: '{user_activity}'")
        logger.info(f"Personalization data keys: {list(personalization_data.keys()) if personalization_data else 'None'}")
        logger.info(f"Full request data keys: {list(data.keys())}")
        
        # If no personalization data, try to construct basic context from available data
        if not personalization_data:
            logger.warning("No personalization_data in request, attempting to construct basic context")
            
            # Check if user data is available in the request directly
            user_name = data.get('name', '')
            user_social = data.get('social', {})
            
            if user_name or user_activity or user_social:
                logger.info(f"Found basic user data: name='{user_name}', activity='{user_activity}', social={bool(user_social)}")
                
                # Create minimal personalization context
                personalization_data = {
                    'user_profile': {
                        'name': user_name,
                        'activity': user_activity,
                        'location': location_data,
                        'social': user_social
                    },
                    'activity': user_activity,
                    'basic_context': True
                }
                logger.info("Created basic personalization context from request data")
            else:
                logger.warning("No user context data available in request")
        
        latitude = location_data.get('latitude')
        longitude = location_data.get('longitude')
        
        # Debug logging
        logger.info(f"Received location_data: {location_data}")
        logger.info(f"Raw coordinates - lat: {latitude} (type: {type(latitude)}), lon: {longitude} (type: {type(longitude)})")
        
        # Try to convert to float if they're strings
        try:
            if latitude is not None:
                latitude = float(latitude)
            if longitude is not None:
                longitude = float(longitude)
        except (ValueError, TypeError) as e:
            logger.error(f"Failed to convert coordinates to float: {e}")
            return jsonify({
                'success': False,
                'message': 'Invalid coordinate format. Coordinates must be numbers.'
            }), 400
        
        if not validate_coordinates(latitude, longitude):
            logger.error(f"Got invalid coordinates: {latitude}, {longitude}")
            return jsonify({
                'success': False,
                'message': 'Valid location is required. Please go back to onboarding and share your location to find events near you.'
            }), 400
        
        # Clear previous markers
        mapping_service.clear_markers()
        
        # Get events from Ticketmaster with enhanced profiling
        logger.info(f"Searching Ticketmaster events for location: {latitude}, {longitude}")
        logger.info(f"Received personalization_data keys: {list(personalization_data.keys()) if personalization_data else 'None'}")
        logger.info(f"User activity from request: '{user_activity}'")
        
        try:
            # Extract enhanced profile data if available
            enhanced_profile_data = personalization_data.get('enhanced_profile', {})
            logger.info(f"Enhanced profile data available: {bool(enhanced_profile_data)}")
            
            # Create a user profile object for the AI analysis
            user_profile_for_ai = None
            if enhanced_profile_data:
                user_profile_for_ai = enhanced_profile_data
                logger.info(f"Using enhanced profile with {len(enhanced_profile_data.get('interests', []))} interests")
            elif personalization_data.get('user_profile'):
                # Fallback to basic profile data
                basic_profile = personalization_data['user_profile']
                user_profile_for_ai = {
                    'name': basic_profile.get('name', ''),
                    'location': basic_profile.get('location', {}),
                    'primary_activity': basic_profile.get('activity', user_activity),  # Use current activity if not in profile
                    'interests': [],
                    'preferences': {'activity_style': 'balanced'},
                    'behavioral_patterns': {},
                    'activity_context': {'intent': 'seeking'},
                    'completion_score': 25  # Basic completion
                }
                logger.info(f"Using basic profile fallback for user: {basic_profile.get('name', 'Anonymous')}")
            elif user_activity:
                # Create minimal profile from just the activity
                user_profile_for_ai = {
                    'name': 'Anonymous',
                    'location': location_data,
                    'primary_activity': user_activity,
                    'interests': [],
                    'preferences': {'activity_style': 'balanced'},
                    'behavioral_patterns': {},
                    'activity_context': {'intent': 'seeking'},
                    'completion_score': 10  # Minimal completion
                }
                logger.info(f"Created minimal profile from activity: '{user_activity}'")
            else:
                logger.warning("No personalization data available - will use basic search only")
            
            ticketmaster_events = ticketmaster_service.search_events(
                location=location_data,
                user_interests=user_interests,
                user_activity=user_activity,
                personalization_data=personalization_data,
                user_profile=user_profile_for_ai  # Pass enhanced profile to AI ranking
            )
            
            if ticketmaster_events:
                mapping_service.add_ticketmaster_events(ticketmaster_events)
                logger.info(f"Added {len(ticketmaster_events)} Ticketmaster events to map")
            else:
                logger.info("No Ticketmaster events found")
                
        except Exception as tm_error:
            logger.warning(f"Ticketmaster search failed: {tm_error}")
        
        # Get events from AllEvents with enhanced profiling
        logger.info(f"Searching AllEvents events for location: {latitude}, {longitude}")
        
        try:
            allevents_events = allevents_service.search_events(
                location=location_data,
                user_interests=user_interests,
                user_activity=user_activity,
                personalization_data=personalization_data,
                user_profile=user_profile_for_ai  # Pass enhanced profile to AI ranking
            )
            
            if allevents_events:
                mapping_service.add_allevents_events(allevents_events)
                logger.info(f"Added {len(allevents_events)} AllEvents events to map")
            else:
                logger.info("No AllEvents events found")
                
        except Exception as ae_error:
            logger.warning(f"AllEvents search failed: {ae_error}")
        
        # TODO: Add other API integrations here
        # mapping_service.add_eventbrite_events(eventbrite_events)
        # mapping_service.add_meetup_events(meetup_events)
        
        # Get map data
        map_data = mapping_service.get_map_data(latitude, longitude)
        category_stats = mapping_service.get_category_stats()
        
        return jsonify({
            'success': True,
            'map_data': map_data,
            'category_stats': category_stats,
            'total_events': len(mapping_service.get_all_markers())
        })
        
    except Exception as e:
        logger.error(f"Error getting map events: {e}")
        return jsonify({
            'success': False,
            'message': 'An error occurred while loading events.'
        }), 500


@main_bp.route('/map/search', methods=['POST'])
def search_map_events():
    """Search events on the map"""
    try:
        data = request.get_json()
        query = data.get('query', '').strip()
        
        if not query:
            return jsonify({
                'success': False,
                'message': 'Please provide a search query.'
            }), 400
        
        # Search markers
        matching_markers = mapping_service.search_markers(query)
        
        return jsonify({
            'success': True,
            'markers': [marker.to_dict() for marker in matching_markers],
            'total_results': len(matching_markers)
        })
        
    except Exception as e:
        logger.error(f"Error searching map events: {e}")
        return jsonify({
            'success': False,
            'message': 'An error occurred while searching events.'
        }), 500


@main_bp.route('/map')
def map_view():
    """Render the map page"""
    # The map page will get user data from sessionStorage via JavaScript
    # We provide empty defaults that will be overridden by the frontend
    return render_template('map.html', 
                         name='', 
                         activity='', 
                         location={}, 
                         social={})

========================================
# File: /home/jarvis/Documents/GitHub/WhatNowAI/Potential Upgrades/integration_utilities.py
========================================

# searchmethods/utils/cache_manager.py
"""
Caching utilities for search results
"""
import json
import hashlib
import os
from datetime import datetime, timedelta
from typing import Dict, Any, Optional
import redis
import pickle
import logging

logger = logging.getLogger(__name__)


class CacheManager:
    """Manage caching of search results"""
    
    def __init__(self, cache_type: str = "file", config: Dict[str, Any] = None):
        """
        Initialize cache manager
        
        Args:
            cache_type: "file", "redis", or "memory"
            config: Cache configuration
        """
        self.cache_type = cache_type
        self.config = config or {}
        self.cache_dir = self.config.get('cache_dir', 'cache/search_results')
        self.ttl = self.config.get('ttl_hours', 24)
        
        if cache_type == "file":
            os.makedirs(self.cache_dir, exist_ok=True)
            self.cache = {}
        elif cache_type == "redis":
            self.redis_client = redis.Redis(
                host=self.config.get('redis_host', 'localhost'),
                port=self.config.get('redis_port', 6379),
                db=self.config.get('redis_db', 0)
            )
        elif cache_type == "memory":
            self.cache = {}
    
    def _generate_key(self, user_profile: Dict[str, Any]) -> str:
        """Generate cache key from user profile"""
        key_data = {
            'name': user_profile.get('full_name', ''),
            'location': user_profile.get('location', {}).get('city', ''),
            'activity': user_profile.get('activity', '')
        }
        
        key_str = json.dumps(key_data, sort_keys=True)
        return hashlib.md5(key_str.encode()).hexdigest()
    
    def get(self, user_profile: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Get cached results"""
        key = self._generate_key(user_profile)
        
        try:
            if self.cache_type == "file":
                cache_file = os.path.join(self.cache_dir, f"{key}.json")
                if os.path.exists(cache_file):
                    # Check age
                    file_age = datetime.now() - datetime.fromtimestamp(os.path.getmtime(cache_file))
                    if file_age < timedelta(hours=self.ttl):
                        with open(cache_file, 'r') as f:
                            return json.load(f)
                            
            elif self.cache_type == "redis":
                data = self.redis_client.get(key)
                if data:
                    return pickle.loads(data)
                    
            elif self.cache_type == "memory":
                if key in self.cache:
                    data, timestamp = self.cache[key]
                    if datetime.now() - timestamp < timedelta(hours=self.ttl):
                        return data
                        
        except Exception as e:
            logger.error(f"Cache get error: {e}")
            
        return None
    
    def set(self, user_profile: Dict[str, Any], results: Dict[str, Any]):
        """Set cache results"""
        key = self._generate_key(user_profile)
        
        try:
            if self.cache_type == "file":
                cache_file = os.path.join(self.cache_dir, f"{key}.json")
                with open(cache_file, 'w') as f:
                    json.dump(results, f, indent=2, default=str)
                    
            elif self.cache_type == "redis":
                self.redis_client.setex(
                    key,
                    timedelta(hours=self.ttl),
                    pickle.dumps(results)
                )
                
            elif self.cache_type == "memory":
                self.cache[key] = (results, datetime.now())
                
        except Exception as e:
            logger.error(f"Cache set error: {e}")
    
    def clear(self):
        """Clear all cache"""
        try:
            if self.cache_type == "file":
                for file in os.listdir(self.cache_dir):
                    if file.endswith('.json'):
                        os.remove(os.path.join(self.cache_dir, file))
                        
            elif self.cache_type == "redis":
                self.redis_client.flushdb()
                
            elif self.cache_type == "memory":
                self.cache.clear()
                
        except Exception as e:
            logger.error(f"Cache clear error: {e}")


# searchmethods/utils/privacy_filter.py
"""
Privacy and content filtering utilities
"""
import re
from typing import List, Dict, Any
import logging

logger = logging.getLogger(__name__)


class PrivacyFilter:
    """Filter sensitive information from search results"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.sensitive_patterns = self._load_sensitive_patterns()
    
    def _load_sensitive_patterns(self) -> Dict[str, re.Pattern]:
        """Load patterns for sensitive information"""
        return {
            'ssn': re.compile(r'\b\d{3}-\d{2}-\d{4}\b'),
            'credit_card': re.compile(r'\b\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}\b'),
            'phone': re.compile(r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b'),
            'email': re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'),
            'address': re.compile(r'\d+\s+[A-Za-z\s]+(?:Street|St|Avenue|Ave|Road|Rd|Boulevard|Blvd|Lane|Ln|Drive|Dr|Court|Ct|Circle|Cir|Plaza|Pl)\b', re.IGNORECASE),
            'date_of_birth': re.compile(r'\b(?:DOB|Date of Birth|Born)[\s:]+\d{1,2}[-/]\d{1,2}[-/]\d{2,4}\b', re.IGNORECASE)
        }
    
    def filter_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Filter sensitive information from results"""
        filtered = []
        
        for result in results:
            # Create a copy to avoid modifying original
            filtered_result = result.copy()
            
            # Filter content
            if 'content' in filtered_result:
                filtered_result['content'] = self._filter_text(filtered_result['content'])
            
            # Filter title
            if 'title' in filtered_result:
                filtered_result['title'] = self._filter_text(filtered_result['title'])
            
            filtered.append(filtered_result)
        
        return filtered
    
    def _filter_text(self, text: str) -> str:
        """Filter sensitive information from text"""
        if not text:
            return text
        
        filtered_text = text
        
        for pattern_name, pattern in self.sensitive_patterns.items():
            if pattern_name == 'email' and self.config.get('preserve_emails', False):
                # Partially mask emails
                filtered_text = pattern.sub(self._mask_email, filtered_text)
            else:
                # Replace with generic marker
                filtered_text = pattern.sub(f'[{pattern_name.upper()}_REDACTED]', filtered_text)
        
        return filtered_text
    
    def _mask_email(self, match):
        """Partially mask email addresses"""
        email = match.group(0)
        parts = email.split('@')
        if len(parts) == 2:
            username = parts[0]
            if len(username) > 3:
                masked = username[:2] + '*' * (len(username) - 3) + username[-1]
                return f"{masked}@{parts[1]}"
        return '[EMAIL_REDACTED]'


# searchmethods/utils/rate_monitor.py
"""
API rate limit monitoring and management
"""
import time
from collections import defaultdict
from typing import Dict, Any
import logging

logger = logging.getLogger(__name__)


class RateMonitor:
    """Monitor and manage API rate limits"""
    
    def __init__(self):
        self.api_calls = defaultdict(list)
        self.rate_limits = {
            'google': {'calls': 100, 'window': 3600},  # 100 calls per hour
            'github': {'calls': 60, 'window': 3600},   # 60 calls per hour
            'eventbrite': {'calls': 1000, 'window': 3600},  # 1000 calls per hour
            'openai': {'calls': 60, 'window': 60},     # 60 calls per minute
        }
    
    def check_rate_limit(self, api_name: str) -> bool:
        """Check if API call is within rate limit"""
        if api_name not in self.rate_limits:
            return True
        
        current_time = time.time()
        window = self.rate_limits[api_name]['window']
        max_calls = self.rate_limits[api_name]['calls']
        
        # Remove old calls outside window
        self.api_calls[api_name] = [
            call_time for call_time in self.api_calls[api_name]
            if current_time - call_time < window
        ]
        
        # Check if under limit
        return len(self.api_calls[api_name]) < max_calls
    
    def record_call(self, api_name: str):
        """Record an API call"""
        self.api_calls[api_name].append(time.time())
    
    def get_wait_time(self, api_name: str) -> float:
        """Get time to wait before next call"""
        if api_name not in self.rate_limits:
            return 0
        
        if self.check_rate_limit(api_name):
            return 0
        
        current_time = time.time()
        window = self.rate_limits[api_name]['window']
        
        # Find oldest call in window
        oldest_call = min(self.api_calls[api_name])
        wait_time = window - (current_time - oldest_call)
        
        return max(0, wait_time)
    
    def get_usage_stats(self) -> Dict[str, Any]:
        """Get current usage statistics"""
        stats = {}
        current_time = time.time()
        
        for api_name, limits in self.rate_limits.items():
            window = limits['window']
            max_calls = limits['calls']
            
            # Count recent calls
            recent_calls = [
                call_time for call_time in self.api_calls[api_name]
                if current_time - call_time < window
            ]
            
            stats[api_name] = {
                'used': len(recent_calls),
                'limit': max_calls,
                'percentage': (len(recent_calls) / max_calls * 100) if max_calls > 0 else 0,
                'window_seconds': window
            }
        
        return stats


# Complete integration guide
COMPLETE_INTEGRATION_GUIDE = """
# WhatNowAI Enhanced Search Integration Guide

## Overview
This guide provides complete instructions for integrating the enhanced search system into your WhatNowAI application.

## Directory Structure
```
whatnowai/
├── searchmethods/
│   ├── core/
│   │   ├── __init__.py
│   │   └── data_processor.py
│   ├── scrapers/
│   │   ├── __init__.py
│   │   └── web_scrapers.py
│   ├── ai/
│   │   ├── __init__.py
│   │   └── ai_processor.py
│   ├── utils/
│   │   ├── __init__.py
│   │   ├── cache_manager.py
│   │   ├── privacy_filter.py
│   │   └── rate_monitor.py
│   ├── __init__.py
│   ├── background_search.py (original)
│   └── enhanced_search.py (new)
├── services/
│   ├── eventbrite_service.py (new)
│   └── ... (existing services)
├── config/
│   └── settings.py (update)
└── requirements.txt (update)
```

## Step-by-Step Integration

### 1. Update requirements.txt
```txt
# Core dependencies
aiohttp>=3.8.0
beautifulsoup4>=4.11.0
requests>=2.28.0
python-dateutil>=2.8.0

# Web scraping
selenium>=4.0.0
cloudscraper>=1.2.0
fake-useragent>=1.4.0

# NLP and ML
nltk>=3.8.0
scikit-learn>=1.3.0
numpy>=1.24.0

# AI (optional)
openai>=1.0.0
transformers>=4.35.0
sentence-transformers>=2.2.0
torch>=2.0.0

# Caching (optional)
redis>=4.5.0

# Utilities
python-dotenv>=1.0.0
```

### 2. Update config/settings.py
```python
import os
from dotenv import load_dotenv

load_dotenv()

# API Keys
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
EVENTBRITE_API_KEY = os.getenv('EVENTBRITE_API_KEY')
HUGGINGFACE_API_KEY = os.getenv('HUGGINGFACE_API_KEY')

# Enhanced Search Configuration
SEARCH_CONFIG = {
    'scrapers': {
        'use_selenium': False,
        'rate_limit': 1,
        'timeout': 30,
        'max_retries': 3
    },
    'orchestrator': {
        'max_workers': 10,
        'timeout': 60
    },
    'interest_extraction': {
        'min_interest_score': 0.3,
        'max_keywords_per_interest': 10
    },
    'data_filtering': {
        'min_relevance_score': 0.5,
        'preserve_emails': False
    },
    'cache': {
        'type': 'file',  # 'file', 'redis', or 'memory'
        'ttl_hours': 24,
        'cache_dir': 'cache/search_results'
    }
}

# AI Configuration
AI_CONFIG = {
    'openai_model': 'gpt-3.5-turbo',
    'use_huggingface': True,
    'hf_model': 'facebook/bart-large-mnli',
    'use_local_embeddings': True,
    'embedding_model': 'all-MiniLM-L6-v2'
}

# Eventbrite Configuration
EVENTBRITE_CONFIG = {
    'MAX_EVENTS': 30,
    'BASE_URL': 'https://www.eventbriteapi.com/v3',
    'TIMEOUT': 10,
    'DEFAULT_RADIUS_MILES': 50,
    'DEFAULT_TIME_RANGE_HOURS': 12
}
```

### 3. Update routes.py
```python
from searchmethods.enhanced_search import perform_enhanced_background_search
from searchmethods.utils.cache_manager import CacheManager
from searchmethods.utils.privacy_filter import PrivacyFilter

# Initialize utilities
cache_manager = CacheManager(
    cache_type=app.config.get('SEARCH_CONFIG', {}).get('cache', {}).get('type', 'file')
)
privacy_filter = PrivacyFilter()

@main_bp.route('/process', methods=['POST'])
def process_request():
    try:
        data = request.get_json()
        
        # Check cache first
        cached_result = cache_manager.get(data)
        if cached_result:
            logger.info("Returning cached results")
            return jsonify(cached_result)
        
        # Prepare user data
        user_data = {
            'name': f"{data.get('name', '')}",
            'location': data.get('location', {}),
            'latitude': data['location'].get('latitude'),
            'longitude': data['location'].get('longitude'),
            'activity': data.get('activity', ''),
            'social_handles': data.get('social', {})
        }
        
        # Run enhanced search
        import asyncio
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        try:
            search_result = loop.run_until_complete(
                perform_enhanced_background_search(user_data)
            )
        finally:
            loop.close()
        
        # Filter sensitive information
        if search_result.get('search_results'):
            for source, results in search_result['search_results'].items():
                if 'top_results' in results:
                    results['top_results'] = privacy_filter.filter_results(
                        results['top_results']
                    )
        
        # Cache results
        cache_manager.set(data, search_result)
        
        # Prepare response
        response = {
            'success': search_result.get('success', False),
            'name': data.get('name'),
            'activity': data.get('activity'),
            'location': data.get('location'),
            'interests': search_result.get('interests', []),
            'events': search_result.get('events', []),
            'search_summaries': {
                'general': f"Found {search_result['metadata']['total_results_found']} results",
                'interests': f"Discovered {len(search_result.get('interests', []))} interests",
                'events': f"Found {len(search_result.get('events', []))} nearby events"
            },
            'redirect_to_map': True,
            'map_url': '/map'
        }
        
        return jsonify(response)
        
    except Exception as e:
        logger.error(f"Error in process_request: {e}")
        return jsonify({
            'success': False,
            'message': 'An error occurred while processing your request.'
        }), 500
```

### 4. Update map integration
```python
# In services/mapping_service.py, add:

def add_eventbrite_events(self, events: List[Dict[str, Any]]):
    """Add Eventbrite events to the map"""
    for event in events:
        try:
            marker = MapMarker(
                id=f"eb_{event['id']}",
                name=event['name'],
                latitude=event['latitude'],
                longitude=event['longitude'],
                category=event['category'],
                subcategory=event.get('subcategory', ''),
                description=event.get('description', ''),
                url=event['url'],
                date=event['start_time'].split('T')[0],
                time=event['start_time'].split('T')[1][:5],
                venue=event['venue_name'],
                address=event['venue_address'],
                price_min=event.get('price_min'),
                price_max=event.get('price_max'),
                image_url=event.get('image_url', ''),
                source="eventbrite"
            )
            self.markers.append(marker)
        except Exception as e:
            logger.warning(f"Failed to add Eventbrite event: {e}")
```

### 5. Create monitoring dashboard
```python
# monitoring/dashboard.py
from flask import Blueprint, render_template, jsonify
from searchmethods.utils.rate_monitor import RateMonitor

monitoring_bp = Blueprint('monitoring', __name__)
rate_monitor = RateMonitor()

@monitoring_bp.route('/monitoring/dashboard')
def dashboard():
    return render_template('monitoring/dashboard.html')

@monitoring_bp.route('/monitoring/api/stats')
def api_stats():
    stats = rate_monitor.get_usage_stats()
    return jsonify(stats)
```

### 6. Privacy and compliance
```python
# Add to your terms of service and privacy policy:
- Data collection disclosure
- Third-party API usage
- Data retention policies
- User consent mechanisms
```

### 7. Performance optimizations
```python
# Add to config:
PERFORMANCE_CONFIG = {
    'enable_caching': True,
    'cache_ttl_hours': 24,
    'max_concurrent_searches': 10,
    'request_timeout': 30,
    'enable_result_pagination': True,
    'results_per_page': 20
}
```

### 8. Error handling and logging
```python
# Update logging configuration
LOGGING_CONFIG = {
    'version': 1,
    'formatters': {
        'detailed': {
            'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        }
    },
    'handlers': {
        'file': {
            'class': 'logging.handlers.RotatingFileHandler',
            'filename': 'logs/enhanced_search.log',
            'maxBytes': 10485760,  # 10MB
            'backupCount': 5,
            'formatter': 'detailed'
        }
    },
    'loggers': {
        'searchmethods': {
            'handlers': ['file'],
            'level': 'INFO'
        }
    }
}
```

## Testing

### Unit tests
```python
# tests/test_enhanced_search.py
import pytest
import asyncio
from searchmethods.enhanced_search import EnhancedSearchSystem

@pytest.mark.asyncio
async def test_search_system():
    system = EnhancedSearchSystem()
    
    result = await system.perform_enhanced_search(
        first_name="Test",
        last_name="User",
        location={'city': 'San Francisco', 'latitude': 37.7749, 'longitude': -122.4194},
        activity="test activity"
    )
    
    assert result['success'] == True
    assert 'interests' in result
    assert 'events' in result
    
    await system.cleanup()
```

## Deployment

### Environment setup
```bash
# .env file
FLASK_ENV=production
OPENAI_API_KEY=your-key
EVENTBRITE_API_KEY=your-key
REDIS_URL=redis://localhost:6379/0
```

### Docker support
```dockerfile
# Dockerfile additions
RUN apt-get update && apt-get install -y \
    chromium-driver \
    && rm -rf /var/lib/apt/lists/*

ENV CHROMEDRIVER_PATH=/usr/bin/chromedriver
```

## Monitoring and maintenance

1. Monitor API usage through dashboard
2. Review logs for errors
3. Update search patterns based on results
4. Refresh API keys as needed
5. Clear cache periodically

## Support

For issues or questions:
1. Check logs in `logs/` directory
2. Review rate limit status
3. Verify API keys are valid
4. Test individual components
"""

# Save complete guide
with open('COMPLETE_INTEGRATION_GUIDE.md', 'w') as f:
    f.write(COMPLETE_INTEGRATION_GUIDE)
    
print("Complete integration guide saved to COMPLETE_INTEGRATION_GUIDE.md")

========================================
# File: /home/jarvis/Documents/GitHub/WhatNowAI/Potential Upgrades/main_integration_module.py
========================================

# searchmethods/enhanced_search.py
"""
Enhanced search integration module that combines all search capabilities
"""
import asyncio
import logging
from typing import Dict, List, Any, Optional
from datetime import datetime
import os
from concurrent.futures import ThreadPoolExecutor
import json

from .core import (
    EnhancedUserProfile, SearchOrchestrator, DataSource,
    SearchResult, UserInterest
)
from .core.data_processor import InterestExtractor, DataFilter
from .scrapers.web_scrapers import (
    GoogleSearchScraper, LinkedInScraper, TwitterScraper,
    InstagramScraper, GitHubScraper, RedditScraper,
    NewsArchiveScraper
)
from .ai.ai_processor import AIProcessor, AIConfig
from ..services.eventbrite_service import EventbriteService
from ..services.mapping_service import MappingService
from ..config.settings import (
    SEARCH_CONFIG, AI_CONFIG, EVENTBRITE_CONFIG,
    EVENTBRITE_API_KEY, OPENAI_API_KEY
)

logger = logging.getLogger(__name__)


class EnhancedSearchSystem:
    """Main search system that orchestrates all components"""
    
    def __init__(self, config: Dict[str, Any] = None):
        """Initialize the enhanced search system"""
        self.config = config or SEARCH_CONFIG
        self._initialize_components()
        
    def _initialize_components(self):
        """Initialize all search components"""
        logger.info("Initializing Enhanced Search System...")
        
        # Initialize scrapers
        scraper_config = self.config.get('scrapers', {})
        self.scrapers = [
            GoogleSearchScraper(scraper_config),
            LinkedInScraper(scraper_config),
            TwitterScraper(scraper_config),
            InstagramScraper(scraper_config),
            GitHubScraper(scraper_config),
            RedditScraper(scraper_config),
            NewsArchiveScraper(scraper_config)
        ]
        
        # Initialize search orchestrator
        self.orchestrator = SearchOrchestrator(
            modules=self.scrapers,
            config=self.config.get('orchestrator', {})
        )
        
        # Initialize data processors
        self.interest_extractor = InterestExtractor(
            config=self.config.get('interest_extraction', {})
        )
        self.data_filter = DataFilter(
            config=self.config.get('data_filtering', {})
        )
        
        # Initialize AI processor (optional)
        ai_config = AIConfig(
            use_openai=bool(OPENAI_API_KEY),
            openai_api_key=OPENAI_API_KEY,
            **AI_CONFIG
        )
        self.ai_processor = AIProcessor(ai_config) if ai_config.use_openai or ai_config.use_huggingface else None
        
        # Initialize Eventbrite service
        self.eventbrite_service = EventbriteService(
            api_key=EVENTBRITE_API_KEY,
            config=EVENTBRITE_CONFIG
        ) if EVENTBRITE_API_KEY else None
        
        logger.info("Enhanced Search System initialized successfully")
    
    async def perform_enhanced_search(
        self,
        first_name: str,
        last_name: str,
        location: Dict[str, Any],
        activity: str,
        social_handles: Dict[str, str] = None
    ) -> Dict[str, Any]:
        """
        Perform comprehensive search and analysis
        
        Args:
            first_name: User's first name
            last_name: User's last name
            location: Location data with lat/lon and city
            activity: What the user wants to do
            social_handles: Optional social media handles
            
        Returns:
            Dictionary with search results, interests, and events
        """
        start_time = datetime.now()
        
        # Create user profile
        user_profile = EnhancedUserProfile(
            first_name=first_name,
            last_name=last_name,
            full_name=f"{first_name} {last_name}",
            location=location,
            activity=activity,
            social_profiles=social_handles or {}
        )
        
        logger.info(f"Starting enhanced search for {user_profile.full_name}")
        
        try:
            # Step 1: Perform web searches
            search_results = await self._perform_searches(user_profile)
            
            # Step 2: Filter and validate results
            filtered_results = await self._filter_results(search_results, user_profile)
            
            # Step 3: Extract interests
            interests = await self._extract_interests(filtered_results, user_profile)
            
            # Step 4: Enhance with AI if available
            if self.ai_processor:
                user_profile = await self.ai_processor.enhance_profile_with_ai(
                    user_profile, filtered_results
                )
            
            # Step 5: Find events based on interests
            events = await self._find_events(user_profile)
            
            # Calculate timing
            elapsed_time = (datetime.now() - start_time).total_seconds()
            
            # Prepare response
            response = {
                'success': True,
                'user_profile': user_profile.to_dict(),
                'search_results': self._summarize_results(filtered_results),
                'interests': [interest.to_dict() for interest in user_profile.get_top_interests(10)],
                'events': events,
                'metadata': {
                    'search_time_seconds': elapsed_time,
                    'total_results_found': user_profile.total_results_found,
                    'data_sources_used': [ds.value for ds in user_profile.data_sources_used],
                    'ai_enhanced': self.ai_processor is not None,
                    'eventbrite_enabled': self.eventbrite_service is not None
                }
            }
            
            logger.info(f"Enhanced search completed in {elapsed_time:.2f} seconds")
            return response
            
        except Exception as e:
            logger.error(f"Enhanced search failed: {e}")
            return {
                'success': False,
                'error': str(e),
                'user_profile': user_profile.to_dict(),
                'search_results': {},
                'interests': [],
                'events': []
            }
    
    async def _perform_searches(
        self, 
        user_profile: EnhancedUserProfile
    ) -> Dict[DataSource, List[SearchResult]]:
        """Perform all searches in parallel"""
        logger.info("Starting parallel searches...")
        
        # Use orchestrator to run all searches
        results = await self.orchestrator.search_all(user_profile)
        
        logger.info(f"Search complete. Found results from {len(results)} sources")
        return results
    
    async def _filter_results(
        self,
        search_results: Dict[DataSource, List[SearchResult]],
        user_profile: EnhancedUserProfile
    ) -> Dict[DataSource, List[SearchResult]]:
        """Filter and validate search results"""
        logger.info("Filtering search results...")
        
        filtered = {}
        
        for source, results in search_results.items():
            # Apply data filter
            filtered_source_results = self.data_filter.filter_results(results, user_profile)
            
            if filtered_source_results:
                filtered[source] = filtered_source_results
                logger.info(f"{source.value}: {len(filtered_source_results)} relevant results")
        
        return filtered
    
    async def _extract_interests(
        self,
        search_results: Dict[DataSource, List[SearchResult]],
        user_profile: EnhancedUserProfile
    ) -> List[UserInterest]:
        """Extract user interests from search results"""
        logger.info("Extracting user interests...")
        
        all_results = []
        for results in search_results.values():
            all_results.extend(results)
        
        # Use traditional extraction
        interests = self.interest_extractor.extract_interests(all_results)
        
        # Add interests to profile
        for interest in interests:
            user_profile.add_interest(interest)
        
        # Enhance with AI if available
        if self.ai_processor:
            ai_interests = await self.ai_processor.extract_interests_with_ai(
                all_results, user_profile
            )
            for interest in ai_interests:
                user_profile.add_interest(interest)
        
        logger.info(f"Extracted {len(user_profile.interests)} total interests")
        return user_profile.interests
    
    async def _find_events(self, user_profile: EnhancedUserProfile) -> List[Dict[str, Any]]:
        """Find events based on user profile and interests"""
        logger.info("Finding events based on interests...")
        
        events = []
        
        if not self.eventbrite_service:
            logger.warning("Eventbrite service not available")
            return events
        
        # Get top interest keywords
        interest_keywords = []
        for interest in user_profile.get_top_interests(5):
            interest_keywords.extend(interest.keywords[:2])
        
        # Add activity keywords
        if user_profile.activity:
            interest_keywords.extend(user_profile.activity.split()[:3])
        
        # Remove duplicates
        interest_keywords = list(set(interest_keywords))
        
        # Search for events
        eventbrite_events = self.eventbrite_service.search_events(
            location=user_profile.location,
            interests=interest_keywords,
            radius_miles=50,
            time_range_hours=12
        )
        
        # Convert to dict format
        for event in eventbrite_events:
            events.append(event.to_dict())
        
        logger.info(f"Found {len(events)} relevant events")
        return events
    
    def _summarize_results(
        self, 
        search_results: Dict[DataSource, List[SearchResult]]
    ) -> Dict[str, Any]:
        """Create summary of search results"""
        summary = {}
        
        for source, results in search_results.items():
            source_name = source.value
            
            # Get top results
            top_results = []
            for result in results[:3]:  # Top 3 per source
                top_results.append({
                    'title': result.title,
                    'url': result.url,
                    'relevance': result.relevance_score,
                    'snippet': result.content[:200] + '...' if len(result.content) > 200 else result.content
                })
            
            summary[source_name] = {
                'total_results': len(results),
                'top_results': top_results
            }
        
        return summary
    
    async def cleanup(self):
        """Cleanup resources"""
        logger.info("Cleaning up search system resources...")
        
        # Cleanup scrapers
        for scraper in self.scrapers:
            if hasattr(scraper, 'cleanup'):
                await scraper.cleanup()
        
        logger.info("Cleanup complete")


# Enhanced version of the original background_search function
async def perform_enhanced_background_search(user_profile_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Enhanced version of the background search that uses all new capabilities
    
    Args:
        user_profile_data: Dictionary with user information
        
    Returns:
        Comprehensive search results with interests and events
    """
    # Extract data
    names = user_profile_data.get('name', '').split(' ', 1)
    first_name = names[0] if names else ''
    last_name = names[1] if len(names) > 1 else ''
    
    location_str = user_profile_data.get('location', '')
    location_parts = location_str.split(',')
    
    location = {
        'city': location_parts[0].strip() if location_parts else '',
        'state': location_parts[1].strip() if len(location_parts) > 1 else '',
        'country': location_parts[2].strip() if len(location_parts) > 2 else 'US',
        'latitude': user_profile_data.get('latitude'),
        'longitude': user_profile_data.get('longitude')
    }
    
    activity = user_profile_data.get('activity', '')
    social_handles = user_profile_data.get('social_handles', {})
    
    # Initialize search system
    search_system = EnhancedSearchSystem()
    
    try:
        # Perform search
        results = await search_system.perform_enhanced_search(
            first_name=first_name,
            last_name=last_name,
            location=location,
            activity=activity,
            social_handles=social_handles
        )
        
        return results
        
    finally:
        # Cleanup
        await search_system.cleanup()


# Update the existing routes.py to use enhanced search
def integrate_enhanced_search(app):
    """
    Integration function to update existing Flask routes
    
    This function should be called in your app.py to integrate
    the enhanced search functionality
    """
    from flask import current_app
    
    # Import the enhanced search
    from searchmethods.enhanced_search import perform_enhanced_background_search
    
    # Replace the existing perform_background_search function
    import searchmethods.background_search
    
    # Monkey patch with async wrapper
    def enhanced_wrapper(user_profile):
        """Wrapper to run async function in sync context"""
        import asyncio
        
        user_data = {
            'name': user_profile.name,
            'location': user_profile.location,
            'social_handles': user_profile.social_handles,
            'activity': user_profile.activity
        }
        
        # Run async function
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        try:
            result = loop.run_until_complete(
                perform_enhanced_background_search(user_data)
            )
            
            # Convert to expected format
            return {
                'raw_results': result.get('search_results', {}),
                'summaries': {
                    'general': f"Found {result['metadata']['total_results_found']} results about {user_profile.name}",
                    'social': f"Analyzed {len(result['user_profile']['social_profiles'])} social media profiles",
                    'location': f"Found {len(result.get('events', []))} local events in {user_profile.location}",
                    'activity': f"Discovered {len(result.get('interests', []))} interests related to {user_profile.activity}"
                },
                'total_results': result['metadata']['total_results_found'],
                'interests': result.get('interests', []),
                'events': result.get('events', [])
            }
            
        finally:
            loop.close()
    
    # Replace the function
    searchmethods.background_search.perform_background_search = enhanced_wrapper
    
    logger.info("Enhanced search integrated successfully")


# Setup and configuration instructions
SETUP_INSTRUCTIONS = """
Enhanced Search System Setup Instructions
=========================================

1. Install Required Dependencies:
   ```bash
   pip install aiohttp beautifulsoup4 selenium cloudscraper fake-useragent
   pip install nltk scikit-learn numpy
   pip install openai transformers sentence-transformers torch
   pip install requests python-dateutil
   ```

2. Download NLTK Data:
   ```python
   import nltk
   nltk.download('punkt')
   nltk.download('stopwords')
   nltk.download('wordnet')
   nltk.download('averaged_perceptron_tagger')
   ```

3. Set Environment Variables:
   ```bash
   export OPENAI_API_KEY="your-openai-api-key"  # Optional
   export EVENTBRITE_API_KEY="your-eventbrite-oauth-token"  # Required for events
   export HUGGINGFACE_API_KEY="your-hf-api-key"  # Optional
   ```

4. Update config/settings.py:
   ```python
   # Add these configurations
   SEARCH_CONFIG = {
       'scrapers': {
           'use_selenium': False,  # Set to True if you have Chrome/Chromium
           'rate_limit': 1,  # Requests per second
       },
       'orchestrator': {
           'max_workers': 10,
       },
       'interest_extraction': {
           'min_interest_score': 0.3,
       },
       'data_filtering': {
           'min_relevance_score': 0.5,
       }
   }
   
   AI_CONFIG = {
       'use_huggingface': True,
       'hf_model': 'facebook/bart-large-mnli',
       'use_local_embeddings': True,
       'embedding_model': 'all-MiniLM-L6-v2'
   }
   
   EVENTBRITE_CONFIG = {
       'MAX_EVENTS': 30,
       'BASE_URL': 'https://www.eventbriteapi.com/v3'
   }
   ```

5. Update your app.py:
   ```python
   from searchmethods.enhanced_search import integrate_enhanced_search
   
   def create_app():
       app = Flask(__name__)
       
       # ... existing code ...
       
       # Integrate enhanced search
       with app.app_context():
           integrate_enhanced_search(app)
       
       return app
   ```

6. For Selenium Support (Optional):
   - Install Chrome/Chromium browser
   - Install ChromeDriver: https://chromedriver.chromium.org/
   - Add to PATH or specify in config

7. API Keys:
   - Eventbrite: Get OAuth token from https://www.eventbrite.com/platform/api
   - OpenAI: Get from https://platform.openai.com/api-keys
   - HuggingFace: Get from https://huggingface.co/settings/tokens

8. Privacy and Ethics:
   - Always respect user privacy
   - Only search publicly available information
   - Implement rate limiting to avoid overloading services
   - Consider GDPR and data protection regulations
   - Get user consent for data collection

9. Performance Optimization:
   - Use caching for repeated searches
   - Implement result pagination
   - Consider using Redis for distributed caching
   - Monitor API usage and costs

10. Error Handling:
    - All modules include comprehensive error handling
    - Check logs in 'logs/' directory
    - Monitor rate limit errors
    - Implement retry logic for transient failures
"""

# Save setup instructions
def save_setup_instructions():
    """Save setup instructions to file"""
    with open('ENHANCED_SEARCH_SETUP.md', 'w') as f:
        f.write(SETUP_INSTRUCTIONS)
    print("Setup instructions saved to ENHANCED_SEARCH_SETUP.md")


if __name__ == "__main__":
    # Save setup instructions
    save_setup_instructions()
    
    # Test the system
    import asyncio
    
    async def test_system():
        """Test the enhanced search system"""
        system = EnhancedSearchSystem()
        
        result = await system.perform_enhanced_search(
            first_name="John",
            last_name="Doe",
            location={
                'city': 'San Francisco',
                'state': 'CA',
                'country': 'US',
                'latitude': 37.7749,
                'longitude': -122.4194
            },
            activity="learn photography",
            social_handles={
                'twitter': 'johndoe',
                'instagram': 'johndoe_photos'
            }
        )
        
        print(json.dumps(result, indent=2))
        
        await system.cleanup()
    
    # Run test
    asyncio.run(test_system())

========================================
# File: /home/jarvis/Documents/GitHub/WhatNowAI/Potential Upgrades/web_scraping_modules.py
========================================

# searchmethods/scrapers/web_scrapers.py
"""
Advanced web scraping modules for various platforms
"""
import asyncio
import aiohttp
import requests
from bs4 import BeautifulSoup
from typing import List, Dict, Any, Optional
import re
import json
from urllib.parse import quote_plus, urljoin
import logging
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
import cloudscraper
from fake_useragent import UserAgent
from datetime import datetime
import time

from ..core import BaseSearchModule, SearchResult, DataSource

logger = logging.getLogger(__name__)


class WebScraperBase(BaseSearchModule):
    """Base class for web scrapers"""
    
    def __init__(self, config: Dict[str, Any] = None):
        super().__init__(config)
        self.ua = UserAgent()
        self.session = None
        self.use_selenium = config.get('use_selenium', False)
        self.driver = None
        
    async def _get_session(self):
        """Get aiohttp session"""
        if not self.session:
            timeout = aiohttp.ClientTimeout(total=30)
            self.session = aiohttp.ClientSession(
                headers={'User-Agent': self.ua.random},
                timeout=timeout
            )
        return self.session
    
    def _get_selenium_driver(self):
        """Get Selenium driver for JavaScript-heavy sites"""
        if not self.driver and self.use_selenium:
            options = Options()
            options.add_argument('--headless')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument(f'user-agent={self.ua.random}')
            
            try:
                self.driver = webdriver.Chrome(options=options)
            except Exception as e:
                logger.error(f"Failed to create Selenium driver: {e}")
                self.use_selenium = False
        
        return self.driver
    
    async def _fetch_page(self, url: str) -> Optional[str]:
        """Fetch page content"""
        try:
            if self.use_selenium:
                driver = self._get_selenium_driver()
                if driver:
                    driver.get(url)
                    time.sleep(2)  # Wait for JS to load
                    return driver.page_source
            
            session = await self._get_session()
            async with session.get(url) as response:
                if response.status == 200:
                    return await response.text()
                    
        except Exception as e:
            logger.error(f"Error fetching {url}: {e}")
            
        return None
    
    def _extract_text(self, soup: BeautifulSoup, selectors: List[str]) -> str:
        """Extract text using CSS selectors"""
        texts = []
        for selector in selectors:
            elements = soup.select(selector)
            for elem in elements:
                text = elem.get_text(strip=True)
                if text:
                    texts.append(text)
        return ' '.join(texts)
    
    async def cleanup(self):
        """Cleanup resources"""
        if self.session:
            await self.session.close()
        if self.driver:
            self.driver.quit()


class GoogleSearchScraper(WebScraperBase):
    """Enhanced Google search scraper"""
    
    def get_source_type(self) -> DataSource:
        return DataSource.WEB_SEARCH
    
    async def search(self, query: str, **kwargs) -> List[SearchResult]:
        """Search Google and parse results"""
        results = []
        num_pages = kwargs.get('num_pages', 3)
        
        for page in range(num_pages):
            url = self._build_search_url(query, page)
            html = await self._fetch_page(url)
            
            if html:
                page_results = self._parse_results(html, query)
                results.extend(page_results)
                
                # Rate limiting between pages
                await asyncio.sleep(2)
        
        return results
    
    def _build_search_url(self, query: str, page: int) -> str:
        """Build Google search URL"""
        start = page * 10
        return f"https://www.google.com/search?q={quote_plus(query)}&start={start}"
    
    def _parse_results(self, html: str, query: str) -> List[SearchResult]:
        """Parse Google search results"""
        soup = BeautifulSoup(html, 'html.parser')
        results = []
        
        # Find result divs
        result_divs = soup.find_all('div', class_='g')
        
        for div in result_divs:
            try:
                # Extract title and URL
                title_elem = div.find('h3')
                link_elem = div.find('a', href=True)
                
                if not title_elem or not link_elem:
                    continue
                
                title = title_elem.get_text(strip=True)
                url = link_elem['href']
                
                # Extract snippet
                snippet_elem = div.find('span', class_='aCOpRe') or div.find('div', class_='IsZvec')
                snippet = snippet_elem.get_text(strip=True) if snippet_elem else ""
                
                # Calculate relevance
                relevance = self._calculate_relevance(title, snippet, query)
                
                result = SearchResult(
                    source=self.get_source_type(),
                    title=title,
                    url=url,
                    content=snippet,
                    relevance_score=relevance,
                    metadata={'search_query': query}
                )
                
                results.append(result)
                
            except Exception as e:
                logger.debug(f"Error parsing result: {e}")
                continue
        
        return results
    
    def _calculate_relevance(self, title: str, content: str, query: str) -> float:
        """Calculate relevance score"""
        text = f"{title} {content}".lower()
        query_terms = query.lower().split()
        
        matches = sum(1 for term in query_terms if term in text)
        relevance = matches / len(query_terms) if query_terms else 0
        
        # Boost for exact phrase match
        if query.lower() in text:
            relevance = min(relevance + 0.3, 1.0)
        
        return relevance


class LinkedInScraper(WebScraperBase):
    """LinkedIn profile scraper"""
    
    def get_source_type(self) -> DataSource:
        return DataSource.PROFESSIONAL
    
    async def search(self, query: str, **kwargs) -> List[SearchResult]:
        """Search LinkedIn profiles"""
        results = []
        
        # LinkedIn search URL
        search_url = f"https://www.google.com/search?q=site:linkedin.com/in+{quote_plus(query)}"
        
        # Use Google to find LinkedIn profiles
        html = await self._fetch_page(search_url)
        
        if html:
            soup = BeautifulSoup(html, 'html.parser')
            
            # Extract LinkedIn URLs from Google results
            linkedin_urls = self._extract_linkedin_urls(soup)
            
            # Scrape each profile
            for url in linkedin_urls[:5]:  # Limit to 5 profiles
                profile_data = await self._scrape_profile(url)
                if profile_data:
                    results.append(profile_data)
                await asyncio.sleep(2)  # Rate limiting
        
        return results
    
    def _extract_linkedin_urls(self, soup: BeautifulSoup) -> List[str]:
        """Extract LinkedIn profile URLs from Google results"""
        urls = []
        
        for link in soup.find_all('a', href=True):
            href = link['href']
            if 'linkedin.com/in/' in href:
                # Clean URL
                if href.startswith('/url?q='):
                    href = href.split('/url?q=')[1].split('&')[0]
                urls.append(href)
        
        return list(set(urls))  # Remove duplicates
    
    async def _scrape_profile(self, url: str) -> Optional[SearchResult]:
        """Scrape LinkedIn profile page"""
        try:
            # Note: Full LinkedIn scraping requires authentication
            # This is a simplified version for public data
            
            html = await self._fetch_page(url)
            if not html:
                return None
            
            soup = BeautifulSoup(html, 'html.parser')
            
            # Extract basic info from page
            title = soup.find('title')
            title_text = title.get_text() if title else "LinkedIn Profile"
            
            # Extract meta description
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            description = meta_desc.get('content', '') if meta_desc else ""
            
            # Extract any visible text
            content = self._extract_text(soup, [
                'h1', 'h2', 'h3', 'p', 
                '.pv-top-card__summary',
                '.pv-about__summary-text'
            ])
            
            return SearchResult(
                source=self.get_source_type(),
                title=title_text,
                url=url,
                content=f"{description} {content}"[:500],
                relevance_score=0.8,
                metadata={'platform': 'linkedin'}
            )
            
        except Exception as e:
            logger.error(f"Error scraping LinkedIn profile {url}: {e}")
            return None


class TwitterScraper(WebScraperBase):
    """Twitter/X scraper using multiple methods"""
    
    def get_source_type(self) -> DataSource:
        return DataSource.SOCIAL_MEDIA
    
    async def search(self, query: str, **kwargs) -> List[SearchResult]:
        """Search Twitter/X"""
        results = []
        
        # Method 1: Use nitter instances (Twitter frontend)
        nitter_results = await self._search_nitter(query)
        results.extend(nitter_results)
        
        # Method 2: Google search for Twitter
        google_results = await self._search_via_google(query)
        results.extend(google_results)
        
        return results
    
    async def _search_nitter(self, query: str) -> List[SearchResult]:
        """Search using Nitter instances"""
        results = []
        
        # List of public Nitter instances
        nitter_instances = [
            'nitter.net',
            'nitter.42l.fr',
            'nitter.pussthecat.org'
        ]
        
        for instance in nitter_instances:
            try:
                url = f"https://{instance}/search?q={quote_plus(query)}&f=users"
                html = await self._fetch_page(url)
                
                if html:
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # Parse user results
                    for user_card in soup.find_all('div', class_='timeline-item'):
                        result = self._parse_nitter_user(user_card, query)
                        if result:
                            results.append(result)
                    
                    if results:
                        break  # Use first working instance
                        
            except Exception as e:
                logger.debug(f"Nitter instance {instance} failed: {e}")
                continue
        
        return results
    
    def _parse_nitter_user(self, user_card, query: str) -> Optional[SearchResult]:
        """Parse Nitter user card"""
        try:
            username = user_card.find('a', class_='username')
            fullname = user_card.find('a', class_='fullname')
            bio = user_card.find('div', class_='tweet-content')
            
            if not username:
                return None
            
            username_text = username.get_text(strip=True)
            fullname_text = fullname.get_text(strip=True) if fullname else ""
            bio_text = bio.get_text(strip=True) if bio else ""
            
            url = f"https://twitter.com/{username_text.replace('@', '')}"
            
            return SearchResult(
                source=self.get_source_type(),
                title=f"{fullname_text} (@{username_text})",
                url=url,
                content=bio_text,
                relevance_score=0.7,
                metadata={
                    'platform': 'twitter',
                    'username': username_text,
                    'search_query': query
                }
            )
            
        except Exception as e:
            logger.debug(f"Error parsing Nitter user: {e}")
            return None
    
    async def _search_via_google(self, query: str) -> List[SearchResult]:
        """Search Twitter via Google"""
        results = []
        
        search_query = f"site:twitter.com {query}"
        url = f"https://www.google.com/search?q={quote_plus(search_query)}"
        
        html = await self._fetch_page(url)
        if html:
            soup = BeautifulSoup(html, 'html.parser')
            
            for result_div in soup.find_all('div', class_='g'):
                try:
                    link = result_div.find('a', href=True)
                    title = result_div.find('h3')
                    snippet = result_div.find('span', class_='aCOpRe')
                    
                    if link and 'twitter.com' in link['href']:
                        results.append(SearchResult(
                            source=self.get_source_type(),
                            title=title.get_text() if title else "Twitter Result",
                            url=link['href'],
                            content=snippet.get_text() if snippet else "",
                            relevance_score=0.6,
                            metadata={'platform': 'twitter', 'via': 'google'}
                        ))
                        
                except Exception as e:
                    logger.debug(f"Error parsing Google Twitter result: {e}")
                    continue
        
        return results


class InstagramScraper(WebScraperBase):
    """Instagram scraper"""
    
    def get_source_type(self) -> DataSource:
        return DataSource.SOCIAL_MEDIA
    
    async def search(self, query: str, **kwargs) -> List[SearchResult]:
        """Search Instagram profiles"""
        results = []
        
        # Method 1: Search via Google
        google_results = await self._search_via_google(query)
        results.extend(google_results)
        
        # Method 2: Use alternative viewers
        viewer_results = await self._search_via_viewers(query)
        results.extend(viewer_results)
        
        return results
    
    async def _search_via_google(self, query: str) -> List[SearchResult]:
        """Search Instagram via Google"""
        results = []
        
        search_query = f"site:instagram.com {query}"
        url = f"https://www.google.com/search?q={quote_plus(search_query)}"
        
        html = await self._fetch_page(url)
        if html:
            soup = BeautifulSoup(html, 'html.parser')
            
            for result_div in soup.find_all('div', class_='g'):
                try:
                    link = result_div.find('a', href=True)
                    title = result_div.find('h3')
                    snippet = result_div.find('span', class_='aCOpRe')
                    
                    if link and 'instagram.com' in link['href']:
                        results.append(SearchResult(
                            source=self.get_source_type(),
                            title=title.get_text() if title else "Instagram Profile",
                            url=link['href'],
                            content=snippet.get_text() if snippet else "",
                            relevance_score=0.6,
                            metadata={'platform': 'instagram', 'via': 'google'}
                        ))
                        
                except Exception as e:
                    logger.debug(f"Error parsing Google Instagram result: {e}")
                    continue
        
        return results
    
    async def _search_via_viewers(self, query: str) -> List[SearchResult]:
        """Search using Instagram viewers"""
        results = []
        
        # List of Instagram viewer sites
        viewers = [
            'picuki.com',
            'imginn.com'
        ]
        
        for viewer in viewers:
            try:
                url = f"https://{viewer}/search/{quote_plus(query)}"
                html = await self._fetch_page(url)
                
                if html:
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # Parse based on viewer structure
                    if 'picuki' in viewer:
                        results.extend(self._parse_picuki_results(soup, query))
                    elif 'imginn' in viewer:
                        results.extend(self._parse_imginn_results(soup, query))
                    
                    if results:
                        break
                        
            except Exception as e:
                logger.debug(f"Instagram viewer {viewer} failed: {e}")
                continue
        
        return results
    
    def _parse_picuki_results(self, soup: BeautifulSoup, query: str) -> List[SearchResult]:
        """Parse Picuki search results"""
        results = []
        
        for profile in soup.find_all('div', class_='profile-result'):
            try:
                username = profile.find('a', class_='username')
                fullname = profile.find('div', class_='fullname')
                bio = profile.find('div', class_='bio')
                
                if username:
                    username_text = username.get_text(strip=True)
                    url = f"https://instagram.com/{username_text}"
                    
                    results.append(SearchResult(
                        source=self.get_source_type(),
                        title=f"{fullname.get_text(strip=True) if fullname else username_text}",
                        url=url,
                        content=bio.get_text(strip=True) if bio else "",
                        relevance_score=0.7,
                        metadata={
                            'platform': 'instagram',
                            'username': username_text,
                            'via': 'picuki'
                        }
                    ))
                    
            except Exception as e:
                logger.debug(f"Error parsing Picuki result: {e}")
                continue
        
        return results
    
    def _parse_imginn_results(self, soup: BeautifulSoup, query: str) -> List[SearchResult]:
        """Parse Imginn search results"""
        # Similar implementation for imginn
        return []


class GitHubScraper(WebScraperBase):
    """GitHub profile and repository scraper"""
    
    def get_source_type(self) -> DataSource:
        return DataSource.PROFESSIONAL
    
    async def search(self, query: str, **kwargs) -> List[SearchResult]:
        """Search GitHub users and repositories"""
        results = []
        
        # Search users
        user_results = await self._search_users(query)
        results.extend(user_results)
        
        # Search repositories
        repo_results = await self._search_repositories(query)
        results.extend(repo_results)
        
        return results
    
    async def _search_users(self, query: str) -> List[SearchResult]:
        """Search GitHub users via API"""
        results = []
        
        try:
            url = f"https://api.github.com/search/users?q={quote_plus(query)}"
            
            session = await self._get_session()
            async with session.get(url) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    for user in data.get('items', [])[:5]:
                        # Fetch user details
                        user_data = await self._fetch_user_details(user['login'])
                        if user_data:
                            results.append(user_data)
                            
        except Exception as e:
            logger.error(f"Error searching GitHub users: {e}")
        
        return results
    
    async def _fetch_user_details(self, username: str) -> Optional[SearchResult]:
        """Fetch detailed user information"""
        try:
            url = f"https://api.github.com/users/{username}"
            
            session = await self._get_session()
            async with session.get(url) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    content = []
                    if data.get('bio'):
                        content.append(f"Bio: {data['bio']}")
                    if data.get('company'):
                        content.append(f"Company: {data['company']}")
                    if data.get('location'):
                        content.append(f"Location: {data['location']}")
                    if data.get('blog'):
                        content.append(f"Website: {data['blog']}")
                    
                    content.append(f"Public repos: {data.get('public_repos', 0)}")
                    content.append(f"Followers: {data.get('followers', 0)}")
                    
                    return SearchResult(
                        source=self.get_source_type(),
                        title=f"{data.get('name', username)} (@{username})",
                        url=data['html_url'],
                        content=' | '.join(content),
                        relevance_score=0.8,
                        metadata={
                            'platform': 'github',
                            'username': username,
                            'type': 'user'
                        }
                    )
                    
        except Exception as e:
            logger.error(f"Error fetching GitHub user {username}: {e}")
            
        return None
    
    async def _search_repositories(self, query: str) -> List[SearchResult]:
        """Search GitHub repositories"""
        results = []
        
        try:
            url = f"https://api.github.com/search/repositories?q={quote_plus(query)}"
            
            session = await self._get_session()
            async with session.get(url) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    for repo in data.get('items', [])[:3]:
                        results.append(SearchResult(
                            source=self.get_source_type(),
                            title=f"{repo['full_name']} - {repo.get('description', 'No description')[:100]}",
                            url=repo['html_url'],
                            content=f"Stars: {repo['stargazers_count']} | Forks: {repo['forks_count']} | Language: {repo.get('language', 'Unknown')}",
                            relevance_score=0.6,
                            metadata={
                                'platform': 'github',
                                'type': 'repository',
                                'owner': repo['owner']['login']
                            }
                        ))
                        
        except Exception as e:
            logger.error(f"Error searching GitHub repositories: {e}")
        
        return results


class RedditScraper(WebScraperBase):
    """Reddit posts and comments scraper"""
    
    def get_source_type(self) -> DataSource:
        return DataSource.FORUMS
    
    async def search(self, query: str, **kwargs) -> List[SearchResult]:
        """Search Reddit posts and comments"""
        results = []
        
        # Use Reddit's JSON API (no auth required for public data)
        search_url = f"https://www.reddit.com/search.json?q={quote_plus(query)}&limit=10"
        
        try:
            session = await self._get_session()
            async with session.get(search_url) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    for post in data['data']['children']:
                        post_data = post['data']
                        
                        # Create result
                        result = SearchResult(
                            source=self.get_source_type(),
                            title=post_data['title'],
                            url=f"https://reddit.com{post_data['permalink']}",
                            content=post_data.get('selftext', '')[:500],
                            relevance_score=self._calculate_reddit_relevance(post_data, query),
                            metadata={
                                'platform': 'reddit',
                                'subreddit': post_data['subreddit'],
                                'author': post_data['author'],
                                'score': post_data['score'],
                                'num_comments': post_data['num_comments']
                            }
                        )
                        
                        results.append(result)
                        
        except Exception as e:
            logger.error(f"Error searching Reddit: {e}")
        
        return results
    
    def _calculate_reddit_relevance(self, post_data: dict, query: str) -> float:
        """Calculate relevance based on Reddit metrics"""
        base_relevance = 0.5
        
        # Boost for query match in title
        if query.lower() in post_data['title'].lower():
            base_relevance += 0.2
        
        # Boost for high engagement
        if post_data['score'] > 100:
            base_relevance += 0.1
        if post_data['num_comments'] > 50:
            base_relevance += 0.1
        
        # Boost for recent posts
        import time
        post_age_days = (time.time() - post_data['created_utc']) / 86400
        if post_age_days < 30:
            base_relevance += 0.1
        
        return min(base_relevance, 1.0)


class NewsArchiveScraper(WebScraperBase):
    """Scraper for news archives and articles"""
    
    def get_source_type(self) -> DataSource:
        return DataSource.NEWS_ARTICLES
    
    async def search(self, query: str, **kwargs) -> List[SearchResult]:
        """Search news archives"""
        results = []
        
        # Search multiple news sources
        sources = [
            ('Google News', self._search_google_news),
            ('Archive.org', self._search_archive_org),
        ]
        
        for source_name, search_func in sources:
            try:
                source_results = await search_func(query)
                results.extend(source_results)
            except Exception as e:
                logger.error(f"Error searching {source_name}: {e}")
        
        return results
    
    async def _search_google_news(self, query: str) -> List[SearchResult]:
        """Search Google News"""
        results = []
        
        url = f"https://news.google.com/search?q={quote_plus(query)}"
        html = await self._fetch_page(url)
        
        if html:
            soup = BeautifulSoup(html, 'html.parser')
            
            # Parse news articles
            for article in soup.find_all('article'):
                try:
                    title_elem = article.find('h3') or article.find('h4')
                    link_elem = article.find('a', href=True)
                    time_elem = article.find('time')
                    
                    if title_elem and link_elem:
                        # Google News links are encoded
                        url = link_elem['href']
                        if url.startswith('./'):
                            url = f"https://news.google.com{url[1:]}"
                        
                        results.append(SearchResult(
                            source=self.get_source_type(),
                            title=title_elem.get_text(strip=True),
                            url=url,
                            content=f"Published: {time_elem.get_text() if time_elem else 'Unknown'}",
                            relevance_score=0.7,
                            metadata={
                                'source': 'google_news',
                                'type': 'news_article'
                            }
                        ))
                        
                except Exception as e:
                    logger.debug(f"Error parsing news article: {e}")
                    continue
        
        return results[:5]  # Limit results
    
    async def _search_archive_org(self, query: str) -> List[SearchResult]:
        """Search Internet Archive"""
        results = []
        
        # Use Archive.org search API
        url = f"https://archive.org/advancedsearch.php?q={quote_plus(query)}&output=json&rows=5"
        
        try:
            session = await self._get_session()
            async with session.get(url) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    for doc in data.get('response', {}).get('docs', []):
                        results.append(SearchResult(
                            source=self.get_source_type(),
                            title=doc.get('title', 'Archived Document'),
                            url=f"https://archive.org/details/{doc.get('identifier', '')}",
                            content=doc.get('description', '')[:300],
                            relevance_score=0.6,
                            metadata={
                                'source': 'archive.org',
                                'date': doc.get('publicdate', ''),
                                'type': doc.get('mediatype', 'unknown')
                            }
                        ))
                        
        except Exception as e:
            logger.error(f"Error searching Archive.org: {e}")
        
        return results

========================================
# File: /home/jarvis/Documents/GitHub/WhatNowAI/Potential Upgrades/enhanced_search_system.py
========================================

"""
Enhanced Modular Search System for WhatNowAI
=============================================

This is the core architecture and module structure for the enhanced search system.
Each module is designed to be independent, testable, and production-ready.
"""

# searchmethods/core/__init__.py
"""
Core search functionality and base classes
"""
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import Dict, List, Any, Optional, Set
from datetime import datetime
import logging
import asyncio
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import json
import hashlib
from enum import Enum

logger = logging.getLogger(__name__)


class DataSource(Enum):
    """Enumeration of available data sources"""
    WEB_SEARCH = "web_search"
    SOCIAL_MEDIA = "social_media"
    PUBLIC_RECORDS = "public_records"
    NEWS_ARTICLES = "news_articles"
    PROFESSIONAL = "professional"
    ACADEMIC = "academic"
    FORUMS = "forums"
    BLOGS = "blogs"


@dataclass
class SearchResult:
    """Standardized search result structure"""
    source: DataSource
    title: str
    url: str
    content: str
    relevance_score: float
    metadata: Dict[str, Any] = field(default_factory=dict)
    timestamp: datetime = field(default_factory=datetime.now)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization"""
        return {
            'source': self.source.value,
            'title': self.title,
            'url': self.url,
            'content': self.content,
            'relevance_score': self.relevance_score,
            'metadata': self.metadata,
            'timestamp': self.timestamp.isoformat()
        }
    
    def get_hash(self) -> str:
        """Generate unique hash for deduplication"""
        content = f"{self.source.value}:{self.url}:{self.title}"
        return hashlib.md5(content.encode()).hexdigest()


@dataclass
class UserInterest:
    """Extracted user interest with confidence score"""
    category: str
    keywords: List[str]
    confidence: float
    source: str
    evidence: str
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'category': self.category,
            'keywords': self.keywords,
            'confidence': self.confidence,
            'source': self.source,
            'evidence': self.evidence
        }


@dataclass
class EnhancedUserProfile:
    """Enhanced user profile with all collected data"""
    # Basic info
    first_name: str
    last_name: str
    full_name: str
    location: Dict[str, Any]
    activity: str
    
    # Extended info
    interests: List[UserInterest] = field(default_factory=list)
    social_profiles: Dict[str, str] = field(default_factory=dict)
    professional_info: Dict[str, Any] = field(default_factory=dict)
    inferred_demographics: Dict[str, Any] = field(default_factory=dict)
    
    # Search metadata
    search_timestamp: datetime = field(default_factory=datetime.now)
    data_sources_used: Set[DataSource] = field(default_factory=set)
    total_results_found: int = 0
    
    def add_interest(self, interest: UserInterest):
        """Add interest with deduplication"""
        for existing in self.interests:
            if existing.category == interest.category and existing.source == interest.source:
                # Update if higher confidence
                if interest.confidence > existing.confidence:
                    existing.keywords.extend(interest.keywords)
                    existing.keywords = list(set(existing.keywords))
                    existing.confidence = interest.confidence
                return
        self.interests.append(interest)
    
    def get_top_interests(self, n: int = 10) -> List[UserInterest]:
        """Get top N interests by confidence"""
        return sorted(self.interests, key=lambda x: x.confidence, reverse=True)[:n]
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'basic_info': {
                'first_name': self.first_name,
                'last_name': self.last_name,
                'full_name': self.full_name,
                'location': self.location,
                'activity': self.activity
            },
            'interests': [i.to_dict() for i in self.interests],
            'social_profiles': self.social_profiles,
            'professional_info': self.professional_info,
            'inferred_demographics': self.inferred_demographics,
            'metadata': {
                'search_timestamp': self.search_timestamp.isoformat(),
                'data_sources_used': [ds.value for ds in self.data_sources_used],
                'total_results_found': self.total_results_found
            }
        }


class BaseSearchModule(ABC):
    """Base class for all search modules"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.logger = logging.getLogger(self.__class__.__name__)
        self.rate_limiter = RateLimiter(
            calls_per_second=self.config.get('rate_limit', 1)
        )
    
    @abstractmethod
    async def search(self, query: str, **kwargs) -> List[SearchResult]:
        """Perform search and return results"""
        pass
    
    @abstractmethod
    def get_source_type(self) -> DataSource:
        """Return the data source type"""
        pass
    
    async def search_with_rate_limit(self, query: str, **kwargs) -> List[SearchResult]:
        """Search with rate limiting"""
        await self.rate_limiter.acquire()
        return await self.search(query, **kwargs)
    
    def validate_results(self, results: List[SearchResult]) -> List[SearchResult]:
        """Validate and clean results"""
        valid_results = []
        for result in results:
            if self._is_valid_result(result):
                valid_results.append(result)
        return valid_results
    
    def _is_valid_result(self, result: SearchResult) -> bool:
        """Check if result is valid"""
        return (
            result.title and
            result.url and
            result.content and
            len(result.content) > 50 and
            result.relevance_score >= 0.0
        )


class RateLimiter:
    """Simple rate limiter for API calls"""
    
    def __init__(self, calls_per_second: float):
        self.calls_per_second = calls_per_second
        self.min_interval = 1.0 / calls_per_second
        self.last_call = 0
        self.lock = asyncio.Lock()
    
    async def acquire(self):
        """Wait if necessary to respect rate limit"""
        async with self.lock:
            elapsed = asyncio.get_event_loop().time() - self.last_call
            if elapsed < self.min_interval:
                await asyncio.sleep(self.min_interval - elapsed)
            self.last_call = asyncio.get_event_loop().time()


class SearchOrchestrator:
    """Orchestrates multiple search modules in parallel"""
    
    def __init__(self, modules: List[BaseSearchModule], config: Dict[str, Any] = None):
        self.modules = modules
        self.config = config or {}
        self.logger = logging.getLogger(__name__)
        self.executor = ThreadPoolExecutor(
            max_workers=self.config.get('max_workers', 10)
        )
    
    async def search_all(self, user_profile: EnhancedUserProfile) -> Dict[str, List[SearchResult]]:
        """Execute all search modules in parallel"""
        self.logger.info(f"Starting orchestrated search for {user_profile.full_name}")
        
        # Prepare search queries
        queries = self._prepare_queries(user_profile)
        
        # Execute searches in parallel
        tasks = []
        for module in self.modules:
            for query in queries:
                task = asyncio.create_task(
                    self._execute_module_search(module, query, user_profile)
                )
                tasks.append((module.get_source_type(), task))
        
        # Collect results
        results = {}
        for source_type, task in tasks:
            try:
                module_results = await task
                if source_type not in results:
                    results[source_type] = []
                results[source_type].extend(module_results)
            except Exception as e:
                self.logger.error(f"Error in {source_type}: {e}")
        
        # Deduplicate results
        for source_type in results:
            results[source_type] = self._deduplicate_results(results[source_type])
        
        return results
    
    def _prepare_queries(self, user_profile: EnhancedUserProfile) -> List[str]:
        """Prepare search queries based on user profile"""
        queries = []
        
        # Name-based queries
        queries.append(f'"{user_profile.full_name}"')
        queries.append(f'"{user_profile.first_name} {user_profile.last_name}"')
        
        # Location-based queries
        if user_profile.location.get('city'):
            queries.append(f'"{user_profile.full_name}" {user_profile.location["city"]}')
        
        # Activity-based queries
        if user_profile.activity:
            queries.append(f'"{user_profile.full_name}" {user_profile.activity}')
        
        # Social media queries
        for platform, handle in user_profile.social_profiles.items():
            if handle:
                queries.append(f'"{handle}" {platform}')
        
        return queries
    
    async def _execute_module_search(
        self, 
        module: BaseSearchModule, 
        query: str, 
        user_profile: EnhancedUserProfile
    ) -> List[SearchResult]:
        """Execute search for a single module"""
        try:
            results = await module.search_with_rate_limit(query)
            validated = module.validate_results(results)
            user_profile.data_sources_used.add(module.get_source_type())
            user_profile.total_results_found += len(validated)
            return validated
        except Exception as e:
            self.logger.error(f"Error in {module.__class__.__name__}: {e}")
            return []
    
    def _deduplicate_results(self, results: List[SearchResult]) -> List[SearchResult]:
        """Remove duplicate results based on hash"""
        seen = set()
        unique = []
        for result in results:
            hash_val = result.get_hash()
            if hash_val not in seen:
                seen.add(hash_val)
                unique.append(result)
        return unique


# searchmethods/core/data_processor.py
"""
Data processing and interest extraction
"""
import re
from typing import List, Dict, Any, Set
import nltk
from collections import Counter
import logging

logger = logging.getLogger(__name__)

# Download required NLTK data
try:
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
    nltk.download('wordnet', quiet=True)
    nltk.download('averaged_perceptron_tagger', quiet=True)
except:
    logger.warning("Failed to download NLTK data")


class InterestExtractor:
    """Extract interests from search results"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.interest_categories = self._load_interest_categories()
        self.stop_words = set(nltk.corpus.stopwords.words('english'))
    
    def _load_interest_categories(self) -> Dict[str, List[str]]:
        """Load interest category mappings"""
        return {
            'music': ['concert', 'music', 'band', 'singer', 'album', 'spotify', 'soundcloud', 
                     'guitar', 'piano', 'drums', 'festival', 'dj', 'electronic', 'rock', 'pop', 
                     'jazz', 'classical', 'hip-hop', 'rap'],
            'sports': ['sports', 'football', 'basketball', 'baseball', 'soccer', 'tennis', 
                      'golf', 'running', 'fitness', 'gym', 'workout', 'athlete', 'team', 
                      'game', 'match', 'tournament', 'league'],
            'technology': ['tech', 'programming', 'coding', 'software', 'developer', 'engineer',
                          'computer', 'ai', 'machine learning', 'data science', 'startup',
                          'app', 'website', 'github', 'code', 'hackathon'],
            'arts': ['art', 'painting', 'drawing', 'sculpture', 'gallery', 'museum', 'artist',
                    'creative', 'design', 'photography', 'photo', 'instagram', 'visual'],
            'food': ['food', 'restaurant', 'cooking', 'chef', 'recipe', 'cuisine', 'dining',
                    'foodie', 'eat', 'taste', 'meal', 'dish', 'culinary'],
            'travel': ['travel', 'trip', 'vacation', 'tourism', 'flight', 'hotel', 'destination',
                      'adventure', 'explore', 'journey', 'abroad', 'passport'],
            'fitness': ['fitness', 'health', 'exercise', 'yoga', 'pilates', 'crossfit', 'marathon',
                       'cycling', 'swimming', 'training', 'wellness', 'nutrition'],
            'gaming': ['gaming', 'game', 'gamer', 'xbox', 'playstation', 'nintendo', 'steam',
                      'twitch', 'esports', 'multiplayer', 'rpg', 'fps'],
            'education': ['education', 'learning', 'course', 'university', 'college', 'degree',
                         'student', 'teacher', 'academic', 'research', 'study', 'school'],
            'business': ['business', 'entrepreneur', 'startup', 'company', 'ceo', 'founder',
                        'marketing', 'sales', 'finance', 'investment', 'linkedin'],
            'entertainment': ['movie', 'film', 'tv', 'show', 'netflix', 'theater', 'cinema',
                             'actor', 'actress', 'director', 'series', 'streaming'],
            'fashion': ['fashion', 'style', 'clothing', 'outfit', 'brand', 'designer', 'trend',
                       'wardrobe', 'accessories', 'shoes', 'jewelry'],
            'nature': ['nature', 'outdoor', 'hiking', 'camping', 'mountain', 'beach', 'forest',
                      'park', 'wildlife', 'environment', 'eco', 'green'],
            'social': ['community', 'volunteer', 'charity', 'nonprofit', 'social', 'cause',
                      'helping', 'support', 'organization', 'impact'],
            'culture': ['culture', 'history', 'heritage', 'tradition', 'language', 'cultural',
                       'festival', 'celebration', 'customs', 'diversity']
        }
    
    def extract_interests(self, search_results: List[SearchResult]) -> List[UserInterest]:
        """Extract interests from search results"""
        interests = []
        
        # Combine all text content
        all_text = ' '.join([
            f"{result.title} {result.content}" 
            for result in search_results
        ])
        
        # Extract keywords
        keywords = self._extract_keywords(all_text)
        
        # Categorize interests
        category_scores = self._categorize_keywords(keywords)
        
        # Create UserInterest objects
        for category, score in category_scores.items():
            if score > self.config.get('min_interest_score', 0.3):
                interest = UserInterest(
                    category=category,
                    keywords=self._get_category_keywords(keywords, category),
                    confidence=min(score, 1.0),
                    source='content_analysis',
                    evidence=f"Found {int(score * 100)} references to {category}-related content"
                )
                interests.append(interest)
        
        return interests
    
    def _extract_keywords(self, text: str) -> List[str]:
        """Extract meaningful keywords from text"""
        # Tokenize and clean
        tokens = nltk.word_tokenize(text.lower())
        
        # Remove stopwords and short words
        keywords = [
            token for token in tokens
            if token not in self.stop_words
            and len(token) > 3
            and token.isalpha()
        ]
        
        # Get POS tags and filter
        pos_tags = nltk.pos_tag(keywords)
        
        # Keep nouns and verbs
        filtered_keywords = [
            word for word, pos in pos_tags
            if pos.startswith('NN') or pos.startswith('VB')
        ]
        
        return filtered_keywords
    
    def _categorize_keywords(self, keywords: List[str]) -> Dict[str, float]:
        """Categorize keywords into interest categories"""
        keyword_freq = Counter(keywords)
        category_scores = {}
        
        for category, category_keywords in self.interest_categories.items():
            score = 0
            matches = 0
            
            for keyword, freq in keyword_freq.items():
                if any(cat_kw in keyword for cat_kw in category_keywords):
                    score += freq
                    matches += 1
            
            if matches > 0:
                # Normalize score
                category_scores[category] = score / (len(keywords) + 1)
        
        return category_scores
    
    def _get_category_keywords(self, keywords: List[str], category: str) -> List[str]:
        """Get keywords that match a category"""
        category_keywords = self.interest_categories.get(category, [])
        matching = []
        
        for keyword in set(keywords):
            if any(cat_kw in keyword for cat_kw in category_keywords):
                matching.append(keyword)
        
        return matching[:10]  # Top 10 keywords


class DataFilter:
    """Filter and validate search results"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.min_relevance = config.get('min_relevance_score', 0.5)
    
    def filter_results(
        self, 
        results: List[SearchResult], 
        user_profile: EnhancedUserProfile
    ) -> List[SearchResult]:
        """Filter results based on relevance to user"""
        filtered = []
        
        for result in results:
            if self._is_relevant(result, user_profile):
                filtered.append(result)
        
        # Sort by relevance
        return sorted(filtered, key=lambda x: x.relevance_score, reverse=True)
    
    def _is_relevant(self, result: SearchResult, user_profile: EnhancedUserProfile) -> bool:
        """Check if result is relevant to user"""
        # Check minimum relevance score
        if result.relevance_score < self.min_relevance:
            return False
        
        # Check name match
        if not self._contains_name(result, user_profile):
            return False
        
        # Check location relevance if available
        if user_profile.location.get('city'):
            if not self._is_location_relevant(result, user_profile):
                result.relevance_score *= 0.8  # Reduce score for non-local
        
        return True
    
    def _contains_name(self, result: SearchResult, user_profile: EnhancedUserProfile) -> bool:
        """Check if result contains user's name"""
        text = f"{result.title} {result.content}".lower()
        
        # Check full name
        if user_profile.full_name.lower() in text:
            return True
        
        # Check partial name
        if (user_profile.first_name.lower() in text and 
            user_profile.last_name.lower() in text):
            return True
        
        # Check social handles
        for handle in user_profile.social_profiles.values():
            if handle and handle.lower() in text:
                return True
        
        return False
    
    def _is_location_relevant(self, result: SearchResult, user_profile: EnhancedUserProfile) -> bool:
        """Check if result is location relevant"""
        text = f"{result.title} {result.content}".lower()
        
        city = user_profile.location.get('city', '').lower()
        state = user_profile.location.get('state', '').lower()
        country = user_profile.location.get('country', '').lower()
        
        return any(loc in text for loc in [city, state, country] if loc)

========================================
# File: /home/jarvis/Documents/GitHub/WhatNowAI/searchmethods/__init__.py
========================================

"""
Search methods package for WhatNowAI
Provides background search and web scraping functionality
"""

from .background_search import (
    BackgroundSearchService,
    UserProfile,
    SearchResult,
    perform_background_search
)

__all__ = [
    'BackgroundSearchService',
    'UserProfile', 
    'SearchResult',
    'perform_background_search'
]

========================================
# File: /home/jarvis/Documents/GitHub/WhatNowAI/searchmethods/background_search.py
========================================

"""
Background search service for gathering user and location context
Uses web scraping and search engines to find relevant information
"""

import asyncio
import aiohttp
import logging
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from urllib.parse import quote_plus, urljoin, urlparse
import json
import re
from bs4 import BeautifulSoup
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

logger = logging.getLogger(__name__)


@dataclass
class UserProfile:
    """User profile data structure"""
    name: str
    location: str = ""
    social_handles: Dict[str, str] = None
    activity: str = ""
    
    def __post_init__(self):
        if self.social_handles is None:
            self.social_handles = {}


@dataclass
class SearchResult:
    """Search result data structure"""
    source: str
    title: str
    url: str
    content: str
    relevance_score: float = 0.0
    timestamp: str = ""


class BackgroundSearchService:
    """Service for performing background searches on user information"""
    
    def __init__(self, config: Dict[str, Any] = None):
        """
        Initialize the background search service
        
        Args:
            config: Configuration dictionary with search parameters
        """
        from config.settings import SEARCH_CONFIG
        
        self.config = config or SEARCH_CONFIG
        self.session = None
        self.results_cache = {}
        
    def _setup_session(self) -> requests.Session:
        """Setup requests session with retries and proper headers"""
        session = requests.Session()
        
        # Setup retry strategy
        retry_strategy = Retry(
            total=3,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["HEAD", "GET", "OPTIONS"]  # Updated parameter name
        )
        
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        # Set headers
        session.headers.update({
            'User-Agent': self.config.get('USER_AGENT', 'WhatNowAI/1.0'),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        })
        
        return session
    
    def search_user_info(self, user_profile: UserProfile) -> Dict[str, List[SearchResult]]:
        """
        Search for information about the user focusing only on social media and local activities
        
        Args:
            user_profile: User profile containing name, location, social handles, etc.
            
        Returns:
            Dictionary of search results organized by source
        """
        logger.info(f"Starting focused search for user: {user_profile.name}")
        
        results = {
            'general': [],  # Will remain empty - no general searches
            'social': [],
            'location': [],
            'activity': []
        }
        
        if not self.session:
            self.session = self._setup_session()
        
        import time
        start_time = time.time()
        search_timeout = 10  # 10 second limit
        
        try:
            # Only search social media platforms if handles are provided
            if user_profile.social_handles:
                # Check if we still have time
                if time.time() - start_time < search_timeout:
                    social_results = self._search_social_media(user_profile.social_handles)
                    results['social'].extend(social_results)
                    logger.info(f"Found {len(social_results)} social media results")
            
            # Search location-specific information if we have time
            if user_profile.location and (time.time() - start_time < search_timeout):
                location_results = self._search_location_info(user_profile.location, user_profile.activity)
                results['location'].extend(location_results)
                logger.info(f"Found {len(location_results)} location results")
            
            # Search activity-related information if we have time
            if user_profile.activity and (time.time() - start_time < search_timeout):
                activity_results = self._search_activity_info(user_profile.activity, user_profile.location)
                results['activity'].extend(activity_results)
                logger.info(f"Found {len(activity_results)} activity results")
            
            # Log if we hit the timeout
            elapsed_time = time.time() - start_time
            if elapsed_time >= search_timeout:
                logger.warning(f"Search timeout reached ({elapsed_time:.1f}s), returning partial results")
            else:
                logger.info(f"Search completed in {elapsed_time:.1f}s")
                
        except Exception as e:
            logger.error(f"Error during focused search: {str(e)}")
        
        return results
    
    def _search_general_info(self, name: str) -> List[SearchResult]:
        """Search for specific information about this person (not celebrities)"""
        results = []
        
        # Focus on finding the actual user, not celebrities with the same name
        # Use more specific search terms to filter out famous people
        specific_queries = [
            f'"{name}" -wikipedia -celebrity -famous -actor -singer -politician',
            f'"{name}" profile personal',
            f'"{name}" professional background'
        ]
        
        # Use DuckDuckGo for privacy-focused search
        for query in specific_queries:
            encoded_query = quote_plus(query)
            search_urls = [
                f"https://duckduckgo.com/html/?q={encoded_query}",
            ]
            
            for url in search_urls:
                try:
                    response = self.session.get(url, timeout=self.config.get('TIMEOUT', 30))
                    if response.status_code == 200:
                        search_results = self._parse_search_results(response.text, 'general')
                        # Filter out celebrity/famous person results
                        filtered_results = [r for r in search_results if not self._is_celebrity_result(r)]
                        results.extend(filtered_results[:3])  # Limit to 3 per query
                        time.sleep(1)  # Rate limiting
                except Exception as e:
                    logger.warning(f"Failed to search {url}: {str(e)}")
                    continue
        
        return results[:self.config.get('MAX_RESULTS_PER_SOURCE', 8)]
    
    def _is_celebrity_result(self, result: SearchResult) -> bool:
        """Check if a search result is about a celebrity/famous person"""
        celebrity_keywords = [
            'wikipedia', 'celebrity', 'famous', 'actor', 'actress', 'singer', 
            'musician', 'politician', 'athlete', 'sports', 'movie', 'film',
            'album', 'song', 'tv show', 'series', 'biography', 'born in',
            'filmography', 'discography', 'awards', 'grammy', 'oscar', 'emmy'
        ]
        
        content_lower = result.content.lower()
        title_lower = result.title.lower()
        
        return any(keyword in content_lower or keyword in title_lower for keyword in celebrity_keywords)
    
    def _search_social_media(self, social_handles: Dict[str, str]) -> List[SearchResult]:
        """Search social media platforms for user information - fast and focused"""
        results = []
        start_time = time.time()
        timeout_per_platform = 2  # Max 2 seconds per platform
        
        for platform, handle in social_handles.items():
            if not handle:
                continue
            
            # Check timeout
            if time.time() - start_time > 8:  # Reserve 2 seconds for other searches
                logger.warning("Social media search timeout reached, skipping remaining platforms")
                break
                
            try:
                platform_start = time.time()
                
                if platform.lower() == 'github':
                    # GitHub API is fastest, prioritize it
                    platform_results = self._search_github_info(handle)
                    results.extend(platform_results)
                elif platform.lower() in ['twitter', 'linkedin', 'instagram', 'tiktok', 'youtube']:
                    # Quick search for other platforms
                    platform_results = self._quick_social_search(platform, handle)
                    results.extend(platform_results)
                
                # Check if this platform took too long
                if time.time() - platform_start > timeout_per_platform:
                    logger.warning(f"{platform} search took too long, skipping remaining platforms")
                    break
                    
            except Exception as e:
                logger.warning(f"Failed to search {platform} for {handle}: {str(e)}")
                continue
        
        return results[:self.config.get('MAX_RESULTS_PER_SOURCE', 3)]
    
    def _quick_social_search(self, platform: str, handle: str) -> List[SearchResult]:
        """Quick search for social media platforms with minimal processing"""
        results = []
        
        try:
            # Simple site-specific search
            query = quote_plus(f"site:{platform}.com {handle}")
            url = f"https://duckduckgo.com/html/?q={query}"
            
            response = self.session.get(url, timeout=3)  # Short timeout
            if response.status_code == 200:
                # Quick parse - just get first few results
                soup = BeautifulSoup(response.text, 'html.parser')
                result_elements = soup.find_all('div', class_='result')[:2]  # Max 2 results
                
                for element in result_elements:
                    try:
                        title_elem = element.find('a', class_='result__a')
                        snippet_elem = element.find('a', class_='result__snippet')
                        
                        if title_elem and snippet_elem:
                            result = SearchResult(
                                source='social',
                                title=title_elem.get_text(strip=True)[:100],  # Truncate for speed
                                url=title_elem.get('href', ''),
                                content=snippet_elem.get_text(strip=True)[:200],  # Truncate for speed
                                relevance_score=0.7
                            )
                            results.append(result)
                    except:
                        continue
                        
        except Exception as e:
            logger.warning(f"Quick search failed for {platform}: {str(e)}")
        
        return results
    
    def _search_location_info(self, location: str, activity: str = "") -> List[SearchResult]:
        """Fast search for location-specific activities"""
        results = []
        
        if not self.config.get('LOCAL_ACTIVITY_SEARCH', True):
            return results
        
        # Limit to most important queries for speed
        queries = [
            f"things to do {location} today",
        ]
        
        # Add one activity-specific query if provided
        if activity:
            queries.append(f"{activity} {location} classes")
        
        for query in queries[:2]:  # Max 2 queries for speed
            try:
                encoded_query = quote_plus(query)
                url = f"https://duckduckgo.com/html/?q={encoded_query}"
                
                response = self.session.get(url, timeout=self.config.get('TIMEOUT', 5))
                if response.status_code == 200:
                    search_results = self._quick_parse_results(response.text, 'location')
                    results.extend(search_results[:2])  # Max 2 per query
                    
            except Exception as e:
                logger.warning(f"Failed location search for {query}: {str(e)}")
                continue
        
        return results[:self.config.get('MAX_RESULTS_PER_SOURCE', 3)]
    
    def _search_activity_info(self, activity: str, location: str = "") -> List[SearchResult]:
        """Fast search for activity-related information"""
        results = []
        
        # Limit to essential queries for speed
        queries = [
            f"how to get started with {activity}",
        ]
        
        # Add location-specific query if provided
        if location:
            queries.append(f"{activity} beginner guide {location}")
        
        for query in queries[:2]:  # Max 2 queries for speed
            try:
                encoded_query = quote_plus(query)
                url = f"https://duckduckgo.com/html/?q={encoded_query}"
                
                response = self.session.get(url, timeout=self.config.get('TIMEOUT', 5))
                if response.status_code == 200:
                    search_results = self._quick_parse_results(response.text, 'activity')
                    results.extend(search_results[:2])  # Max 2 per query
                    
            except Exception as e:
                logger.warning(f"Failed activity search for {query}: {str(e)}")
                continue
        
        return results[:self.config.get('MAX_RESULTS_PER_SOURCE', 3)]
    
    def _quick_parse_results(self, html_content: str, source_type: str) -> List[SearchResult]:
        """Fast parsing of search results with minimal processing"""
        results = []
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            result_elements = soup.find_all('div', class_='result')[:3]  # Max 3 for speed
            
            for element in result_elements:
                try:
                    title_elem = element.find('a', class_='result__a')
                    snippet_elem = element.find('a', class_='result__snippet')
                    
                    if title_elem and snippet_elem:
                        result = SearchResult(
                            source=source_type,
                            title=title_elem.get_text(strip=True)[:80],  # Truncate for speed
                            url=title_elem.get('href', ''),
                            content=snippet_elem.get_text(strip=True)[:150],  # Truncate for speed
                            relevance_score=0.5
                        )
                        results.append(result)
                        
                except Exception:
                    continue
                    
        except Exception as e:
            logger.warning(f"Quick parse failed: {str(e)}")
        
        return results
    
    def _search_twitter_info(self, handle: str) -> List[SearchResult]:
        """Search for Twitter profile information (public data only)"""
        results = []
        
        # Search for public information about the Twitter handle
        query = quote_plus(f"site:twitter.com {handle}")
        url = f"https://duckduckgo.com/html/?q={query}"
        
        try:
            response = self.session.get(url, timeout=self.config.get('TIMEOUT', 30))
            if response.status_code == 200:
                results = self._parse_search_results(response.text, 'social')
        except Exception as e:
            logger.warning(f"Failed to search Twitter info for {handle}: {str(e)}")
        
        return results
    
    def _search_linkedin_info(self, handle: str) -> List[SearchResult]:
        """Search for LinkedIn profile information (public data only)"""
        results = []
        
        # Search for public LinkedIn information
        query = quote_plus(f"site:linkedin.com {handle}")
        url = f"https://duckduckgo.com/html/?q={query}"
        
        try:
            response = self.session.get(url, timeout=self.config.get('TIMEOUT', 30))
            if response.status_code == 200:
                results = self._parse_search_results(response.text, 'social')
        except Exception as e:
            logger.warning(f"Failed to search LinkedIn info for {handle}: {str(e)}")
        
        return results
    
    def _search_github_info(self, handle: str) -> List[SearchResult]:
        """Search for GitHub profile information using GitHub API"""
        results = []
        
        try:
            # Try to get public GitHub profile information
            api_url = f"https://api.github.com/users/{handle}"
            response = self.session.get(api_url, timeout=self.config.get('TIMEOUT', 30))
            
            if response.status_code == 200:
                data = response.json()
                
                # Create a search result from GitHub profile
                result = SearchResult(
                    source='github',
                    title=f"GitHub Profile: {data.get('name', handle)}",
                    url=data.get('html_url', f"https://github.com/{handle}"),
                    content=f"Bio: {data.get('bio', 'No bio available')}. "
                           f"Public repos: {data.get('public_repos', 0)}. "
                           f"Followers: {data.get('followers', 0)}. "
                           f"Location: {data.get('location', 'Not specified')}.",
                    relevance_score=0.8
                )
                results.append(result)
                
        except Exception as e:
            logger.warning(f"Failed to search GitHub info for {handle}: {str(e)}")
        
        return results
    
    def _search_instagram_info(self, handle: str) -> List[SearchResult]:
        """Search for Instagram profile information (public data only)"""
        results = []
        
        # Search for public Instagram information
        query = quote_plus(f"site:instagram.com {handle}")
        url = f"https://duckduckgo.com/html/?q={query}"
        
        try:
            response = self.session.get(url, timeout=self.config.get('TIMEOUT', 30))
            if response.status_code == 200:
                results = self._parse_search_results(response.text, 'social')
        except Exception as e:
            logger.warning(f"Failed to search Instagram info for {handle}: {str(e)}")
        
        return results
    
    def _search_tiktok_info(self, handle: str) -> List[SearchResult]:
        """Search for TikTok profile information (public data only)"""
        results = []
        
        # Search for public TikTok information
        query = quote_plus(f"site:tiktok.com @{handle}")
        url = f"https://duckduckgo.com/html/?q={query}"
        
        try:
            response = self.session.get(url, timeout=self.config.get('TIMEOUT', 30))
            if response.status_code == 200:
                results = self._parse_search_results(response.text, 'social')
        except Exception as e:
            logger.warning(f"Failed to search TikTok info for {handle}: {str(e)}")
        
        return results
    
    def _search_youtube_info(self, handle: str) -> List[SearchResult]:
        """Search for YouTube channel information (public data only)"""
        results = []
        
        # Search for public YouTube information
        queries = [
            quote_plus(f"site:youtube.com {handle}"),
            quote_plus(f"site:youtube.com/c/{handle}"),
            quote_plus(f"site:youtube.com/@{handle}")
        ]
        
        for query in queries:
            try:
                url = f"https://duckduckgo.com/html/?q={query}"
                response = self.session.get(url, timeout=self.config.get('TIMEOUT', 30))
                if response.status_code == 200:
                    search_results = self._parse_search_results(response.text, 'social')
                    results.extend(search_results[:2])
                    time.sleep(1)
            except Exception as e:
                logger.warning(f"Failed to search YouTube info for {handle}: {str(e)}")
                continue
        
        return results

    def _parse_search_results(self, html_content: str, source_type: str) -> List[SearchResult]:
        """Parse HTML search results and extract relevant information"""
        results = []
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Parse DuckDuckGo results
            if 'duckduckgo.com' in html_content or 'ddg' in html_content:
                results.extend(self._parse_duckduckgo_results(soup, source_type))
            # Parse Bing results
            elif 'bing.com' in html_content:
                results.extend(self._parse_bing_results(soup, source_type))
                
        except Exception as e:
            logger.warning(f"Failed to parse search results: {str(e)}")
        
        return results
    
    def _parse_duckduckgo_results(self, soup: BeautifulSoup, source_type: str) -> List[SearchResult]:
        """Parse DuckDuckGo search results"""
        results = []
        
        # Find result elements (DuckDuckGo HTML structure)
        result_elements = soup.find_all('div', class_='result')
        
        for element in result_elements[:self.config.get('MAX_RESULTS_PER_SOURCE', 10)]:
            try:
                title_elem = element.find('a', class_='result__a')
                snippet_elem = element.find('a', class_='result__snippet')
                
                if title_elem and snippet_elem:
                    title = title_elem.get_text(strip=True)
                    url = title_elem.get('href', '')
                    content = snippet_elem.get_text(strip=True)
                    
                    if title and url and content:
                        result = SearchResult(
                            source=source_type,
                            title=title,
                            url=url,
                            content=content,
                            relevance_score=0.5
                        )
                        results.append(result)
                        
            except Exception as e:
                logger.warning(f"Failed to parse individual result: {str(e)}")
                continue
        
        return results
    
    def _parse_bing_results(self, soup: BeautifulSoup, source_type: str) -> List[SearchResult]:
        """Parse Bing search results"""
        results = []
        
        # Find result elements (Bing HTML structure)
        result_elements = soup.find_all('li', class_='b_algo')
        
        for element in result_elements[:self.config.get('MAX_RESULTS_PER_SOURCE', 10)]:
            try:
                title_elem = element.find('h2')
                if title_elem:
                    title_link = title_elem.find('a')
                    if title_link:
                        title = title_link.get_text(strip=True)
                        url = title_link.get('href', '')
                        
                        # Find snippet
                        snippet_elem = element.find('p')
                        content = snippet_elem.get_text(strip=True) if snippet_elem else ""
                        
                        if title and url:
                            result = SearchResult(
                                source=source_type,
                                title=title,
                                url=url,
                                content=content,
                                relevance_score=0.5
                            )
                            results.append(result)
                            
            except Exception as e:
                logger.warning(f"Failed to parse individual Bing result: {str(e)}")
                continue
        
        return results
    
    def summarize_search_results(self, search_results: Dict[str, List[SearchResult]]) -> Dict[str, str]:
        """
        Summarize search results into a concise format - optimized for speed
        
        Args:
            search_results: Dictionary of search results by category
            
        Returns:
            Dictionary of summarized information by category
        """
        summaries = {}
        
        for category, results in search_results.items():
            if not results:
                if category == 'general':
                    summaries[category] = "General search skipped for faster personalization."
                else:
                    summaries[category] = f"No relevant {category} information found."
                continue
            
            # Quick summary generation for speed
            if category == 'social':
                platforms = set()
                for result in results:
                    if 'github' in result.url.lower():
                        platforms.add('GitHub')
                    elif 'twitter' in result.url.lower():
                        platforms.add('Twitter')
                    elif 'linkedin' in result.url.lower():
                        platforms.add('LinkedIn')
                    elif 'instagram' in result.url.lower():
                        platforms.add('Instagram')
                    elif 'tiktok' in result.url.lower():
                        platforms.add('TikTok')
                    elif 'youtube' in result.url.lower():
                        platforms.add('YouTube')
                
                if platforms:
                    platform_list = ', '.join(sorted(platforms))
                    summaries[category] = f"Found social presence on: {platform_list}. This provides context for personalized recommendations."
                else:
                    summaries[category] = f"Found {len(results)} social media references for personalization context."
            
            elif category == 'location':
                summaries[category] = f"Found {len(results)} local activities and events in your area for relevant suggestions."
            
            elif category == 'activity':
                summaries[category] = f"Found {len(results)} learning resources and guides for your activity interests."
            
            else:
                # Generic summary for any other categories
                summaries[category] = f"Found {len(results)} relevant results for enhanced personalization."
        
        return summaries
    
    def close(self):
        """Clean up resources"""
        if self.session:
            self.session.close()


# Convenience function for easy integration
def perform_background_search(user_profile: UserProfile) -> Dict[str, Any]:
    """
    Perform background search and return summarized results
    
    Args:
        user_profile: User profile information
        
    Returns:
        Dictionary containing raw results and summaries
    """
    search_service = BackgroundSearchService()
    
    try:
        # Perform the search
        raw_results = search_service.search_user_info(user_profile)
        
        # Summarize the results
        summaries = search_service.summarize_search_results(raw_results)
        
        return {
            'raw_results': raw_results,
            'summaries': summaries,
            'total_results': sum(len(results) for results in raw_results.values())
        }
        
    finally:
        search_service.close()


if __name__ == "__main__":
    # Test the search functionality
    test_profile = UserProfile(
        name="John Doe",
        location="San Francisco, CA",
        social_handles={"github": "johndoe", "twitter": "johndoe"},
        activity="learn python programming"
    )
    
    results = perform_background_search(test_profile)
    print(json.dumps(results['summaries'], indent=2))

========================================
# File: /home/jarvis/Documents/GitHub/WhatNowAI/utils/__init__.py
========================================

# Utils package

========================================
# File: /home/jarvis/Documents/GitHub/WhatNowAI/utils/helpers.py
========================================

"""
Utility functions for data processing
"""
import re
from typing import Dict, Any, Optional


def clean_text_for_tts(text: str) -> str:
    """
    Clean text for better TTS pronunciation
    
    Args:
        text: Input text to clean
        
    Returns:
        Cleaned text suitable for TTS
    """
    if not text:
        return ""
    
    # Remove markdown formatting
    text = re.sub(r'\*\*(.*?)\*\*', r'\1', text)  # Remove bold
    text = re.sub(r'\*(.*?)\*', r'\1', text)      # Remove italic
    text = re.sub(r'`(.*?)`', r'\1', text)        # Remove code
    
    # Replace multiple newlines with periods
    text = re.sub(r'\n\n+', '. ', text)
    text = re.sub(r'\n', '. ', text)
    
    # Clean up multiple periods
    text = re.sub(r'\.{2,}', '.', text)
    
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text


def validate_coordinates(latitude: Optional[float], longitude: Optional[float]) -> bool:
    """
    Validate latitude and longitude coordinates
    
    Args:
        latitude: Latitude value
        longitude: Longitude value
        
    Returns:
        True if coordinates are valid, False otherwise
    """
    if latitude is None or longitude is None:
        return False
    
    try:
        lat = float(latitude)
        lon = float(longitude)
        
        # Check if coordinates are in valid range
        if -90 <= lat <= 90 and -180 <= lon <= 180:
            return True
        return False
    except (ValueError, TypeError, OverflowError):
        return False


def sanitize_social_handle(handle: str) -> str:
    """
    Sanitize social media handle
    
    Args:
        handle: Raw social media handle
        
    Returns:
        Cleaned handle without @ symbol
    """
    if not handle:
        return ""
    
    # Remove @ symbol and whitespace
    return handle.replace('@', '').strip()


def format_location_string(location_data: Dict[str, Any]) -> str:
    """
    Format location data into a readable string
    
    Args:
        location_data: Dictionary containing location information
        
    Returns:
        Formatted location string
    """
    if not location_data:
        return "Unknown location"
    
    city = location_data.get('city', 'Unknown')
    country = location_data.get('country', 'Unknown')
    zipcode = location_data.get('zipcode', 'Unknown')
    
    location_str = f"{city}, {country}"
    if zipcode != 'Unknown':
        location_str += f" ({zipcode})"
    
    return location_str


def generate_response_text(name: str, activity: str, location_data: Dict, social_data: Dict, search_summaries: Dict = None) -> str:
    """
    Generate the main response text for user request
    
    Args:
        name: User's name
        activity: User's desired activity
        location_data: Location information
        social_data: Social media information
        search_summaries: Background search summaries (optional)
        
    Returns:
        Formatted response text
    """
    location_str = format_location_string(location_data)
    
    result = f"Great news, {name}! I've analyzed your request to {activity} in {location_str}.\n\n"
    
    # Add background search context if available
    if search_summaries:
        result += "Based on my background research, here's what I found:\n\n"
        
        # Add location insights
        if search_summaries.get('location') and 'No relevant' not in search_summaries['location']:
            result += f"📍 **Location Insights**: {search_summaries['location']}\n\n"
        
        # Add activity insights
        if search_summaries.get('activity') and 'No relevant' not in search_summaries['activity']:
            result += f"🎯 **Activity Insights**: {search_summaries['activity']}\n\n"
        
        # Add social media insights
        if search_summaries.get('social') and 'No relevant' not in search_summaries['social']:
            result += f"👥 **Social Context**: {search_summaries['social']}\n\n"
    
    # Add social media context if provided
    twitter_handle = sanitize_social_handle(social_data.get('twitter', ''))
    instagram_handle = sanitize_social_handle(social_data.get('instagram', ''))
    
    if twitter_handle or instagram_handle:
        result += f"I noticed you're active on social media"
        if twitter_handle and instagram_handle:
            result += f" (@{twitter_handle} on X, @{instagram_handle} on Instagram)"
        elif twitter_handle:
            result += f" (@{twitter_handle} on X)"
        elif instagram_handle:
            result += f" (@{instagram_handle} on Instagram)"
        result += f". This gives me additional context about your interests!\n\n"
    
    # Add personalized recommendations based on search results
    result += f"Here are my personalized recommendations for you:\n\n"
    
    # Add location-specific suggestions
    country = location_data.get('country', 'Unknown')
    result += f"1. **Start with the basics**: Break down '{activity}' into smaller, manageable steps\n" \
              f"2. **Local resources**: Research what's available in {country} to help with {activity}\n" \
              f"3. **Requirements check**: Look for any location-specific requirements or regulations\n" \
              f"4. **Timeline planning**: Set realistic milestones and deadlines\n" \
              f"5. **Community connection**: Find local groups or communities interested in {activity}\n\n"
    
    # Add social media suggestions if applicable
    if twitter_handle or instagram_handle:
        result += f"6. **Social sharing**: Document your {activity} journey to connect with like-minded people\n" \
                 f"7. **Follow experts**: Connect with relevant accounts and hashtags related to {activity}\n\n"
    
    # Add search-informed suggestions
    if search_summaries and any('No relevant' not in summary for summary in search_summaries.values()):
        result += f"8. **Research-based insights**: Based on my background research, focus on the most promising approaches mentioned above\n\n"
    
    # Add precise location info if available
    latitude = location_data.get('latitude')
    longitude = location_data.get('longitude')
    if validate_coordinates(latitude, longitude):
        result += f"📍 **Precise location advantage**: With your exact coordinates ({latitude:.4f}, {longitude:.4f}), " \
                 f"I can provide hyper-local recommendations.\n\n"
    
    result += f"💡 **Next steps**: Would you like me to create a detailed, step-by-step action plan " \
              f"tailored specifically to your location and background?"
    
    return result

========================================
# File: /home/jarvis/Documents/GitHub/WhatNowAI/config/settings.py
========================================

"""
Application configuration
"""
import os
from pathlib import Path

# Base directory
BASE_DIR = Path(__file__).parent.parent

def load_secrets():
    """
    Load API keys and secrets from secrets.txt file and fall back to environment variables if not found.

    Returns:
        dict: A dictionary containing secrets.
    """
    vars = ["OPENAI_API_KEY", "TICKETMASTER_CONSUMER_KEY", "TICKETMASTER_CONSUMER_SECRET"]
    secrets = {}
    secrets_file = BASE_DIR / 'secrets.txt'

    # Load from secrets.txt if it exists
    if secrets_file.exists():
        try:
            with open(secrets_file, 'r') as f:
                for line in f:
                    line = line.strip()
                    if line and '=' in line and not line.startswith('#'):
                        key, value = line.split('=', 1)
                        secrets[key.strip()] = value.strip()
        except Exception as e:
            print(f"Warning: Could not load secrets.txt: {e}")

    # Fallback to environment variables for missing keys
    for var in vars:
        if var not in secrets:
            env_val = os.getenv(var)
            if env_val:
                secrets[var] = env_val

    return secrets

# Load secrets from file
_secrets = load_secrets()

# Audio configuration
AUDIO_DIR = BASE_DIR / 'static' / 'audio'
DEFAULT_TTS_VOICE = "en-US-JennyNeural"
AUDIO_CLEANUP_HOURS = 24

# Flask configuration
FLASK_CONFIG = {
    'DEBUG': True,
    'HOST': '0.0.0.0',
    'PORT': 5002
}

# Logging configuration
LOGGING_CONFIG = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'default': {
            'format': '[%(asctime)s] %(levelname)s in %(module)s: %(message)s',
        }
    },
    'handlers': {
        'console': {
            'class': 'logging.StreamHandler',
            'level': 'INFO',
            'formatter': 'default',
            'stream': 'ext://sys.stdout'
        }
    },
    'root': {
        'level': 'INFO',
        'handlers': ['console']
    }
}

# Geocoding configuration
GEOCODING_CONFIG = {
    'USER_AGENT': 'WhatNowAI/1.0',
    'TIMEOUT': 10
}

# API Keys from secrets.txt file and environment variables (env vars take precedence)
TICKETMASTER_API_KEY = os.getenv('TICKETMASTER_API_KEY', _secrets.get('TICKETMASTER_CONSUMER_KEY', ''))
ALLEVENTS_API_KEY = os.getenv('ALLEVENTS_API_KEY', _secrets.get('ALLEVENTS_API_KEY', ''))
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', _secrets.get('OPENAI_API_KEY', ''))

# Optional API keys for advanced features
HUGGINGFACE_TOKEN = os.getenv('HUGGINGFACE_TOKEN', _secrets.get('HUGGINGFACE_TOKEN', ''))

# Debug function to check API key status
def check_api_keys():
    """Check which API keys are available"""
    keys_status = {
        'TICKETMASTER_API_KEY': 'SET' if TICKETMASTER_API_KEY else 'NOT SET',
        'ALLEVENTS_API_KEY': 'SET' if ALLEVENTS_API_KEY else 'NOT SET',
        'OPENAI_API_KEY': 'SET' if OPENAI_API_KEY else 'NOT SET',
        'HUGGINGFACE_TOKEN': 'SET' if HUGGINGFACE_TOKEN else 'NOT SET'
    }
    
    print("🔑 API Keys Status:")
    for key, status in keys_status.items():
        print(f"   {key}: {status}")
        if status == 'SET' and key in ['TICKETMASTER_API_KEY', 'ALLEVENTS_API_KEY']:
            print(f"   {key} value: {globals()[key][:10]}...")
    
    return keys_status

# Search configuration
SEARCH_CONFIG = {
    'MAX_RESULTS_PER_SOURCE': 3,  # Reduced for faster searches
    'TIMEOUT': 5,  # Reduced timeout per request
    'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'SOCIAL_PLATFORMS': ['twitter', 'linkedin', 'instagram', 'github', 'tiktok', 'youtube'],
    'MAX_CONCURRENT_REQUESTS': 3,  # Reduced for faster processing
    'FOCUS_ON_USER': True,  # Only search for the specific user, not celebrities/general info
    'LOCAL_ACTIVITY_SEARCH': True,  # Include local activity searches
    'SEARCH_TIMEOUT': 10,  # Total search timeout in seconds
    'SKIP_GENERAL_SEARCH': True  # Skip general name searches
}

# Ticketmaster API configuration
TICKETMASTER_CONFIG = {
    'BASE_URL': 'https://app.ticketmaster.com/discovery/v2',
    'SEARCH_RADIUS': 50,  # miles
    'MAX_EVENTS': 20,
    'DEFAULT_CATEGORIES': ['music', 'sports', 'arts', 'miscellaneous'],
    'TIMEOUT': 10,
    'MIN_RELEVANCE_SCORE': 0.15  # Minimum relevance score for event filtering
}

# AllEvents API configuration
ALLEVENTS_CONFIG = {
    'BASE_URL': 'https://allevents.developer.azure-api.net/api',
    'SEARCH_RADIUS': 50,  # km
    'MAX_EVENTS': 30,
    'TIMEOUT': 10,
    'MIN_RELEVANCE_SCORE': 0.15  # Minimum relevance score for event filtering
}

# Map configuration
MAP_CONFIG = {
    'DEFAULT_ZOOM': 12,
    'MAX_MARKERS': 50,
    'TILE_SERVER': 'https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png',
    'ATTRIBUTION': '&copy; <a href="https://www.openstreetmap.org/copyright">OpenStreetMap</a> contributors'
}

========================================
# File: /home/jarvis/Documents/GitHub/WhatNowAI/config/__init__.py
========================================

# Config package

========================================
# File: /home/jarvis/Documents/GitHub/WhatNowAI/services/allevents_service.py
========================================

"""
AllEvents API service for event discovery

This module integrates with the AllEvents API to find local events
and activities based on user location and interests. Includes advanced personalization,
event categorization, filtering, and comprehensive error handling for intelligent
event discovery that adapts to user preferences and behavioral patterns.
"""

import requests
import logging
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from datetime import datetime, timedelta
import json
import re

logger = logging.getLogger(__name__)


class AllEventsService:
    """Service for intelligent event discovery from AllEvents API with personalization"""
    
    def __init__(self, api_key: str, config: Dict[str, Any]):
        """
        Initialize AllEvents service
        
        Args:
            api_key: AllEvents API key
            config: Configuration dictionary
        """
        self.api_key = api_key
        self.config = config
        self.base_url = config.get('BASE_URL', 'https://allevents.developer.azure-api.net/api')
        self.session = requests.Session()
        
        # Set default headers for API
        self.session.headers.update({
            'Ocp-Apim-Subscription-Key': self.api_key,
            'Content-Type': 'application/json'
        })
        
    def search_events(self, location: Dict[str, Any], user_interests: List[str] = None, 
                     user_activity: str = "", personalization_data: Dict[str, Any] = None,
                     user_profile: Any = None) -> List[Any]:
        """
        Search for events near a location based on user interests and enhanced personalization
        
        Args:
            location: Dictionary with latitude, longitude, city, country
            user_interests: List of user interest categories
            user_activity: What the user wants to do
            personalization_data: Enhanced personalization data from background search
            user_profile: Enhanced user profile from user_profiling_service
            
        Returns:
            List of Event objects (using same structure as Ticketmaster) filtered and ranked by AI and user preferences
        """
        if not self.api_key:
            logger.warning("AllEvents API key not provided")
            return []
        
        latitude = location.get('latitude')
        longitude = location.get('longitude')
        city = location.get('city', '')
        country = location.get('country', '')
        
        # Convert to float if needed
        try:
            if latitude is not None:
                latitude = float(latitude)
            if longitude is not None:
                longitude = float(longitude)
        except (ValueError, TypeError) as e:
            logger.error(f"Failed to convert coordinates to float in AllEvents service: {e}")
            return []
        
        if not latitude or not longitude:
            logger.warning("Location coordinates not provided or invalid for AllEvents")
            return []
        
        logger.info(f"Searching AllEvents with AI-enhanced personalization: "
                   f"location=({latitude},{longitude}), "
                   f"basic_interests={user_interests}, "
                   f"activity='{user_activity}', "
                   f"has_personalization_data={bool(personalization_data)}, "
                   f"has_profile={bool(user_profile)}")
        
        events = []
        
        try:
            # Build search parameters
            params = {
                'latitude': latitude,
                'longitude': longitude,
                'radius': 50,  # 50km radius
                'limit': 50,   # Get more events for better filtering
                'sort': 'relevance'
            }
            
            # Add city/location if available
            if city:
                params['city'] = city
            
            # Add date range (next 30 days)
            today = datetime.now()
            end_date = today + timedelta(days=30)
            params['start_date'] = today.strftime('%Y-%m-%d')
            params['end_date'] = end_date.strftime('%Y-%m-%d')
            
            # Add categories based on user interests and activity
            categories = self._map_interests_to_categories(user_interests, user_activity, user_profile)
            if categories:
                params['categories'] = ','.join(categories)
            
            logger.info(f"AllEvents API request params: {params}")
            
            # Make API request
            response = self.session.get(
                f"{self.base_url}/events/search",
                params=params,
                timeout=self.config.get('TIMEOUT', 10)
            )
            
            if response.status_code == 200:
                data = response.json()
                raw_events = data.get('events', [])
                
                logger.info(f"AllEvents API returned {len(raw_events)} raw events")
                
                # Convert to our Event format
                for event_data in raw_events:
                    try:
                        event = self._convert_to_event_format(event_data, location)
                        if event:
                            events.append(event)
                    except Exception as e:
                        logger.warning(f"Failed to convert AllEvents event: {e}")
                        continue
                
                logger.info(f"Successfully converted {len(events)} AllEvents events")
                
                # Apply AI-powered filtering and ranking
                if user_profile and events:
                    events = self._apply_ai_filtering(events, user_profile, user_activity, personalization_data)
                
            else:
                logger.error(f"AllEvents API error: {response.status_code} - {response.text}")
                
        except requests.exceptions.RequestException as e:
            logger.error(f"AllEvents API request failed: {e}")
        except Exception as e:
            logger.error(f"Unexpected error in AllEvents search: {e}")
        
        return events
    
    def _map_interests_to_categories(self, user_interests: List[str], user_activity: str, user_profile: Any) -> List[str]:
        """Map user interests and activities to AllEvents categories"""
        category_mapping = {
            # Music and Entertainment
            'music': ['music', 'concerts', 'festivals'],
            'concerts': ['music', 'concerts'],
            'festivals': ['festivals', 'music', 'food'],
            'nightlife': ['nightlife', 'parties'],
            'comedy': ['comedy', 'entertainment'],
            'theatre': ['theatre', 'performing-arts'],
            'entertainment': ['entertainment', 'performing-arts'],
            
            # Sports and Fitness
            'sports': ['sports', 'fitness'],
            'fitness': ['fitness', 'sports', 'health'],
            'running': ['sports', 'fitness', 'running'],
            'yoga': ['fitness', 'health', 'wellness'],
            'gym': ['fitness', 'health'],
            
            # Arts and Culture
            'art': ['art', 'exhibitions', 'culture'],
            'museums': ['art', 'culture', 'exhibitions'],
            'exhibitions': ['art', 'exhibitions', 'culture'],
            'culture': ['culture', 'art', 'history'],
            'history': ['culture', 'history', 'education'],
            
            # Food and Drink
            'food': ['food', 'restaurants', 'culinary'],
            'restaurants': ['food', 'culinary'],
            'cooking': ['food', 'culinary', 'workshops'],
            'wine': ['food', 'wine', 'culinary'],
            'beer': ['food', 'beer', 'nightlife'],
            
            # Technology and Business
            'technology': ['technology', 'business', 'conferences'],
            'tech': ['technology', 'business'],
            'business': ['business', 'networking', 'conferences'],
            'networking': ['business', 'networking', 'professional'],
            'conferences': ['conferences', 'business', 'education'],
            
            # Outdoor and Nature
            'outdoor': ['outdoor', 'nature', 'adventure'],
            'hiking': ['outdoor', 'nature', 'sports'],
            'nature': ['nature', 'outdoor', 'environment'],
            'adventure': ['adventure', 'outdoor', 'sports'],
            'cycling': ['sports', 'outdoor', 'cycling'],
            
            # Family and Kids
            'family': ['family', 'kids', 'children'],
            'kids': ['kids', 'family', 'children'],
            'children': ['children', 'family', 'kids'],
            
            # Education and Learning
            'education': ['education', 'workshops', 'learning'],
            'workshops': ['workshops', 'education', 'learning'],
            'learning': ['education', 'workshops', 'personal-development'],
            'books': ['education', 'literature', 'culture'],
            
            # Health and Wellness
            'health': ['health', 'wellness', 'fitness'],
            'wellness': ['wellness', 'health', 'mindfulness'],
            'meditation': ['wellness', 'mindfulness', 'health'],
            
            # Community and Social
            'community': ['community', 'social', 'networking'],
            'volunteering': ['community', 'charity', 'social'],
            'charity': ['charity', 'community', 'volunteering']
        }
        
        categories = set()
        
        # Add categories based on user interests
        if user_interests:
            for interest in user_interests:
                interest_lower = interest.lower()
                if interest_lower in category_mapping:
                    categories.update(category_mapping[interest_lower])
        
        # Add categories based on activity text
        if user_activity:
            activity_lower = user_activity.lower()
            for keyword, cats in category_mapping.items():
                if keyword in activity_lower:
                    categories.update(cats)
        
        # Add categories based on enhanced user profile
        if user_profile and hasattr(user_profile, 'get'):
            profile_interests = user_profile.get('interests', [])
            for interest in profile_interests:
                if isinstance(interest, dict):
                    interest_text = interest.get('category', '').lower()
                elif hasattr(interest, 'category'):
                    interest_text = interest.category.lower()
                else:
                    interest_text = str(interest).lower()
                
                if interest_text in category_mapping:
                    categories.update(category_mapping[interest_text])
        
        return list(categories)
    
    def _convert_to_event_format(self, event_data: Dict[str, Any], location: Dict[str, Any]) -> Any:
        """Convert AllEvents API response to our standard Event format"""
        try:
            # Import Event class from ticketmaster_service to maintain consistency
            from services.ticketmaster_service import Event
            
            # Extract basic event information
            event_id = str(event_data.get('id', ''))
            name = event_data.get('title', '').strip()
            url = event_data.get('url', '')
            
            # Parse date and time
            start_date = event_data.get('start_date', '')
            start_time = event_data.get('start_time', '')
            
            # Format date and time
            date_str = start_date if start_date else 'TBA'
            time_str = start_time if start_time else 'TBA'
            
            # Venue information
            venue_info = event_data.get('venue', {})
            venue_name = venue_info.get('name', 'TBA')
            venue_address = venue_info.get('address', '')
            
            # Location coordinates
            venue_lat = venue_info.get('latitude')
            venue_lon = venue_info.get('longitude')
            
            # Use provided location as fallback
            if not venue_lat or not venue_lon:
                venue_lat = location.get('latitude', 0.0)
                venue_lon = location.get('longitude', 0.0)
            
            # Convert to float
            try:
                venue_lat = float(venue_lat) if venue_lat else 0.0
                venue_lon = float(venue_lon) if venue_lon else 0.0
            except (ValueError, TypeError):
                venue_lat = float(location.get('latitude', 0.0))
                venue_lon = float(location.get('longitude', 0.0))
            
            # Category mapping
            category = event_data.get('category', 'Other')
            subcategory = event_data.get('subcategory', '')
            
            # Image URL
            image_url = event_data.get('image_url', '')
            if isinstance(event_data.get('images'), list) and event_data['images']:
                image_url = event_data['images'][0].get('url', image_url)
            
            # Description
            description = event_data.get('description', '')
            
            # Note: Removing price information as requested
            # price_min = None
            # price_max = None
            
            # Create Event object
            event = Event(
                id=f"allevents_{event_id}",  # Prefix to distinguish from Ticketmaster
                name=name,
                url=url,
                date=date_str,
                time=time_str,
                venue=venue_name,
                address=venue_address,
                city=location.get('city', ''),
                latitude=venue_lat,
                longitude=venue_lon,
                category=category,
                subcategory=subcategory,
                price_min=None,  # Removed as requested
                price_max=None,  # Removed as requested
                image_url=image_url,
                description=description
            )
            
            return event
            
        except Exception as e:
            logger.error(f"Error converting AllEvents event to standard format: {e}")
            logger.error(f"Event data: {event_data}")
            return None
    
    def _apply_ai_filtering(self, events: List[Any], user_profile: Any, user_activity: str, 
                          personalization_data: Dict[str, Any]) -> List[Any]:
        """Apply AI-powered filtering and ranking to events"""
        try:
            # Import AI filtering from ticketmaster service for consistency
            from services.ticketmaster_service import TicketmasterService
            
            # Create a temporary instance to use its AI filtering methods
            # We'll use the same AI logic for consistency
            temp_service = TicketmasterService("", {})
            
            if hasattr(temp_service, '_apply_ai_filtering_and_ranking'):
                return temp_service._apply_ai_filtering_and_ranking(
                    events, user_profile, user_activity, personalization_data
                )
            else:
                # Fallback: simple relevance scoring
                for event in events:
                    event.relevance_score = self._calculate_simple_relevance(
                        event, user_profile, user_activity
                    )
                
                # Sort by relevance score
                events.sort(key=lambda x: getattr(x, 'relevance_score', 0), reverse=True)
                
                return events[:20]  # Return top 20 events
                
        except Exception as e:
            logger.warning(f"AI filtering failed for AllEvents, using simple filtering: {e}")
            return events[:20]  # Return first 20 events as fallback
    
    def _calculate_simple_relevance(self, event: Any, user_profile: Any, user_activity: str) -> float:
        """Calculate simple relevance score for an event"""
        score = 0.5  # Base score
        
        try:
            # Check if event category matches user interests
            if user_profile and hasattr(user_profile, 'get'):
                interests = user_profile.get('interests', [])
                event_category = getattr(event, 'category', '').lower()
                
                for interest in interests:
                    if isinstance(interest, dict):
                        interest_text = interest.get('category', '').lower()
                    elif hasattr(interest, 'category'):
                        interest_text = interest.category.lower()
                    else:
                        interest_text = str(interest).lower()
                    
                    if interest_text in event_category or event_category in interest_text:
                        score += 0.3
                        break
            
            # Check if event name/description matches user activity
            if user_activity:
                event_text = f"{getattr(event, 'name', '')} {getattr(event, 'description', '')}".lower()
                activity_words = user_activity.lower().split()
                
                for word in activity_words:
                    if len(word) > 2 and word in event_text:
                        score += 0.1
            
            # Prefer events with images
            if getattr(event, 'image_url', ''):
                score += 0.1
            
            # Prefer events with detailed descriptions
            if len(getattr(event, 'description', '')) > 50:
                score += 0.1
                
        except Exception as e:
            logger.warning(f"Error calculating relevance score: {e}")
        
        return min(score, 1.0)  # Cap at 1.0

========================================
# File: /home/jarvis/Documents/GitHub/WhatNowAI/services/geocoding_service.py
========================================

"""
Geocoding service for location handling

This module provides location services using OpenStreetMap's Nominatim API,
including reverse geocoding for converting coordinates to address information.
Privacy-focused implementation with configurable timeouts and user agents.
"""
import requests
import logging
from typing import Dict, Optional

logger = logging.getLogger(__name__)


class GeocodingService:
    """Service for handling geocoding operations"""
    
    def __init__(self, user_agent: str = "WhatNowAI/1.0"):
        """
        Initialize geocoding service
        
        Args:
            user_agent: User agent string for API requests
        """
        self.user_agent = user_agent
        self.base_url = "https://nominatim.openstreetmap.org/reverse"
    
    def reverse_geocode(self, latitude: float, longitude: float) -> Optional[Dict]:
        """
        Reverse geocode coordinates to address information
        
        Args:
            latitude: Latitude coordinate
            longitude: Longitude coordinate
            
        Returns:
            Dictionary with location information or None if failed
        """
        try:
            params = {
                'format': 'json',
                'lat': latitude,
                'lon': longitude,
                'zoom': 18,
                'addressdetails': 1
            }
            
            headers = {
                'User-Agent': self.user_agent
            }
            
            response = requests.get(
                self.base_url, 
                params=params, 
                headers=headers, 
                timeout=10
            )
            
            if response.status_code == 200:
                geo_data = response.json()
                return self._extract_location_info(geo_data, latitude, longitude)
            else:
                logger.error(f"Geocoding API returned status {response.status_code}")
                return None
                
        except requests.RequestException as e:
            logger.error(f"Geocoding request error: {e}")
            return None
        except Exception as e:
            logger.error(f"Geocoding error: {e}")
            return None
    
    def _extract_location_info(self, geo_data: Dict, latitude: float, longitude: float) -> Dict:
        """
        Extract relevant location information from geocoding response
        
        Args:
            geo_data: Raw geocoding response
            latitude: Original latitude
            longitude: Original longitude
            
        Returns:
            Cleaned location information dictionary
        """
        address = geo_data.get('address', {})
        
        # Extract city with fallback options
        city = (address.get('city') or 
                address.get('town') or 
                address.get('village') or 
                address.get('hamlet') or 
                'Unknown')
        
        return {
            'country': address.get('country', 'Unknown'),
            'city': city,
            'zipcode': address.get('postcode', 'Unknown'),
            'latitude': latitude,
            'longitude': longitude,
            'full_address': geo_data.get('display_name', 'Unknown')
        }

========================================
# File: /home/jarvis/Documents/GitHub/WhatNowAI/services/unified_events_service.py
========================================

"""
Unified Events Service

This service coordinates multiple event APIs (Ticketmaster, AllEvents, etc.),
applies AI-powered filtering and ranking, and provides a unified interface
for event discovery with advanced personalization.
"""

import logging
import asyncio
from typing import Dict, List, Optional, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
import json

logger = logging.getLogger(__name__)


@dataclass
class EventSource:
    """Metadata about an event source"""
    name: str
    priority: float  # Higher priority = more trusted source
    reliability: float  # 0-1 reliability score
    coverage: str  # geographical coverage description


class UnifiedEventsService:
    """
    Unified service that coordinates multiple event sources and applies AI evaluation
    """
    
    def __init__(self, ticketmaster_service=None, allevents_service=None, ai_service=None):
        """
        Initialize the unified events service
        
        Args:
            ticketmaster_service: Ticketmaster API service instance
            allevents_service: AllEvents API service instance  
            ai_service: AI service for intelligent filtering (optional)
        """
        self.ticketmaster_service = ticketmaster_service
        self.allevents_service = allevents_service
        self.ai_service = ai_service
        
        # Define event sources with metadata
        self.event_sources = {
            'ticketmaster': EventSource(
                name='Ticketmaster',
                priority=0.9,
                reliability=0.95,
                coverage='Global, strong in North America and Europe'
            ),
            'allevents': EventSource(
                name='AllEvents',
                priority=0.7,
                reliability=0.8,
                coverage='Global, diverse event types'
            )
        }
        
        # Configuration
        self.max_events_per_source = 50
        self.final_event_limit = 30
        self.ai_confidence_threshold = 0.6
        
    def search_unified_events(self, location: Dict[str, Any], user_interests: List[str] = None,
                            user_activity: str = "", personalization_data: Dict[str, Any] = None,
                            user_profile: Any = None) -> Dict[str, Any]:
        """
        Search for events from all available sources, group them, and apply AI evaluation
        
        Args:
            location: Dictionary with latitude, longitude, city, country
            user_interests: List of user interest categories
            user_activity: What the user wants to do
            personalization_data: Enhanced personalization data from background search
            user_profile: Enhanced user profile from user_profiling_service
            
        Returns:
            Dictionary containing:
            - events: List of AI-evaluated and ranked events
            - sources_used: List of sources that were queried
            - ai_insights: AI analysis of user preferences and event matching
            - total_found: Total events found before filtering
            - total_returned: Number of events after AI filtering
        """
        logger.info(f"Starting unified event search for location: {location}")
        logger.info(f"User activity: '{user_activity}', interests: {user_interests}")
        
        all_events = []
        sources_used = []
        search_results = {}
        
        # Collect events from all available sources in parallel
        with ThreadPoolExecutor(max_workers=3) as executor:
            future_to_source = {}
            
            # Submit search tasks for each available service
            if self.ticketmaster_service:
                future = executor.submit(
                    self._search_source_safely,
                    'ticketmaster',
                    self.ticketmaster_service,
                    location, user_interests, user_activity, personalization_data, user_profile
                )
                future_to_source[future] = 'ticketmaster'
            
            if self.allevents_service:
                future = executor.submit(
                    self._search_source_safely,
                    'allevents',
                    self.allevents_service,
                    location, user_interests, user_activity, personalization_data, user_profile
                )
                future_to_source[future] = 'allevents'
            
            # Collect results as they complete
            for future in as_completed(future_to_source):
                source_name = future_to_source[future]
                try:
                    events = future.result(timeout=30)  # 30 second timeout per source
                    if events:
                        all_events.extend(events)
                        sources_used.append(source_name)
                        search_results[source_name] = len(events)
                        logger.info(f"✅ {source_name}: Found {len(events)} events")
                    else:
                        logger.info(f"⚠️ {source_name}: No events found")
                        search_results[source_name] = 0
                except Exception as e:
                    logger.error(f"❌ {source_name}: Search failed - {e}")
                    search_results[source_name] = 0
        
        logger.info(f"Total events collected from all sources: {len(all_events)}")
        
        # Remove duplicates and group similar events
        deduplicated_events = self._deduplicate_events(all_events)
        logger.info(f"Events after deduplication: {len(deduplicated_events)}")
        
        # Apply AI-powered evaluation and ranking
        ai_results = self._apply_ai_evaluation(
            deduplicated_events, user_profile, user_activity, personalization_data
        )
        
        # Final filtering and ranking
        final_events = self._final_ranking_and_filtering(
            ai_results.get('ranked_events', deduplicated_events),
            user_profile,
            user_activity
        )
        
        # Remove cost information as requested
        final_events = self._remove_cost_information(final_events)
        
        logger.info(f"Final events returned after AI evaluation: {len(final_events)}")
        
        return {
            'events': final_events,
            'sources_used': sources_used,
            'search_results': search_results,
            'ai_insights': ai_results.get('insights', {}),
            'total_found': len(all_events),
            'total_returned': len(final_events),
            'deduplication_saved': len(all_events) - len(deduplicated_events)
        }
    
    def _search_source_safely(self, source_name: str, service: Any, *args) -> List[Any]:
        """Safely search a single event source with error handling"""
        try:
            logger.info(f"🔍 Searching {source_name}...")
            events = service.search_events(*args)
            return events if events else []
        except Exception as e:
            logger.error(f"Error searching {source_name}: {e}")
            return []
    
    def _deduplicate_events(self, events: List[Any]) -> List[Any]:
        """
        Remove duplicate events and group similar ones
        Uses event name, venue, and date for deduplication
        """
        seen_events = {}
        deduplicated = []
        
        for event in events:
            # Create a unique key for the event
            event_key = self._create_event_key(event)
            
            if event_key not in seen_events:
                seen_events[event_key] = event
                deduplicated.append(event)
            else:
                # If we find a duplicate, keep the one with higher reliability
                existing_event = seen_events[event_key]
                if self._should_replace_event(existing_event, event):
                    # Replace the existing event
                    deduplicated.remove(existing_event)
                    deduplicated.append(event)
                    seen_events[event_key] = event
        
        return deduplicated
    
    def _create_event_key(self, event: Any) -> str:
        """Create a unique key for event deduplication"""
        try:
            name = getattr(event, 'name', '').lower().strip()
            venue = getattr(event, 'venue', '').lower().strip()
            date = getattr(event, 'date', '').strip()
            
            # Normalize the name (remove common variations)
            name = self._normalize_event_name(name)
            
            return f"{name}|{venue}|{date}"
        except Exception as e:
            logger.warning(f"Error creating event key: {e}")
            return f"unknown_{id(event)}"
    
    def _normalize_event_name(self, name: str) -> str:
        """Normalize event name for better deduplication"""
        import re
        
        # Remove common prefixes/suffixes and normalize
        name = re.sub(r'\\b(the|a|an)\\b', '', name, flags=re.IGNORECASE)
        name = re.sub(r'[^a-z0-9\\s]', '', name)
        name = re.sub(r'\\s+', ' ', name).strip()
        
        return name
    
    def _should_replace_event(self, existing: Any, new: Any) -> bool:
        """Determine if a new event should replace an existing duplicate"""
        try:
            # Prefer events with more information
            existing_score = self._calculate_completeness_score(existing)
            new_score = self._calculate_completeness_score(new)
            
            return new_score > existing_score
        except Exception:
            return False
    
    def _calculate_completeness_score(self, event: Any) -> float:
        """Calculate how complete an event's information is"""
        score = 0.0
        
        # Check various fields for completeness
        if getattr(event, 'name', ''):
            score += 1.0
        if getattr(event, 'description', ''):
            score += 1.0
        if getattr(event, 'image_url', ''):
            score += 0.5
        if getattr(event, 'venue', '') != 'TBA':
            score += 0.5
        if getattr(event, 'address', ''):
            score += 0.5
        if getattr(event, 'time', '') != 'TBA':
            score += 0.5
        if getattr(event, 'url', ''):
            score += 0.5
        
        # Prefer events from higher reliability sources
        event_id = getattr(event, 'id', '')
        if 'ticketmaster' in event_id:
            score += 0.5  # Ticketmaster bonus
        
        return score
    
    def _apply_ai_evaluation(self, events: List[Any], user_profile: Any, 
                           user_activity: str, personalization_data: Dict[str, Any]) -> Dict[str, Any]:
        """Apply AI-powered evaluation and ranking to events"""
        try:
            # If we have an AI service, use it for advanced evaluation
            if self.ai_service:
                return self._advanced_ai_evaluation(events, user_profile, user_activity, personalization_data)
            else:
                # Fallback to rule-based evaluation
                return self._rule_based_evaluation(events, user_profile, user_activity, personalization_data)
        except Exception as e:
            logger.error(f"AI evaluation failed, using fallback: {e}")
            return {
                'ranked_events': events,
                'insights': {'error': str(e), 'method': 'fallback'}
            }
    
    def _rule_based_evaluation(self, events: List[Any], user_profile: Any,
                             user_activity: str, personalization_data: Dict[str, Any]) -> Dict[str, Any]:
        """Rule-based evaluation and ranking when AI service is not available"""
        
        logger.info("Applying rule-based event evaluation")
        
        for event in events:
            score = 0.5  # Base score
            factors = {}
            
            # Activity matching
            if user_activity:
                activity_score = self._calculate_activity_match(event, user_activity)
                score += activity_score * 0.3
                factors['activity_match'] = activity_score
            
            # Interest matching
            if user_profile:
                interest_score = self._calculate_interest_match(event, user_profile)
                score += interest_score * 0.3
                factors['interest_match'] = interest_score
            
            # Time relevance (prefer closer dates)
            time_score = self._calculate_time_relevance(event)
            score += time_score * 0.2
            factors['time_relevance'] = time_score
            
            # Completeness bonus
            completeness_score = self._calculate_completeness_score(event) / 5.0
            score += completeness_score * 0.2
            factors['completeness'] = completeness_score
            
            # Set the relevance score
            event.relevance_score = min(score, 1.0)
            event.personalization_factors = factors
            event.recommendation_reason = self._generate_recommendation_reason(factors, user_activity)
        
        # Sort by relevance score
        ranked_events = sorted(events, key=lambda x: getattr(x, 'relevance_score', 0), reverse=True)
        
        insights = {
            'method': 'rule_based',
            'total_evaluated': len(events),
            'avg_score': sum(getattr(e, 'relevance_score', 0) for e in events) / len(events) if events else 0,
            'top_factors': ['activity_match', 'interest_match', 'time_relevance', 'completeness']
        }
        
        return {
            'ranked_events': ranked_events,
            'insights': insights
        }
    
    def _calculate_activity_match(self, event: Any, user_activity: str) -> float:
        """Calculate how well an event matches the user's stated activity"""
        try:
            event_text = f"{getattr(event, 'name', '')} {getattr(event, 'description', '')} {getattr(event, 'category', '')}".lower()
            activity_lower = user_activity.lower()
            
            # Simple keyword matching
            activity_words = [word for word in activity_lower.split() if len(word) > 2]
            matches = sum(1 for word in activity_words if word in event_text)
            
            if not activity_words:
                return 0.0
            
            return min(matches / len(activity_words), 1.0)
        except Exception:
            return 0.0
    
    def _calculate_interest_match(self, event: Any, user_profile: Any) -> float:
        """Calculate how well an event matches the user's interests"""
        try:
            if not user_profile or not hasattr(user_profile, 'get'):
                return 0.0
            
            interests = user_profile.get('interests', [])
            if not interests:
                return 0.0
            
            event_category = getattr(event, 'category', '').lower()
            event_name = getattr(event, 'name', '').lower()
            
            matches = 0
            for interest in interests[:10]:  # Check top 10 interests
                if isinstance(interest, dict):
                    interest_text = interest.get('category', '').lower()
                    keywords = interest.get('keywords', [])
                elif hasattr(interest, 'category'):
                    interest_text = interest.category.lower()
                    keywords = getattr(interest, 'keywords', [])
                else:
                    interest_text = str(interest).lower()
                    keywords = []
                
                # Check category match
                if interest_text in event_category or event_category in interest_text:
                    matches += 1
                
                # Check keyword matches
                for keyword in keywords[:5]:  # Check top 5 keywords
                    if keyword.lower() in event_name:
                        matches += 0.5
            
            return min(matches / len(interests), 1.0)
        except Exception:
            return 0.0
    
    def _calculate_time_relevance(self, event: Any) -> float:
        """Calculate time relevance (prefer events happening soon)"""
        try:
            from datetime import datetime, timedelta
            
            event_date = getattr(event, 'date', '')
            if not event_date or event_date == 'TBA':
                return 0.3  # Neutral score for unknown dates
            
            # Try to parse the date
            try:
                # Assuming date format is YYYY-MM-DD or similar
                event_dt = datetime.strptime(event_date[:10], '%Y-%m-%d')
                now = datetime.now()
                days_diff = (event_dt - now).days
                
                if days_diff < 0:
                    return 0.1  # Past event
                elif days_diff <= 7:
                    return 1.0  # This week
                elif days_diff <= 14:
                    return 0.8  # Next week
                elif days_diff <= 30:
                    return 0.6  # This month
                else:
                    return 0.4  # Future
            except ValueError:
                return 0.3  # Can't parse date
                
        except Exception:
            return 0.3
    
    def _generate_recommendation_reason(self, factors: Dict[str, float], user_activity: str) -> str:
        """Generate a human-readable recommendation reason"""
        reasons = []
        
        if factors.get('activity_match', 0) > 0.7:
            reasons.append(f"closely matches your interest in '{user_activity}'")
        elif factors.get('activity_match', 0) > 0.4:
            reasons.append(f"relates to your interest in '{user_activity}'")
        
        if factors.get('interest_match', 0) > 0.7:
            reasons.append("aligns with your profile interests")
        elif factors.get('interest_match', 0) > 0.4:
            reasons.append("matches some of your interests")
        
        if factors.get('time_relevance', 0) > 0.8:
            reasons.append("happening soon")
        
        if factors.get('completeness', 0) > 0.8:
            reasons.append("has detailed information available")
        
        if not reasons:
            return "recommended based on your location and general interests"
        
        return "Recommended because it " + " and ".join(reasons)
    
    def _final_ranking_and_filtering(self, events: List[Any], user_profile: Any, user_activity: str) -> List[Any]:
        """Apply final ranking and filtering to events"""
        # Filter out events with very low relevance scores
        filtered_events = [e for e in events if getattr(e, 'relevance_score', 0) > 0.2]
        
        # Limit to configured maximum
        final_events = filtered_events[:self.final_event_limit]
        
        logger.info(f"Final filtering: {len(events)} -> {len(filtered_events)} -> {len(final_events)}")
        
        return final_events
    
    def _remove_cost_information(self, events: List[Any]) -> List[Any]:
        """Remove cost/price information from events as requested"""
        for event in events:
            # Set price fields to None
            if hasattr(event, 'price_min'):
                event.price_min = None
            if hasattr(event, 'price_max'):
                event.price_max = None
        
        return events
    
    def _advanced_ai_evaluation(self, events: List[Any], user_profile: Any,
                              user_activity: str, personalization_data: Dict[str, Any]) -> Dict[str, Any]:
        """Advanced AI evaluation using external AI service (placeholder for future implementation)"""
        # This would integrate with OpenAI or other AI services for more sophisticated evaluation
        # For now, fall back to rule-based evaluation
        logger.info("Advanced AI evaluation not yet implemented, using rule-based evaluation")
        return self._rule_based_evaluation(events, user_profile, user_activity, personalization_data)

========================================
# File: /home/jarvis/Documents/GitHub/WhatNowAI/services/ticketmaster_service.py
========================================

"""
Ticketmaster API service for event discovery

This module integrates with the Ticketmaster Discovery API to find local events
and activities based on user location and interests. Includes advanced personalization,
event categorization, filtering, and comprehensive error handling for intelligent
event discovery that adapts to user preferences and behavioral patterns.
"""

import requests
import logging
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from datetime import datetime, timedelta
import json
import re

logger = logging.getLogger(__name__)


@dataclass
class Event:
    """Enhanced event data structure with personalization metrics"""
    id: str
    name: str
    url: str
    date: str
    time: str
    venue: str
    address: str
    city: str
    latitude: float
    longitude: float
    category: str
    subcategory: str = ""
    price_min: Optional[float] = None
    price_max: Optional[float] = None
    image_url: str = ""
    description: str = ""
    
    # Enhanced fields for personalization
    relevance_score: float = 0.0
    personalization_factors: Dict[str, float] = None
    recommendation_reason: str = ""
    interest_matches: List[str] = None
    behavioral_fit: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.personalization_factors is None:
            self.personalization_factors = {}
        if self.interest_matches is None:
            self.interest_matches = []
        if self.behavioral_fit is None:
            self.behavioral_fit = {}
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert event to dictionary for JSON serialization"""
        return {
            'id': self.id,
            'name': self.name,
            'url': self.url,
            'date': self.date,
            'time': self.time,
            'venue': self.venue,
            'address': self.address,
            'city': self.city,
            'latitude': self.latitude,
            'longitude': self.longitude,
            'category': self.category,
            'subcategory': self.subcategory,
            'price_min': self.price_min,
            'price_max': self.price_max,
            'image_url': self.image_url,
            'description': self.description,
            'relevance_score': getattr(self, 'relevance_score', 0.0),
            'personalization_factors': getattr(self, 'personalization_factors', {}),
            'recommendation_reason': getattr(self, 'recommendation_reason', ''),
            'interest_matches': getattr(self, 'interest_matches', []),
            'behavioral_fit': getattr(self, 'behavioral_fit', {}),
            'ai_analysis': {
                'score': getattr(self, 'personalization_factors', {}).get('ai_score', 0),
                'confidence': getattr(self, 'personalization_factors', {}).get('ai_confidence', 0),
                'reason': getattr(self, 'personalization_factors', {}).get('ai_reason', '')
            }
        }


class TicketmasterService:
    """Advanced service for intelligent event discovery with personalization"""
    
    def __init__(self, api_key: str, config: Dict[str, Any]):
        """
        Initialize Ticketmaster service
        
        Args:
            api_key: Ticketmaster API key
            config: Configuration dictionary
        """
        self.api_key = api_key
        self.config = config
        self.base_url = config.get('BASE_URL', 'https://app.ticketmaster.com/discovery/v2')
        self.session = requests.Session()
        
        # Enhanced personalization components
        self.category_mapper = self._init_category_mapper()
        self.preference_analyzer = self._init_preference_analyzer()
        self.behavioral_filter = self._init_behavioral_filter()
        
    def search_events(self, location: Dict[str, Any], user_interests: List[str] = None, 
                     user_activity: str = "", personalization_data: Dict[str, Any] = None,
                     user_profile: Any = None) -> List[Event]:
        """
        Search for events near a location based on user interests and enhanced personalization
        
        Args:
            location: Dictionary with latitude, longitude, city, country
            user_interests: List of user interest categories
            user_activity: What the user wants to do
            personalization_data: Enhanced personalization data from background search
            user_profile: Enhanced user profile from user_profiling_service
            
        Returns:
            List of Event objects filtered and ranked by AI and user preferences
        """
        if not self.api_key:
            logger.warning("Ticketmaster API key not provided")
            return []
        
        latitude = location.get('latitude')
        longitude = location.get('longitude')
        
        # Convert to float if needed
        try:
            if latitude is not None:
                latitude = float(latitude)
            if longitude is not None:
                longitude = float(longitude)
        except (ValueError, TypeError) as e:
            logger.error(f"Failed to convert coordinates to float in Ticketmaster service: {e}")
            return []
        
        if not latitude or not longitude:
            logger.warning("Location coordinates not provided or invalid")
            return []
        
        logger.info(f"Searching events with AI-enhanced personalization: "
                   f"location=({latitude},{longitude}), "
                   f"basic_interests={user_interests}, "
                   f"activity='{user_activity}', "
                   f"has_personalization_data={bool(personalization_data)}, "
                   f"has_user_profile={bool(user_profile)}")
        
        events = []
        
        try:
            # Create enhanced personalization data that includes user profile
            enhanced_personalization = self._merge_personalization_data(
                personalization_data, user_profile, user_activity
            )
            
            # Extract enhanced interests from all sources
            enhanced_interests = self._extract_enhanced_interests(
                enhanced_personalization, user_interests, user_activity
            )
            
            # Search for events based on location
            base_events = self._search_by_location(latitude, longitude)
            events.extend(base_events)
            
            # Search for events based on enhanced user profile
            if enhanced_interests['categories'] or user_activity:
                interest_events = self._search_by_enhanced_interests(
                    latitude, longitude, enhanced_interests, user_activity
                )
                events.extend(interest_events)
            
            # Remove duplicates based on event ID
            unique_events = {event.id: event for event in events}
            events = list(unique_events.values())
            
            # Filter and rank events using AI and personalization
            events = self._filter_and_rank_events(events, enhanced_interests, enhanced_personalization)
            
            # Sort by relevance score (highest first) then by date
            events.sort(key=lambda e: (-getattr(e, 'relevance_score', 0), e.date))
            
            # Limit results
            max_events = self.config.get('MAX_EVENTS', 20)
            final_events = events[:max_events]
            
            # Add personalization metadata to events
            self._add_personalization_metadata(final_events, enhanced_interests, enhanced_personalization)
            
            logger.info(f"AI-enhanced search completed: {len(final_events)} events returned")
            
            return final_events
            
        except Exception as e:
            logger.error(f"Error searching Ticketmaster events: {e}")
            return []
    
    def _merge_personalization_data(self, personalization_data: Dict[str, Any], 
                                   user_profile: Any, user_activity: str) -> Dict[str, Any]:
        """Merge personalization data from different sources"""
        merged_data = personalization_data.copy() if personalization_data else {}
        
        # Add user activity
        merged_data['activity'] = user_activity
        
        # If we have minimal data, ensure we have at least basic context
        if not merged_data and user_activity:
            logger.info("Creating minimal personalization context from activity")
            merged_data = {
                'activity': user_activity,
                'user_profile': {
                    'primary_activity': user_activity,
                    'interests': [],
                    'preferences': {'activity_style': 'balanced'},
                    'behavioral_patterns': {},
                    'activity_context': {'intent': 'seeking'},
                    'completion_score': 10
                },
                'minimal_context': True
            }
        
        # Add user profile data if available
        if user_profile:
            try:
                # Get recommendation context from enhanced user profile
                from services.user_profiling_service import EnhancedUserProfilingService
                
                if hasattr(user_profile, 'get_top_interests'):
                    # It's a UserProfile object
                    merged_data['user_profile'] = {
                        'name': user_profile.name,
                        'location': user_profile.location,
                        'primary_activity': user_profile.activity,
                        'completion_score': getattr(user_profile, 'profile_completion', 0),
                        'interests': [i.to_dict() for i in user_profile.get_top_interests(10)],
                        'preferences': getattr(user_profile, 'preferences', {}),
                        'behavioral_patterns': getattr(user_profile, 'behavioral_patterns', {}),
                        'activity_context': getattr(user_profile, 'activity_context', {}),
                        'demographic_hints': getattr(user_profile, 'demographic_hints', {})
                    }
                elif isinstance(user_profile, dict):
                    # It's already a dictionary
                    merged_data['user_profile'] = user_profile
                
            except Exception as e:
                logger.warning(f"Failed to merge user profile data: {e}")
        
        return merged_data
    
    def _add_personalization_metadata(self, events: List[Event], enhanced_interests: Dict[str, Any],
                                     personalization_data: Dict[str, Any]):
        """Add detailed personalization metadata to events"""
        for event in events:
            if not hasattr(event, 'personalization_factors'):
                setattr(event, 'personalization_factors', {})
            
            # Add interest matches
            event_text = f"{event.name} {event.description} {event.subcategory}".lower()
            keywords = enhanced_interests.get('keywords', [])
            
            matched_keywords = [kw for kw in keywords if kw.lower() in event_text]
            setattr(event, 'interest_matches', matched_keywords)
            
            # Add behavioral fit analysis
            user_profile = personalization_data.get('user_profile', {})
            behavioral_patterns = user_profile.get('behavioral_patterns', {})
            preferences = user_profile.get('preferences', {})
            
            behavioral_fit = {}
            
            # Social preference fit
            social_pref = preferences.get('social_preference', 'flexible')
            if social_pref != 'flexible':
                if social_pref == 'group' and event.category in ['music', 'sports']:
                    behavioral_fit['social_fit'] = 'high'
                elif social_pref == 'solo' and event.category in ['arts']:
                    behavioral_fit['social_fit'] = 'high'
                else:
                    behavioral_fit['social_fit'] = 'medium'
            
            # Activity style fit
            activity_style = preferences.get('activity_style', 'balanced')
            if 'adventure' in activity_style and any(word in event_text for word in ['outdoor', 'extreme', 'adventure']):
                behavioral_fit['adventure_fit'] = 'high'
            elif 'creative' in activity_style and event.category == 'arts':
                behavioral_fit['creative_fit'] = 'high'
            elif 'educational' in activity_style and any(word in event_text for word in ['workshop', 'lecture', 'learn']):
                behavioral_fit['educational_fit'] = 'high'
            
            setattr(event, 'behavioral_fit', behavioral_fit)
            
            # Update personalization factors
            event.personalization_factors.update({
                'keyword_matches': len(matched_keywords),
                'category_relevance': 1.0 if event.category in enhanced_interests.get('categories', []) else 0.5,
                'behavioral_fit_score': len(behavioral_fit) / 3.0,  # Normalize by number of possible fits
                'user_profile_completion': user_profile.get('completion_score', 0) / 100.0
            })
    
    def _search_by_location(self, latitude: float, longitude: float) -> List[Event]:
        """Search for events by location"""
        events = []
        
        params = {
            'apikey': self.api_key,
            'latlong': f"{latitude},{longitude}",
            'radius': self.config.get('SEARCH_RADIUS', 50),
            'unit': 'miles',
            'size': 20,
            'sort': 'date,asc'
        }
        
        try:
            response = self._make_request('/events.json', params)
            if response and '_embedded' in response:
                events = self._parse_events(response['_embedded']['events'])
        except Exception as e:
            logger.error(f"Error searching events by location: {e}")
        
        return events
    
    def _search_by_interests(self, latitude: float, longitude: float, 
                           interests: List[str], activity: str) -> List[Event]:
        """Search for events based on user interests and activity"""
        events = []
        
        # Map user interests to Ticketmaster categories
        category_mapping = {
            'music': 'music',
            'sports': 'sports',
            'theater': 'arts',
            'comedy': 'arts',
            'family': 'family',
            'food': 'miscellaneous',
            'fitness': 'sports',
            'technology': 'miscellaneous',
            'art': 'arts',
            'dance': 'arts'
        }
        
        # Determine search categories
        search_categories = []
        if interests:
            for interest in interests:
                category = category_mapping.get(interest.lower())
                if category and category not in search_categories:
                    search_categories.append(category)
        
        # Use AI to determine category from activity if available
        if activity and not search_categories:
            search_categories = self._categorize_activity_with_ai(activity)
        
        # If no specific categories, use default
        if not search_categories:
            search_categories = self.config.get('DEFAULT_CATEGORIES', ['music', 'sports', 'arts'])
        
        # Search for each category
        for category in search_categories:
            try:
                params = {
                    'apikey': self.api_key,
                    'latlong': f"{latitude},{longitude}",
                    'radius': self.config.get('SEARCH_RADIUS', 50),
                    'unit': 'miles',
                    'size': 10,
                    'sort': 'date,asc',
                    'classificationName': category
                }
                
                response = self._make_request('/events.json', params)
                if response and '_embedded' in response:
                    category_events = self._parse_events(response['_embedded']['events'])
                    events.extend(category_events)
                    
            except Exception as e:
                logger.error(f"Error searching events for category {category}: {e}")
                continue
        
        return events
    
    def _categorize_activity_with_ai(self, activity: str) -> List[str]:
        """Use OpenAI to categorize user activity into Ticketmaster categories"""
        try:
            from config.settings import OPENAI_API_KEY
            if not OPENAI_API_KEY:
                return ['miscellaneous']
            
            from openai import OpenAI
            client = OpenAI(api_key=OPENAI_API_KEY)
            
            prompt = f"""
            Based on the activity "{activity}", suggest the most relevant Ticketmaster event categories.
            
            Available categories:
            - music (concerts, festivals, shows)
            - sports (games, tournaments, fitness events)
            - arts (theater, comedy, exhibitions, dance)
            - family (kid-friendly events)
            - miscellaneous (other events)
            
            Return only the category names as a comma-separated list, maximum 3 categories.
            Activity: {activity}
            Categories:
            """
            
            response = client.completions.create(
                model="gpt-3.5-turbo-instruct",
                prompt=prompt,
                max_tokens=50,
                temperature=0.3
            )
            
            categories_text = response.choices[0].text.strip()
            categories = [cat.strip().lower() for cat in categories_text.split(',')]
            
            # Validate categories
            valid_categories = ['music', 'sports', 'arts', 'family', 'miscellaneous']
            return [cat for cat in categories if cat in valid_categories]
            
        except Exception as e:
            logger.warning(f"AI categorization failed: {e}")
            return ['miscellaneous']
    
    def _make_request(self, endpoint: str, params: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Make a request to the Ticketmaster API"""
        url = self.base_url + endpoint
        
        try:
            response = self.session.get(
                url, 
                params=params, 
                timeout=self.config.get('TIMEOUT', 10)
            )
            response.raise_for_status()
            return response.json()
            
        except requests.exceptions.RequestException as e:
            logger.error(f"Ticketmaster API request failed: {e}")
            return None
    
    def _parse_events(self, events_data: List[Dict[str, Any]]) -> List[Event]:
        """Parse events from Ticketmaster API response"""
        events = []
        
        for event_data in events_data:
            try:
                event = self._parse_single_event(event_data)
                if event:
                    events.append(event)
            except Exception as e:
                logger.warning(f"Failed to parse event: {e}")
                continue
        
        return events
    
    def _parse_single_event(self, event_data: Dict[str, Any]) -> Optional[Event]:
        """Parse a single event from API response"""
        try:
            # Basic event info
            event_id = event_data.get('id', '')
            name = event_data.get('name', 'Unknown Event')
            url = event_data.get('url', '')
            
            # Date and time
            dates = event_data.get('dates', {})
            start = dates.get('start', {})
            date = start.get('localDate', '')
            time = start.get('localTime', '')
            
            # Venue information
            venues = event_data.get('_embedded', {}).get('venues', [])
            if not venues:
                return None
            
            venue_data = venues[0]
            venue_name = venue_data.get('name', 'Unknown Venue')
            
            # Address
            address_data = venue_data.get('address', {})
            address = address_data.get('line1', '')
            
            city_data = venue_data.get('city', {})
            city = city_data.get('name', '')
            
            # Coordinates
            location_data = venue_data.get('location', {})
            try:
                latitude = float(location_data.get('latitude', 0))
                longitude = float(location_data.get('longitude', 0))
            except (ValueError, TypeError):
                return None
            
            if not latitude or not longitude:
                return None
            
            # Category
            classifications = event_data.get('classifications', [])
            category = 'miscellaneous'
            subcategory = ''
            
            if classifications:
                segment = classifications[0].get('segment', {})
                genre = classifications[0].get('genre', {})
                category = segment.get('name', 'miscellaneous').lower()
                subcategory = genre.get('name', '')
            
            # Price information
            price_ranges = event_data.get('priceRanges', [])
            price_min = None
            price_max = None
            
            if price_ranges:
                price_min = price_ranges[0].get('min')
                price_max = price_ranges[0].get('max')
            
            # Images
            images = event_data.get('images', [])
            image_url = ''
            if images:
                # Get the largest image
                image_url = max(images, key=lambda img: img.get('width', 0) * img.get('height', 0)).get('url', '')
            
            # Description
            description = event_data.get('info', '')
            
            return Event(
                id=event_id,
                name=name,
                url=url,
                date=date,
                time=time,
                venue=venue_name,
                address=address,
                city=city,
                latitude=latitude,
                longitude=longitude,
                category=category,
                subcategory=subcategory,
                price_min=price_min,
                price_max=price_max,
                image_url=image_url,
                description=description
            )
            
        except Exception as e:
            logger.error(f"Error parsing event data: {e}")
            return None
    
    def _extract_enhanced_interests(self, personalization_data: Dict[str, Any], 
                                  basic_interests: List[str], activity: str) -> Dict[str, Any]:
        """
        Extract and combine interests from personalization data and basic inputs
        
        Args:
            personalization_data: Data from background search with interests
            basic_interests: Basic interest categories
            activity: User activity description
            
        Returns:
            Enhanced interests dictionary with categories, keywords, and confidence scores
        """
        enhanced_interests = {
            'categories': [],
            'keywords': [],
            'category_scores': {},
            'activity_keywords': []
        }
        
        # Start with basic interests
        if basic_interests:
            enhanced_interests['categories'].extend(basic_interests)
            for interest in basic_interests:
                enhanced_interests['category_scores'][interest] = 0.5  # Default confidence
        
        # Extract interests from personalization data
        if personalization_data:
            # Check for interests from background search
            search_results = personalization_data.get('search_results', {})
            search_summaries = personalization_data.get('search_summaries', {})
            
            # Extract from search summaries (these might contain interest indicators)
            for source, summary in search_summaries.items():
                if summary and isinstance(summary, str):
                    interest_keywords = self._extract_keywords_from_text(summary)
                    enhanced_interests['keywords'].extend(interest_keywords)
            
            # If there are extracted interests in the data
            interests_data = personalization_data.get('interests', [])
            if interests_data:
                for interest_item in interests_data:
                    if isinstance(interest_item, dict):
                        category = interest_item.get('category', '')
                        confidence = interest_item.get('confidence', 0.5)
                        keywords = interest_item.get('keywords', [])
                        
                        if category and category not in enhanced_interests['categories']:
                            enhanced_interests['categories'].append(category)
                            enhanced_interests['category_scores'][category] = confidence
                            enhanced_interests['keywords'].extend(keywords)
        
        # Extract keywords from activity description
        if activity:
            activity_keywords = self._extract_keywords_from_text(activity)
            enhanced_interests['activity_keywords'] = activity_keywords
            enhanced_interests['keywords'].extend(activity_keywords)
        
        # Remove duplicates
        enhanced_interests['keywords'] = list(set(enhanced_interests['keywords']))
        enhanced_interests['categories'] = list(set(enhanced_interests['categories']))
        
        logger.info(f"Enhanced interests extracted: {len(enhanced_interests['categories'])} categories, "
                   f"{len(enhanced_interests['keywords'])} keywords")
        
        return enhanced_interests
    
    def _extract_keywords_from_text(self, text: str) -> List[str]:
        """Extract relevant keywords from text content"""
        if not text:
            return []
        
        # Simple keyword extraction - can be enhanced with NLP libraries
        import re
        
        # Define interest-related keywords
        interest_keywords = {
            'music': ['music', 'concert', 'band', 'singer', 'album', 'song', 'artist', 'festival', 'show', 'performance'],
            'sports': ['sport', 'game', 'team', 'fitness', 'exercise', 'basketball', 'football', 'soccer', 'tennis', 'golf'],
            'arts': ['art', 'theater', 'museum', 'gallery', 'dance', 'exhibition', 'culture', 'painting', 'sculpture'],
            'food': ['food', 'restaurant', 'cooking', 'cuisine', 'chef', 'dining', 'culinary', 'recipe', 'meal'],
            'technology': ['tech', 'programming', 'code', 'software', 'computer', 'digital', 'innovation', 'startup'],
            'entertainment': ['movie', 'film', 'tv', 'show', 'entertainment', 'comedy', 'drama', 'cinema'],
            'nature': ['nature', 'outdoor', 'hiking', 'camping', 'park', 'beach', 'environment', 'eco'],
            'social': ['community', 'social', 'networking', 'meetup', 'group', 'volunteer', 'charity'],
            'education': ['education', 'learning', 'workshop', 'seminar', 'course', 'training', 'lecture'],
            'business': ['business', 'networking', 'entrepreneur', 'startup', 'conference', 'professional']
        }
        
        text_lower = text.lower()
        found_keywords = []
        
        for category, keywords in interest_keywords.items():
            for keyword in keywords:
                if keyword in text_lower:
                    found_keywords.append(keyword)
        
        return found_keywords
    
    def _search_by_enhanced_interests(self, latitude: float, longitude: float, 
                                    enhanced_interests: Dict[str, Any], activity: str) -> List[Event]:
        """Search for events based on enhanced interest profile"""
        events = []
        
        # Map enhanced interests to Ticketmaster categories
        category_mapping = {
            'music': 'music',
            'sports': 'sports', 
            'theater': 'arts',
            'arts': 'arts',
            'comedy': 'arts',
            'family': 'family',
            'food': 'miscellaneous',
            'fitness': 'sports',
            'technology': 'miscellaneous',
            'entertainment': 'arts',
            'nature': 'miscellaneous',
            'social': 'miscellaneous',
            'education': 'miscellaneous',
            'business': 'miscellaneous'
        }
        
        # Get categories with confidence scores
        search_categories = []
        category_scores = enhanced_interests.get('category_scores', {})
        
        for category in enhanced_interests.get('categories', []):
            tm_category = category_mapping.get(category.lower())
            if tm_category and tm_category not in search_categories:
                search_categories.append(tm_category)
        
        # If no categories from interests, try to derive from keywords
        if not search_categories and enhanced_interests.get('keywords'):
            keywords_text = ' '.join(enhanced_interests['keywords'])
            search_categories = self._categorize_activity_with_ai(keywords_text)
        
        # If still no categories, use activity
        if not search_categories and activity:
            search_categories = self._categorize_activity_with_ai(activity)
        
        # Fallback to default categories
        if not search_categories:
            search_categories = self.config.get('DEFAULT_CATEGORIES', ['music', 'sports', 'arts'])
        
        # Search for each category with keyword filtering
        for category in search_categories:
            try:
                params = {
                    'apikey': self.api_key,
                    'latlong': f"{latitude},{longitude}",
                    'radius': self.config.get('SEARCH_RADIUS', 50),
                    'unit': 'miles',
                    'size': 15,  # Get more results for better filtering
                    'sort': 'date,asc',
                    'classificationName': category
                }
                
                # Add keyword filtering if available
                keywords = enhanced_interests.get('keywords', [])
                activity_keywords = enhanced_interests.get('activity_keywords', [])
                all_keywords = keywords + activity_keywords
                
                if all_keywords:
                    # Use most relevant keywords for search
                    top_keywords = all_keywords[:3]  # Limit to avoid too restrictive search
                    keyword_query = ' OR '.join(top_keywords)
                    params['keyword'] = keyword_query
                
                response = self._make_request('/events.json', params)
                if response and '_embedded' in response:
                    category_events = self._parse_events(response['_embedded']['events'])
                    events.extend(category_events)
                    
            except Exception as e:
                logger.error(f"Error searching events for enhanced category {category}: {e}")
                continue
        
        return events
    
    def _filter_and_rank_events(self, events: List[Event], enhanced_interests: Dict[str, Any], 
                               personalization_data: Dict[str, Any]) -> List[Event]:
        """
        Filter and rank events based on enhanced user profile using AI analysis
        
        Args:
            events: List of events to filter and rank
            enhanced_interests: Enhanced interest data
            personalization_data: Full personalization data
            
        Returns:
            Filtered and ranked events with relevance scores and AI recommendations
        """
        if not events:
            return events
        
        # Use AI to intelligently rank events
        events = self._ai_rank_events(events, enhanced_interests, personalization_data)
        
        # Apply traditional scoring as backup/boost
        events = self._apply_traditional_scoring(events, enhanced_interests)
        
        # Filter out events with very low relevance
        min_relevance = self.config.get('MIN_RELEVANCE_SCORE', 0.15)
        filtered_events = [event for event in events if getattr(event, 'relevance_score', 0) >= min_relevance]
        
        logger.info(f"AI-filtered {len(events)} events down to {len(filtered_events)} relevant events")
        
        return filtered_events
    
    def _ai_rank_events(self, events: List[Event], enhanced_interests: Dict[str, Any], 
                       personalization_data: Dict[str, Any]) -> List[Event]:
        """Use OpenAI to intelligently rank events based on user profile and activity request"""
        try:
            from config.settings import OPENAI_API_KEY
            if not OPENAI_API_KEY or len(events) == 0:
                return events
            
            from openai import OpenAI
            client = OpenAI(api_key=OPENAI_API_KEY)
            
            # Extract user context
            user_activity = personalization_data.get('activity', '') if personalization_data else ''
            user_interests = enhanced_interests.get('categories', [])
            user_keywords = enhanced_interests.get('keywords', [])
            activity_keywords = enhanced_interests.get('activity_keywords', [])
            
            # Get user profile data if available
            user_profile_data = personalization_data.get('user_profile', {}) if personalization_data else {}
            preferences = user_profile_data.get('preferences', {})
            behavioral_patterns = user_profile_data.get('behavioral_patterns', {})
            activity_context = user_profile_data.get('activity_context', {})
            
            # Create event summaries for AI analysis
            event_summaries = []
            for i, event in enumerate(events[:15]):  # Limit to top 15 events to avoid token limits
                event_summary = {
                    'index': i,
                    'name': event.name,
                    'category': event.category,
                    'subcategory': event.subcategory,
                    'venue': event.venue,
                    'date': event.date,
                    'time': event.time,
                    'description': event.description[:200] if event.description else '',
                    'price_min': event.price_min,
                    'price_max': event.price_max
                }
                event_summaries.append(event_summary)
            
            # Create comprehensive prompt for AI analysis
            prompt = self._create_ai_ranking_prompt(
                user_activity, user_interests, user_keywords, activity_keywords,
                preferences, behavioral_patterns, activity_context, event_summaries
            )
            
            # Get AI ranking
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "You are an expert event recommendation AI. Analyze user profiles and rank events based on relevance to their specific activity request and interests."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1000,
                temperature=0.3
            )
            
            # Parse AI response and apply scores
            ai_rankings = self._parse_ai_rankings(response.choices[0].message.content)
            events = self._apply_ai_rankings(events, ai_rankings)
            
            logger.info(f"AI ranking completed for {len(events)} events")
            
        except Exception as e:
            logger.warning(f"AI ranking failed, falling back to traditional scoring: {e}")
        
        return events
    
    def _create_ai_ranking_prompt(self, user_activity: str, user_interests: List[str], 
                                 user_keywords: List[str], activity_keywords: List[str],
                                 preferences: Dict[str, Any], behavioral_patterns: Dict[str, Any],
                                 activity_context: Dict[str, Any], event_summaries: List[Dict]) -> str:
        """Create a comprehensive prompt for AI event ranking"""
        
        prompt = f"""
You are helping a user find the most relevant events based on their specific request and profile.

USER'S ACTIVITY REQUEST: "{user_activity}"

USER PROFILE:
- Primary Interests: {', '.join(user_interests) if user_interests else 'None specified'}
- Interest Keywords: {', '.join(user_keywords) if user_keywords else 'None'}
- Activity Keywords: {', '.join(activity_keywords) if activity_keywords else 'None'}

PREFERENCES:
- Preferred Categories: {preferences.get('preferred_categories', [])}
- Social Preference: {preferences.get('social_preference', 'flexible')}
- Activity Style: {preferences.get('activity_style', 'balanced')}
- Budget Preference: {preferences.get('budget_preference', 'medium')}
- Preferred Time: {preferences.get('preferred_time', 'flexible')}

BEHAVIORAL PATTERNS:
- Adventure Seeking: {behavioral_patterns.get('adventure_seeker', 0):.2f}
- Social Learning: {behavioral_patterns.get('social_learner', 0):.2f}
- Creative Expression: {behavioral_patterns.get('creative_expression', 0):.2f}
- Learning Oriented: {behavioral_patterns.get('learning_oriented', 0):.2f}

ACTIVITY CONTEXT:
- Intent: {activity_context.get('intent', 'unknown')}
- Urgency: {activity_context.get('urgency', 'medium')}
- Social Setting: {activity_context.get('social_setting', 'flexible')}
- Budget Preference: {activity_context.get('budget_preference', 'medium')}

AVAILABLE EVENTS:
"""
        
        for event in event_summaries:
            price_info = ""
            if event['price_min'] is not None:
                if event['price_max'] and event['price_max'] != event['price_min']:
                    price_info = f" (${event['price_min']}-${event['price_max']})"
                else:
                    price_info = f" (${event['price_min']})"
            elif event['price_min'] == 0 or event['price_min'] is None:
                price_info = " (Free/TBD)"
            
            prompt += f"""
{event['index']}. {event['name']}
   Category: {event['category']} | {event['subcategory']}
   Date/Time: {event['date']} at {event['time']}
   Venue: {event['venue']}{price_info}
   Description: {event['description']}
"""
        
        prompt += f"""

TASK:
Rank these events from 0-10 based on how well they match:
1. The user's specific activity request: "{user_activity}"
2. Their interests and behavioral patterns
3. Their preferences and context

Focus heavily on what the user actually said they want to do that day.

Provide your response as a JSON array with this format:
[
  {{"index": 0, "score": 8.5, "reason": "Perfect match for user's request because..."}},
  {{"index": 1, "score": 7.2, "reason": "Good fit because..."}},
  ...
]

Only include events with scores 5.0 or higher. Sort by score (highest first).
"""
        
        return prompt
    
    def _parse_ai_rankings(self, ai_response: str) -> Dict[int, Dict[str, Any]]:
        """Parse AI ranking response into usable format"""
        rankings = {}
        
        try:
            import json
            import re
            
            # Try to extract JSON from response
            json_match = re.search(r'\[.*?\]', ai_response, re.DOTALL)
            if json_match:
                rankings_data = json.loads(json_match.group())
                
                for item in rankings_data:
                    if isinstance(item, dict) and 'index' in item and 'score' in item:
                        index = item['index']
                        rankings[index] = {
                            'score': float(item['score']),
                            'reason': item.get('reason', ''),
                            'ai_confidence': 0.9
                        }
            
        except Exception as e:
            logger.warning(f"Failed to parse AI rankings: {e}")
        
        return rankings
    
    def _apply_ai_rankings(self, events: List[Event], ai_rankings: Dict[int, Dict[str, Any]]) -> List[Event]:
        """Apply AI rankings to events"""
        for i, event in enumerate(events):
            if i in ai_rankings:
                ranking = ai_rankings[i]
                # Convert AI score (0-10) to relevance score (0-1)
                ai_score = ranking['score'] / 10.0
                
                # Set relevance score and metadata
                setattr(event, 'relevance_score', ai_score)
                setattr(event, 'recommendation_reason', ranking['reason'])
                
                # Update personalization factors
                if not hasattr(event, 'personalization_factors'):
                    setattr(event, 'personalization_factors', {})
                
                event.personalization_factors.update({
                    'ai_score': ai_score,
                    'ai_confidence': ranking['ai_confidence'],
                    'ai_reason': ranking['reason']
                })
            else:
                # Default score for events not ranked by AI
                setattr(event, 'relevance_score', 0.3)
        
        return events
    
    def _apply_traditional_scoring(self, events: List[Event], enhanced_interests: Dict[str, Any]) -> List[Event]:
        """Apply traditional scoring as backup and boost for AI scores"""
        keywords = enhanced_interests.get('keywords', [])
        categories = enhanced_interests.get('categories', [])
        category_scores = enhanced_interests.get('category_scores', {})
        
        for event in events:
            current_score = getattr(event, 'relevance_score', 0.0)
            traditional_boost = 0.0
            
            # Category match bonus
            event_category = event.category.lower()
            if event_category in [cat.lower() for cat in categories]:
                category_confidence = max([category_scores.get(cat, 0.5) for cat in categories 
                                         if cat.lower() == event_category])
                traditional_boost += category_confidence * 0.2
            
            # Keyword matching boost
            event_text = f"{event.name} {event.description} {event.subcategory}".lower()
            keyword_matches = sum(1 for keyword in keywords if keyword.lower() in event_text)
            
            if keywords and keyword_matches > 0:
                keyword_score = (keyword_matches / len(keywords)) * 0.15
                traditional_boost += keyword_score
            
            # Time preference boost
            try:
                from datetime import datetime
                event_date = datetime.strptime(event.date, '%Y-%m-%d')
                days_ahead = (event_date - datetime.now()).days
                
                if 0 <= days_ahead <= 7:  # This week
                    traditional_boost += 0.1
                elif 0 <= days_ahead <= 30:  # This month
                    traditional_boost += 0.05
                    
            except (ValueError, AttributeError):
                pass
            
            # Free events boost
            if event.price_min is None or event.price_min == 0:
                traditional_boost += 0.05
            
            # Apply boost to current score
            final_score = min(current_score + traditional_boost, 1.0)
            setattr(event, 'relevance_score', final_score)
        
        return events
    
    def _init_category_mapper(self) -> Dict[str, Dict[str, Any]]:
        """Initialize enhanced category mapping with subcategories and attributes"""
        return {
            'music': {
                'tm_categories': ['music'],
                'subcategories': {
                    'rock': ['rock', 'alternative', 'indie', 'punk'],
                    'pop': ['pop', 'mainstream', 'dance'],
                    'electronic': ['electronic', 'edm', 'techno', 'house'],
                    'jazz': ['jazz', 'blues', 'swing'],
                    'classical': ['classical', 'orchestra', 'symphony'],
                    'country': ['country', 'folk', 'americana'],
                    'hip-hop': ['hip-hop', 'rap', 'urban'],
                    'festival': ['festival', 'multi-day', 'outdoor']
                },
                'venue_types': ['concert hall', 'arena', 'club', 'festival ground', 'theater'],
                'keywords': ['concert', 'live music', 'band', 'artist', 'album', 'tour']
            },
            'sports': {
                'tm_categories': ['sports'],
                'subcategories': {
                    'team_sports': ['football', 'basketball', 'baseball', 'soccer', 'hockey'],
                    'individual': ['tennis', 'golf', 'boxing', 'mma', 'wrestling'],
                    'outdoor': ['cycling', 'running', 'triathlon', 'skiing'],
                    'motorsports': ['racing', 'formula', 'nascar', 'motorcycle']
                },
                'venue_types': ['stadium', 'arena', 'court', 'field', 'track'],
                'keywords': ['game', 'match', 'championship', 'tournament', 'league']
            },
            'arts': {
                'tm_categories': ['arts', 'theatre'],
                'subcategories': {
                    'theater': ['play', 'musical', 'drama', 'comedy'],
                    'dance': ['ballet', 'contemporary', 'cultural'],
                    'visual': ['exhibition', 'gallery', 'museum'],
                    'comedy': ['stand-up', 'improv', 'sketch']
                },
                'venue_types': ['theater', 'gallery', 'museum', 'arts center'],
                'keywords': ['performance', 'exhibition', 'show', 'cultural', 'artistic']
            },
            'family': {
                'tm_categories': ['family'],
                'subcategories': {
                    'kids': ['children', 'family-friendly', 'educational'],
                    'entertainment': ['circus', 'magic', 'puppet'],
                    'seasonal': ['holiday', 'christmas', 'halloween']
                },
                'venue_types': ['family center', 'park', 'indoor venue'],
                'keywords': ['family', 'kids', 'children', 'all ages']
            },
            'miscellaneous': {
                'tm_categories': ['miscellaneous'],
                'subcategories': {
                    'food': ['food festival', 'wine tasting', 'culinary'],
                    'technology': ['tech conference', 'startup', 'innovation'],
                    'health': ['wellness', 'fitness', 'yoga'],
                    'business': ['networking', 'conference', 'professional']
                },
                'venue_types': ['conference center', 'convention hall', 'outdoor space'],
                'keywords': ['festival', 'expo', 'conference', 'workshop']
            }
        }
    
    def _init_preference_analyzer(self) -> Dict[str, Any]:
        """Initialize preference analysis patterns"""
        return {
            'time_preferences': {
                'morning': {'keywords': ['morning', 'early', 'breakfast', 'sunrise'], 'hours': [6, 7, 8, 9, 10, 11]},
                'afternoon': {'keywords': ['afternoon', 'lunch', 'midday'], 'hours': [12, 13, 14, 15, 16, 17]},
                'evening': {'keywords': ['evening', 'dinner', 'night', 'sunset'], 'hours': [18, 19, 20, 21, 22]},
                'late_night': {'keywords': ['late', 'midnight', 'after hours'], 'hours': [23, 0, 1, 2]}
            },
            'price_sensitivity': {
                'budget': {'keywords': ['cheap', 'free', 'budget', 'affordable'], 'max_price': 25},
                'moderate': {'keywords': ['reasonable', 'fair', 'moderate'], 'max_price': 75},
                'premium': {'keywords': ['premium', 'luxury', 'high-end', 'exclusive'], 'max_price': None}
            },
            'venue_preferences': {
                'intimate': {'keywords': ['small', 'intimate', 'cozy', 'close'], 'capacity': 500},
                'medium': {'keywords': ['medium', 'moderate', 'comfortable'], 'capacity': 2000},
                'large': {'keywords': ['big', 'arena', 'stadium', 'massive'], 'capacity': None}
            }
        }
    
    def _init_behavioral_filter(self) -> Dict[str, Dict[str, Any]]:
        """Initialize behavioral filtering patterns"""
        return {
            'social_preference': {
                'solo': {
                    'boost_categories': ['arts', 'education', 'cultural'],
                    'avoid_categories': ['party', 'festival'],
                    'venue_preference': 'intimate'
                },
                'group': {
                    'boost_categories': ['music', 'sports', 'festival'],
                    'avoid_categories': ['meditation', 'lecture'],
                    'venue_preference': 'large'
                },
                'family': {
                    'boost_categories': ['family', 'educational', 'cultural'],
                    'filter_content': True,
                    'time_preference': 'afternoon'
                }
            },
            'activity_style': {
                'adventurous': {
                    'boost_keywords': ['new', 'unique', 'extreme', 'adventure', 'outdoor'],
                    'avoid_keywords': ['traditional', 'classic', 'routine']
                },
                'educational': {
                    'boost_keywords': ['learn', 'workshop', 'lecture', 'educational', 'cultural'],
                    'boost_categories': ['arts', 'miscellaneous']
                },
                'creative': {
                    'boost_keywords': ['creative', 'artistic', 'hands-on', 'workshop', 'exhibition'],
                    'boost_categories': ['arts']
                }
            },
            'lifestyle': {
                'active': {
                    'boost_categories': ['sports', 'outdoor'],
                    'boost_keywords': ['fitness', 'active', 'outdoor', 'physical']
                },
                'cultural': {
                    'boost_categories': ['arts', 'music'],
                    'boost_keywords': ['cultural', 'artistic', 'performance', 'exhibition']
                },
                'social': {
                    'boost_keywords': ['social', 'networking', 'community', 'group']
                }
            }
        }

========================================
# File: /home/jarvis/Documents/GitHub/WhatNowAI/services/user_profiling_service.py
========================================

"""
Enhanced User Profiling Service

This service creates comprehensive user profiles by analyzing multiple data sources
and extracting detailed interests, preferences, and behavioral patterns to improve
event recommendations and personalization.
"""

import logging
import re
import json
from typing import Dict, List, Any, Optional, Set
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from collections import Counter
import asyncio
from concurrent.futures import ThreadPoolExecutor

logger = logging.getLogger(__name__)


@dataclass
class UserInterest:
    """Detailed user interest with confidence metrics"""
    category: str
    keywords: List[str]
    confidence: float
    source: str
    evidence: str
    frequency: int = 1
    context: str = ""
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'category': self.category,
            'keywords': self.keywords,
            'confidence': self.confidence,
            'source': self.source,
            'evidence': self.evidence,
            'frequency': self.frequency,
            'context': self.context
        }


@dataclass
class UserProfile:
    """Comprehensive user profile with behavioral insights"""
    name: str
    location: Dict[str, Any]
    activity: str
    social_data: Dict[str, str] = field(default_factory=dict)
    
    # Enhanced profile data
    interests: List[UserInterest] = field(default_factory=list)
    preferences: Dict[str, Any] = field(default_factory=dict)
    behavioral_patterns: Dict[str, Any] = field(default_factory=dict)
    demographic_hints: Dict[str, Any] = field(default_factory=dict)
    activity_context: Dict[str, Any] = field(default_factory=dict)
    
    # Metadata
    profile_completion: float = 0.0
    last_updated: datetime = field(default_factory=datetime.now)
    
    def add_interest(self, interest: UserInterest):
        """Add interest with deduplication and scoring"""
        for existing in self.interests:
            if existing.category == interest.category and existing.source == interest.source:
                # Merge and update
                existing.keywords.extend(interest.keywords)
                existing.keywords = list(set(existing.keywords))
                existing.confidence = max(existing.confidence, interest.confidence)
                existing.frequency += 1
                return
        self.interests.append(interest)
    
    def get_top_interests(self, n: int = 10) -> List[UserInterest]:
        """Get top N interests by confidence and frequency"""
        return sorted(
            self.interests, 
            key=lambda x: (x.confidence * x.frequency), 
            reverse=True
        )[:n]
    
    def calculate_completion(self):
        """Calculate profile completion percentage"""
        score = 0.0
        
        # Basic info (20%)
        if self.name: score += 5
        if self.location.get('city'): score += 5
        if self.activity: score += 10
        
        # Social data (20%)
        social_count = len([v for v in self.social_data.values() if v])
        score += min(social_count * 3, 20)
        
        # Interests (30%)
        interest_score = min(len(self.interests) * 3, 30)
        score += interest_score
        
        # Preferences (15%)
        if self.preferences: score += 15
        
        # Behavioral patterns (15%)
        if self.behavioral_patterns: score += 15
        
        self.profile_completion = min(score, 100.0)
        return self.profile_completion


class AdvancedInterestExtractor:
    """Advanced interest extraction with NLP and context analysis"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.interest_taxonomy = self._load_interest_taxonomy()
        self.contextual_patterns = self._load_contextual_patterns()
        self.sentiment_indicators = self._load_sentiment_indicators()
    
    def _load_interest_taxonomy(self) -> Dict[str, Dict[str, Any]]:
        """Load comprehensive interest taxonomy with hierarchical categories"""
        return {
            'music': {
                'keywords': [
                    'music', 'concert', 'festival', 'band', 'artist', 'album', 'song',
                    'guitar', 'piano', 'drums', 'violin', 'jazz', 'rock', 'pop', 'classical',
                    'hip-hop', 'rap', 'electronic', 'edm', 'country', 'blues', 'reggae',
                    'spotify', 'soundcloud', 'vinyl', 'live music', 'symphony', 'opera'
                ],
                'subcategories': ['instruments', 'genres', 'venues', 'streaming', 'recording'],
                'indicators': ['plays', 'listens', 'performs', 'composes', 'produces'],
                'venues': ['concert hall', 'club', 'stadium', 'amphitheater', 'bar'],
                'sentiment_boost': ['love', 'passion', 'obsessed', 'favorite', 'amazing']
            },
            'sports': {
                'keywords': [
                    'sports', 'football', 'basketball', 'baseball', 'soccer', 'tennis',
                    'golf', 'hockey', 'swimming', 'running', 'cycling', 'fitness', 'gym',
                    'marathon', 'triathlon', 'yoga', 'pilates', 'crossfit', 'weightlifting',
                    'volleyball', 'softball', 'wrestling', 'boxing', 'mma', 'skiing'
                ],
                'subcategories': ['team_sports', 'individual_sports', 'fitness', 'outdoor'],
                'indicators': ['plays', 'trains', 'competes', 'coaches', 'watches'],
                'venues': ['stadium', 'gym', 'court', 'field', 'track', 'pool'],
                'sentiment_boost': ['competitive', 'athletic', 'active', 'champion']
            },
            'arts': {
                'keywords': [
                    'art', 'painting', 'drawing', 'sculpture', 'photography', 'gallery',
                    'museum', 'exhibition', 'artist', 'creative', 'design', 'theater',
                    'drama', 'acting', 'dance', 'ballet', 'contemporary', 'crafts',
                    'pottery', 'jewelry', 'fashion', 'illustration', 'digital art'
                ],
                'subcategories': ['visual_arts', 'performing_arts', 'crafts', 'design'],
                'indicators': ['creates', 'exhibits', 'performs', 'designs', 'collects'],
                'venues': ['gallery', 'museum', 'theater', 'studio', 'workshop'],
                'sentiment_boost': ['creative', 'artistic', 'expressive', 'inspiring']
            },
            'technology': {
                'keywords': [
                    'technology', 'programming', 'coding', 'software', 'developer',
                    'engineer', 'computer', 'ai', 'machine learning', 'data science',
                    'startup', 'app', 'website', 'github', 'python', 'javascript',
                    'blockchain', 'crypto', 'iot', 'robotics', 'vr', 'ar', 'gaming'
                ],
                'subcategories': ['programming', 'ai_ml', 'hardware', 'gaming', 'crypto'],
                'indicators': ['develops', 'codes', 'builds', 'programs', 'hacks'],
                'venues': ['hackathon', 'conference', 'meetup', 'coworking', 'lab'],
                'sentiment_boost': ['innovative', 'cutting-edge', 'passionate', 'expert']
            },
            'food': {
                'keywords': [
                    'food', 'cooking', 'baking', 'cuisine', 'restaurant', 'chef',
                    'recipe', 'culinary', 'dining', 'foodie', 'wine', 'beer', 'coffee',
                    'tea', 'organic', 'vegan', 'vegetarian', 'nutrition', 'gourmet',
                    'street food', 'fine dining', 'barbecue', 'dessert', 'cocktails'
                ],
                'subcategories': ['cooking', 'dining', 'beverages', 'nutrition', 'culture'],
                'indicators': ['cooks', 'bakes', 'tastes', 'reviews', 'explores'],
                'venues': ['restaurant', 'kitchen', 'market', 'festival', 'tasting'],
                'sentiment_boost': ['delicious', 'gourmet', 'passionate', 'expert']
            },
            'travel': {
                'keywords': [
                    'travel', 'tourism', 'vacation', 'trip', 'adventure', 'backpacking',
                    'hotel', 'flight', 'destination', 'explore', 'culture', 'sightseeing',
                    'beach', 'mountain', 'city', 'country', 'international', 'domestic',
                    'cruise', 'road trip', 'camping', 'hiking', 'photography'
                ],
                'subcategories': ['destinations', 'activities', 'accommodation', 'transport'],
                'indicators': ['visits', 'explores', 'travels', 'photographs', 'blogs'],
                'venues': ['destinations', 'hotels', 'airports', 'attractions', 'tours'],
                'sentiment_boost': ['wanderlust', 'adventure', 'explorer', 'globe-trotter']
            },
            'nature': {
                'keywords': [
                    'nature', 'outdoor', 'hiking', 'camping', 'wildlife', 'conservation',
                    'environment', 'ecology', 'sustainability', 'gardening', 'plants',
                    'animals', 'birds', 'forest', 'mountains', 'ocean', 'rivers',
                    'national parks', 'trails', 'fishing', 'hunting', 'photography'
                ],
                'subcategories': ['outdoor_activities', 'wildlife', 'conservation', 'gardening'],
                'indicators': ['hikes', 'camps', 'explores', 'photographs', 'conserves'],
                'venues': ['parks', 'trails', 'forests', 'lakes', 'mountains'],
                'sentiment_boost': ['eco-friendly', 'naturalist', 'outdoorsy', 'green']
            }
        }
    
    def _load_contextual_patterns(self) -> Dict[str, List[str]]:
        """Load patterns that indicate depth of interest"""
        return {
            'high_engagement': [
                'passionate about', 'obsessed with', 'love', 'dedicated to',
                'professional', 'expert', 'years of experience', 'certified',
                'compete', 'perform', 'teach', 'mentor', 'lead'
            ],
            'medium_engagement': [
                'enjoy', 'like', 'interested in', 'hobby', 'amateur',
                'learning', 'practicing', 'member', 'participant'
            ],
            'low_engagement': [
                'sometimes', 'occasionally', 'beginner', 'trying',
                'curious about', 'thinking about', 'might'
            ],
            'frequency_indicators': [
                'daily', 'weekly', 'monthly', 'regularly', 'often',
                'frequently', 'always', 'constantly', 'every day'
            ]
        }
    
    def _load_sentiment_indicators(self) -> Dict[str, float]:
        """Load sentiment indicators and their weights"""
        return {
            'love': 0.9, 'passion': 0.9, 'obsessed': 0.8, 'amazing': 0.7,
            'fantastic': 0.7, 'excellent': 0.6, 'great': 0.5, 'good': 0.4,
            'like': 0.3, 'okay': 0.2, 'hate': -0.8, 'terrible': -0.7,
            'awful': -0.6, 'bad': -0.5, 'dislike': -0.4
        }
    
    def extract_interests(self, text: str, source: str, context: str = "") -> List[UserInterest]:
        """Extract interests from text with advanced NLP analysis"""
        if not text:
            return []
        
        text_lower = text.lower()
        interests = []
        
        for category, category_data in self.interest_taxonomy.items():
            # Find keyword matches
            keyword_matches = self._find_keyword_matches(text_lower, category_data['keywords'])
            
            if keyword_matches:
                # Calculate confidence based on multiple factors
                confidence = self._calculate_confidence(
                    text_lower, keyword_matches, category_data
                )
                
                # Extract evidence and context
                evidence = self._extract_evidence(text, keyword_matches)
                interest_context = self._extract_context(text, keyword_matches)
                
                if confidence > 0.2:  # Minimum confidence threshold
                    interest = UserInterest(
                        category=category,
                        keywords=keyword_matches,
                        confidence=confidence,
                        source=source,
                        evidence=evidence,
                        context=f"{context} | {interest_context}".strip(" | ")
                    )
                    interests.append(interest)
        
        return interests
    
    def _find_keyword_matches(self, text: str, keywords: List[str]) -> List[str]:
        """Find keyword matches in text"""
        matches = []
        for keyword in keywords:
            if keyword in text:
                matches.append(keyword)
        return matches
    
    def _calculate_confidence(self, text: str, matches: List[str], category_data: Dict) -> float:
        """Calculate confidence score for interest category"""
        base_confidence = len(matches) / len(category_data['keywords'])
        
        # Boost for engagement indicators
        engagement_boost = 0.0
        for level, patterns in self.contextual_patterns.items():
            for pattern in patterns:
                if pattern in text:
                    if level == 'high_engagement':
                        engagement_boost += 0.3
                    elif level == 'medium_engagement':
                        engagement_boost += 0.2
                    elif level == 'frequency_indicators':
                        engagement_boost += 0.15
        
        # Boost for sentiment
        sentiment_boost = 0.0
        for sentiment, weight in self.sentiment_indicators.items():
            if sentiment in text:
                sentiment_boost += weight * 0.1
        
        # Boost for venue/activity mentions
        venue_boost = 0.0
        for venue in category_data.get('venues', []):
            if venue in text:
                venue_boost += 0.1
        
        # Boost for action indicators
        action_boost = 0.0
        for indicator in category_data.get('indicators', []):
            if indicator in text:
                action_boost += 0.1
        
        total_confidence = min(
            base_confidence + engagement_boost + sentiment_boost + venue_boost + action_boost,
            1.0
        )
        
        return total_confidence
    
    def _extract_evidence(self, text: str, matches: List[str]) -> str:
        """Extract evidence sentences containing keyword matches"""
        sentences = re.split(r'[.!?]+', text)
        evidence_sentences = []
        
        for sentence in sentences:
            if any(match in sentence.lower() for match in matches):
                evidence_sentences.append(sentence.strip())
        
        return ' '.join(evidence_sentences[:2])  # Top 2 evidence sentences
    
    def _extract_context(self, text: str, matches: List[str]) -> str:
        """Extract contextual information about the interest"""
        text_lower = text.lower()
        context_parts = []
        
        # Look for time indicators
        time_patterns = ['since', 'for', 'years', 'months', 'recently', 'started']
        for pattern in time_patterns:
            if pattern in text_lower:
                context_parts.append(f"temporal:{pattern}")
        
        # Look for skill level indicators
        skill_patterns = ['beginner', 'intermediate', 'advanced', 'professional', 'expert']
        for pattern in skill_patterns:
            if pattern in text_lower:
                context_parts.append(f"skill:{pattern}")
        
        return ', '.join(context_parts)


class BehavioralAnalyzer:
    """Analyze user behavior patterns from text and social data"""
    
    def __init__(self):
        self.behavior_patterns = self._load_behavior_patterns()
    
    def _load_behavior_patterns(self) -> Dict[str, List[str]]:
        """Load behavioral pattern indicators"""
        return {
            'social_learner': [
                'group', 'class', 'workshop', 'meetup', 'club', 'team',
                'community', 'together', 'friends', 'social'
            ],
            'solo_activities': [
                'alone', 'individual', 'personal', 'solo', 'private',
                'meditation', 'reading', 'writing', 'reflection'
            ],
            'adventure_seeker': [
                'adventure', 'extreme', 'adrenaline', 'challenge', 'risk',
                'new', 'explore', 'discover', 'unknown', 'exciting'
            ],
            'comfort_zone': [
                'familiar', 'routine', 'regular', 'same', 'usual',
                'comfortable', 'safe', 'known', 'predictable'
            ],
            'creative_expression': [
                'create', 'make', 'build', 'design', 'artistic',
                'original', 'unique', 'innovative', 'express'
            ],
            'learning_oriented': [
                'learn', 'study', 'education', 'knowledge', 'skill',
                'improve', 'develop', 'grow', 'understand', 'research'
            ],
            'health_conscious': [
                'healthy', 'wellness', 'fitness', 'nutrition', 'organic',
                'exercise', 'mindful', 'balance', 'wellbeing'
            ],
            'time_preferences': {
                'morning': ['morning', 'early', 'dawn', 'sunrise', 'am'],
                'evening': ['evening', 'night', 'sunset', 'pm', 'late'],
                'weekend': ['weekend', 'saturday', 'sunday', 'days off'],
                'weekday': ['weekday', 'workday', 'monday', 'friday']
            }
        }
    
    def analyze_behavior(self, text: str, social_data: Dict[str, str]) -> Dict[str, Any]:
        """Analyze behavioral patterns from user data"""
        text_lower = text.lower() if text else ""
        patterns = {}
        
        # Analyze text for behavioral indicators
        for pattern_type, keywords in self.behavior_patterns.items():
            if pattern_type == 'time_preferences':
                patterns[pattern_type] = {}
                for time_type, time_keywords in keywords.items():
                    score = sum(1 for keyword in time_keywords if keyword in text_lower)
                    patterns[pattern_type][time_type] = score / len(time_keywords)
            else:
                score = sum(1 for keyword in keywords if keyword in text_lower)
                patterns[pattern_type] = score / len(keywords)
        
        # Analyze social data for platform preferences
        platform_activity = {}
        for platform, handle in social_data.items():
            if handle:
                platform_activity[platform] = self._infer_platform_behavior(platform)
        
        patterns['platform_preferences'] = platform_activity
        
        return patterns
    
    def _infer_platform_behavior(self, platform: str) -> Dict[str, str]:
        """Infer behavioral traits from social platform usage"""
        platform_traits = {
            'twitter': {'communication': 'concise', 'engagement': 'public', 'content': 'news_opinion'},
            'instagram': {'communication': 'visual', 'engagement': 'aesthetic', 'content': 'lifestyle'},
            'linkedin': {'communication': 'professional', 'engagement': 'networking', 'content': 'career'},
            'github': {'communication': 'technical', 'engagement': 'collaborative', 'content': 'development'},
            'tiktok': {'communication': 'creative', 'engagement': 'viral', 'content': 'entertainment'},
            'youtube': {'communication': 'educational', 'engagement': 'educational', 'content': 'learning'}
        }
        return platform_traits.get(platform, {'communication': 'social', 'engagement': 'casual'})


class EnhancedUserProfilingService:
    """Main service for enhanced user profiling"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.interest_extractor = AdvancedInterestExtractor(config)
        self.behavior_analyzer = BehavioralAnalyzer()
        self.executor = ThreadPoolExecutor(max_workers=4)
    
    def create_enhanced_profile(
        self, 
        name: str, 
        location: Dict[str, Any], 
        activity: str,
        social_data: Dict[str, str] = None,
        search_results: Dict[str, Any] = None
    ) -> UserProfile:
        """Create comprehensive user profile with advanced analysis"""
        
        logger.info(f"Creating enhanced profile for {name}")
        
        profile = UserProfile(
            name=name,
            location=location,
            activity=activity,
            social_data=social_data or {}
        )
        
        # Extract interests from activity description
        if activity:
            activity_interests = self.interest_extractor.extract_interests(
                activity, 'user_input', 'primary_activity'
            )
            for interest in activity_interests:
                profile.add_interest(interest)
        
        # Extract interests from search results if available
        if search_results:
            self._process_search_results(profile, search_results)
        
        # Analyze behavioral patterns
        all_text = f"{activity} {' '.join(social_data.values() if social_data else [])}"
        profile.behavioral_patterns = self.behavior_analyzer.analyze_behavior(
            all_text, social_data or {}
        )
        
        # Infer demographic hints
        profile.demographic_hints = self._infer_demographics(profile)
        
        # Analyze activity context
        profile.activity_context = self._analyze_activity_context(activity)
        
        # Set preferences based on analysis
        profile.preferences = self._generate_preferences(profile)
        
        # Calculate profile completion
        profile.calculate_completion()
        
        logger.info(f"Enhanced profile created with {len(profile.interests)} interests "
                   f"and {profile.profile_completion:.1f}% completion")
        
        return profile
    
    def _process_search_results(self, profile: UserProfile, search_results: Dict[str, Any]):
        """Process search results to extract additional interests and context"""
        
        # Process search summaries
        summaries = search_results.get('search_summaries', {})
        for source, summary in summaries.items():
            if summary and isinstance(summary, str):
                interests = self.interest_extractor.extract_interests(
                    summary, f'search_{source}', source
                )
                for interest in interests:
                    profile.add_interest(interest)
        
        # Process raw search results if available
        raw_results = search_results.get('search_results', {})
        for source, results in raw_results.items():
            if isinstance(results, list):
                for result in results[:3]:  # Process top 3 results per source
                    if isinstance(result, dict):
                        content = result.get('content', '')
                        if content:
                            interests = self.interest_extractor.extract_interests(
                                content, f'search_{source}', 'web_content'
                            )
                            for interest in interests:
                                profile.add_interest(interest)
    
    def _infer_demographics(self, profile: UserProfile) -> Dict[str, Any]:
        """Infer demographic hints from profile data"""
        demographics = {}
        
        # Age group inference from interests and language
        young_adult_indicators = ['college', 'university', 'student', 'party', 'club', 'gaming']
        middle_age_indicators = ['family', 'career', 'professional', 'mortgage', 'kids']
        senior_indicators = ['retirement', 'grandchildren', 'volunteer', 'garden']
        
        all_text = f"{profile.activity} {' '.join([i.evidence for i in profile.interests])}"
        text_lower = all_text.lower()
        
        young_score = sum(1 for indicator in young_adult_indicators if indicator in text_lower)
        middle_score = sum(1 for indicator in middle_age_indicators if indicator in text_lower)
        senior_score = sum(1 for indicator in senior_indicators if indicator in text_lower)
        
        if young_score > middle_score and young_score > senior_score:
            demographics['age_group_hint'] = 'young_adult'
        elif middle_score > senior_score:
            demographics['age_group_hint'] = 'middle_age'
        elif senior_score > 0:
            demographics['age_group_hint'] = 'senior'
        
        # Lifestyle inference
        if any(cat in ['fitness', 'sports', 'nature'] for cat in [i.category for i in profile.interests]):
            demographics['lifestyle_hint'] = 'active'
        elif any(cat in ['arts', 'music', 'technology'] for cat in [i.category for i in profile.interests]):
            demographics['lifestyle_hint'] = 'creative'
        elif any(cat in ['food', 'travel', 'culture'] for cat in [i.category for i in profile.interests]):
            demographics['lifestyle_hint'] = 'experiential'
        
        return demographics
    
    def _analyze_activity_context(self, activity: str) -> Dict[str, Any]:
        """Analyze the context and intent behind user's activity request"""
        if not activity:
            return {}
        
        activity_lower = activity.lower()
        context = {}
        
        # Intent analysis
        if any(word in activity_lower for word in ['want', 'need', 'looking for', 'search']):
            context['intent'] = 'seeking'
        elif any(word in activity_lower for word in ['love', 'enjoy', 'passion']):
            context['intent'] = 'pursuing_interest'
        elif any(word in activity_lower for word in ['learn', 'try', 'new']):
            context['intent'] = 'exploring'
        
        # Urgency analysis
        if any(word in activity_lower for word in ['tonight', 'today', 'now', 'immediate']):
            context['urgency'] = 'high'
        elif any(word in activity_lower for word in ['weekend', 'soon', 'this week']):
            context['urgency'] = 'medium'
        else:
            context['urgency'] = 'low'
        
        # Social context
        if any(word in activity_lower for word in ['with friends', 'group', 'family', 'date']):
            context['social_setting'] = 'group'
        elif any(word in activity_lower for word in ['alone', 'solo', 'myself']):
            context['social_setting'] = 'solo'
        else:
            context['social_setting'] = 'flexible'
        
        # Budget hints
        if any(word in activity_lower for word in ['free', 'cheap', 'budget', 'affordable']):
            context['budget_preference'] = 'low'
        elif any(word in activity_lower for word in ['premium', 'high-end', 'luxury', 'expensive']):
            context['budget_preference'] = 'high'
        else:
            context['budget_preference'] = 'medium'
        
        return context
    
    def _generate_preferences(self, profile: UserProfile) -> Dict[str, Any]:
        """Generate user preferences based on profile analysis"""
        preferences = {}
        
        # Event type preferences based on interests
        top_interests = profile.get_top_interests(5)
        preferences['preferred_categories'] = [i.category for i in top_interests]
        
        # Time preferences from behavioral patterns
        time_prefs = profile.behavioral_patterns.get('time_preferences', {})
        if time_prefs:
            preferred_time = max(time_prefs, key=time_prefs.get)
            preferences['preferred_time'] = preferred_time
        
        # Social preferences
        if profile.behavioral_patterns.get('social_learner', 0) > 0.3:
            preferences['social_preference'] = 'group'
        elif profile.behavioral_patterns.get('solo_activities', 0) > 0.3:
            preferences['social_preference'] = 'solo'
        else:
            preferences['social_preference'] = 'flexible'
        
        # Activity style preferences
        if profile.behavioral_patterns.get('adventure_seeker', 0) > 0.3:
            preferences['activity_style'] = 'adventurous'
        elif profile.behavioral_patterns.get('learning_oriented', 0) > 0.3:
            preferences['activity_style'] = 'educational'
        elif profile.behavioral_patterns.get('creative_expression', 0) > 0.3:
            preferences['activity_style'] = 'creative'
        else:
            preferences['activity_style'] = 'balanced'
        
        # Budget preference from activity context
        budget_pref = profile.activity_context.get('budget_preference', 'medium')
        preferences['budget_preference'] = budget_pref
        
        return preferences
    
    def get_recommendation_context(self, profile: UserProfile) -> Dict[str, Any]:
        """Generate context for event recommendations"""
        return {
            'user_profile': {
                'name': profile.name,
                'location': profile.location,
                'primary_activity': profile.activity,
                'completion_score': profile.profile_completion
            },
            'interests': [i.to_dict() for i in profile.get_top_interests(10)],
            'preferences': profile.preferences,
            'behavioral_patterns': profile.behavioral_patterns,
            'activity_context': profile.activity_context,
            'demographic_hints': profile.demographic_hints
        }

========================================
# File: /home/jarvis/Documents/GitHub/WhatNowAI/services/mapping_service.py
========================================

"""
Mapping service for interactive event visualization

This module manages map markers, event categorization, and geographical data
for the interactive map interface. Supports filtering, searching, and real-time
event display with category-based organization and statistics.
"""

import logging
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import json

logger = logging.getLogger(__name__)


@dataclass
class MapMarker:
    """Map marker data structure"""
    id: str
    name: str
    latitude: float
    longitude: float
    category: str
    subcategory: str = ""
    description: str = ""
    url: str = ""
    date: str = ""
    time: str = ""
    venue: str = ""
    address: str = ""
    price_min: Optional[float] = None
    price_max: Optional[float] = None
    image_url: str = ""
    source: str = "unknown"  # ticketmaster, eventbrite, etc.
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert marker to dictionary for JSON serialization"""
        return {
            'id': self.id,
            'name': self.name,
            'latitude': self.latitude,
            'longitude': self.longitude,
            'category': self.category,
            'subcategory': self.subcategory,
            'description': self.description,
            'url': self.url,
            'date': self.date,
            'time': self.time,
            'venue': self.venue,
            'address': self.address,
            'price_min': self.price_min,
            'price_max': self.price_max,
            'image_url': self.image_url,
            'source': self.source
        }


class MappingService:
    """Service for aggregating events from multiple APIs and displaying on a map"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        Initialize mapping service
        
        Args:
            config: Configuration dictionary
        """
        self.config = config
        self.markers = []
        
    def clear_markers(self):
        """Clear all markers"""
        self.markers = []
    
    def add_ticketmaster_events(self, events: List[Any]):
        """Add events from Ticketmaster to the map"""
        for event in events:
            marker = MapMarker(
                id=f"tm_{event.id}",
                name=event.name,
                latitude=event.latitude,
                longitude=event.longitude,
                category=event.category,
                subcategory=event.subcategory,
                description=event.description,
                url=event.url,
                date=event.date,
                time=event.time,
                venue=event.venue,
                address=event.address,
                price_min=event.price_min,
                price_max=event.price_max,
                image_url=event.image_url,
                source="ticketmaster"
            )
            self.markers.append(marker)
    
    def add_allevents_events(self, events: List[Any]):
        """Add events from AllEvents to the map"""
        for event in events:
            marker = MapMarker(
                id=f"ae_{event.id}",
                name=event.name,
                latitude=event.latitude,
                longitude=event.longitude,
                category=event.category,
                subcategory=event.subcategory,
                description=event.description,
                url=event.url,
                date=event.date,
                time=event.time,
                venue=event.venue,
                address=event.address,
                price_min=event.price_min,
                price_max=event.price_max,
                image_url=event.image_url,
                source="allevents"
            )
            self.markers.append(marker)

    def add_eventbrite_events(self, events: List[Dict[str, Any]]):
        """Add events from Eventbrite to the map (placeholder for future integration)"""
        for event in events:
            try:
                marker = MapMarker(
                    id=f"eb_{event.get('id', '')}",
                    name=event.get('name', {}).get('text', 'Unknown Event'),
                    latitude=float(event.get('venue', {}).get('latitude', 0)),
                    longitude=float(event.get('venue', {}).get('longitude', 0)),
                    category=event.get('category', 'miscellaneous'),
                    description=event.get('description', {}).get('text', ''),
                    url=event.get('url', ''),
                    date=event.get('start', {}).get('local', '').split('T')[0],
                    time=event.get('start', {}).get('local', '').split('T')[1] if 'T' in event.get('start', {}).get('local', '') else '',
                    venue=event.get('venue', {}).get('name', ''),
                    address=event.get('venue', {}).get('address', {}).get('localized_address_display', ''),
                    source="eventbrite"
                )
                
                if marker.latitude and marker.longitude:
                    self.markers.append(marker)
                    
            except Exception as e:
                logger.warning(f"Failed to parse Eventbrite event: {e}")
                continue
    
    def add_meetup_events(self, events: List[Dict[str, Any]]):
        """Add events from Meetup to the map (placeholder for future integration)"""
        for event in events:
            try:
                venue = event.get('venue', {})
                marker = MapMarker(
                    id=f"mu_{event.get('id', '')}",
                    name=event.get('name', 'Unknown Event'),
                    latitude=float(venue.get('lat', 0)),
                    longitude=float(venue.get('lon', 0)),
                    category='meetup',
                    description=event.get('description', ''),
                    url=event.get('link', ''),
                    date=event.get('local_date', ''),
                    time=event.get('local_time', ''),
                    venue=venue.get('name', ''),
                    address=venue.get('address_1', ''),
                    source="meetup"
                )
                
                if marker.latitude and marker.longitude:
                    self.markers.append(marker)
                    
            except Exception as e:
                logger.warning(f"Failed to parse Meetup event: {e}")
                continue
    
    def add_custom_locations(self, locations: List[Dict[str, Any]]):
        """Add custom locations to the map"""
        for location in locations:
            try:
                marker = MapMarker(
                    id=f"custom_{location.get('id', len(self.markers))}",
                    name=location.get('name', 'Custom Location'),
                    latitude=float(location.get('latitude', 0)),
                    longitude=float(location.get('longitude', 0)),
                    category=location.get('category', 'custom'),
                    description=location.get('description', ''),
                    url=location.get('url', ''),
                    address=location.get('address', ''),
                    source="custom"
                )
                
                if marker.latitude and marker.longitude:
                    self.markers.append(marker)
                    
            except Exception as e:
                logger.warning(f"Failed to parse custom location: {e}")
                continue
    
    def get_markers_by_category(self, category: str) -> List[MapMarker]:
        """Get all markers for a specific category"""
        return [marker for marker in self.markers if marker.category.lower() == category.lower()]
    
    def get_markers_by_source(self, source: str) -> List[MapMarker]:
        """Get all markers from a specific source"""
        return [marker for marker in self.markers if marker.source.lower() == source.lower()]
    
    def get_all_markers(self) -> List[MapMarker]:
        """Get all markers"""
        return self.markers
    
    def get_map_data(self, center_lat: float, center_lng: float) -> Dict[str, Any]:
        """
        Get map data for frontend display
        
        Args:
            center_lat: Center latitude for the map
            center_lng: Center longitude for the map
            
        Returns:
            Dictionary containing map configuration and markers
        """
        # Limit markers for performance
        max_markers = self.config.get('MAX_MARKERS', 50)
        limited_markers = self.markers[:max_markers]
        
        # Group markers by category for better organization
        categories = {}
        for marker in limited_markers:
            category = marker.category
            if category not in categories:
                categories[category] = []
            categories[category].append(marker.to_dict())
        
        return {
            'center': {
                'latitude': center_lat,
                'longitude': center_lng
            },
            'zoom': self.config.get('DEFAULT_ZOOM', 12),
            'markers': [marker.to_dict() for marker in limited_markers],
            'categories': categories,
            'total_markers': len(self.markers),
            'sources': list(set(marker.source for marker in self.markers)),
            'tile_server': self.config.get('TILE_SERVER', 'https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png'),
            'attribution': self.config.get('ATTRIBUTION', '&copy; OpenStreetMap contributors')
        }
    
    def get_category_stats(self) -> Dict[str, int]:
        """Get statistics about markers by category"""
        stats = {}
        for marker in self.markers:
            category = marker.category
            stats[category] = stats.get(category, 0) + 1
        return stats
    
    def filter_markers_by_distance(self, center_lat: float, center_lng: float, 
                                 max_distance_km: float) -> List[MapMarker]:
        """
        Filter markers by distance from a center point
        
        Args:
            center_lat: Center latitude
            center_lng: Center longitude
            max_distance_km: Maximum distance in kilometers
            
        Returns:
            List of markers within the specified distance
        """
        import math
        
        def haversine_distance(lat1, lon1, lat2, lon2):
            """Calculate the haversine distance between two points"""
            R = 6371  # Earth's radius in kilometers
            
            lat1_rad = math.radians(lat1)
            lon1_rad = math.radians(lon1)
            lat2_rad = math.radians(lat2)
            lon2_rad = math.radians(lon2)
            
            dlat = lat2_rad - lat1_rad
            dlon = lon2_rad - lon1_rad
            
            a = math.sin(dlat/2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(dlon/2)**2
            c = 2 * math.asin(math.sqrt(a))
            
            return R * c
        
        filtered_markers = []
        for marker in self.markers:
            distance = haversine_distance(
                center_lat, center_lng,
                marker.latitude, marker.longitude
            )
            if distance <= max_distance_km:
                filtered_markers.append(marker)
        
        return filtered_markers
    
    def search_markers(self, query: str) -> List[MapMarker]:
        """
        Search markers by name, description, or venue
        
        Args:
            query: Search query string
            
        Returns:
            List of matching markers
        """
        query_lower = query.lower()
        matching_markers = []
        
        for marker in self.markers:
            if (query_lower in marker.name.lower() or
                query_lower in marker.description.lower() or
                query_lower in marker.venue.lower() or
                query_lower in marker.category.lower()):
                matching_markers.append(marker)
        
        return matching_markers

========================================
# File: /home/jarvis/Documents/GitHub/WhatNowAI/services/__init__.py
========================================

# Services package

========================================
# File: /home/jarvis/Documents/GitHub/WhatNowAI/services/tts_service.py
========================================

"""
Text-to-Speech service using Microsoft Edge TTS

This module provides text-to-speech functionality for the WhatNowAI application,
including dynamic text generation for onboarding steps and audio file management.
"""
import asyncio
import edge_tts
import os
import uuid
from typing import Optional, Tuple, Dict
import logging
from datetime import datetime

logger = logging.getLogger(__name__)


class TTSService:
    """Text-to-Speech service for generating audio from text"""
    
    def __init__(self, audio_dir: str, voice: str = "en-US-JennyNeural"):
        """
        Initialize TTS service
        
        Args:
            audio_dir: Directory to save audio files
            voice: Voice to use for TTS
        """
        self.audio_dir = audio_dir
        self.voice = voice
        self._ensure_audio_dir()
    
    def _ensure_audio_dir(self) -> None:
        """Ensure audio directory exists"""
        os.makedirs(self.audio_dir, exist_ok=True)
    
    async def generate_audio(self, text: str, voice: Optional[str] = None) -> Tuple[Optional[str], Optional[str]]:
        """
        Generate audio from text using edge-tts
        
        Args:
            text: Text to convert to speech
            voice: Voice to use (optional, uses default if not provided)
            
        Returns:
            Tuple of (audio_id, audio_path) or (None, None) if failed
        """
        try:
            if not text.strip():
                logger.warning("Empty text provided for TTS generation")
                return None, None
            
            # Create unique filename
            audio_id = str(uuid.uuid4())
            audio_path = os.path.join(self.audio_dir, f"{audio_id}.mp3")
            
            # Use provided voice or default
            selected_voice = voice or self.voice
            
            # Generate speech
            communicate = edge_tts.Communicate(text, selected_voice)
            await communicate.save(audio_path)
            
            logger.info(f"Audio generated successfully: {audio_id}")
            return audio_id, audio_path
            
        except Exception as e:
            logger.error(f"TTS Error: {e}")
            return None, None
    
    def generate_audio_sync(self, text: str, voice: Optional[str] = None) -> Tuple[Optional[str], Optional[str]]:
        """
        Synchronous wrapper for TTS generation
        
        Args:
            text: Text to convert to speech
            voice: Voice to use (optional)
            
        Returns:
            Tuple of (audio_id, audio_path) or (None, None) if failed
        """
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            return loop.run_until_complete(self.generate_audio(text, voice))
        except Exception as e:
            logger.error(f"TTS Sync Error: {e}")
            return None, None
        finally:
            loop.close()
    
    def get_audio_path(self, audio_id: str) -> str:
        """Get full path for audio file"""
        return os.path.join(self.audio_dir, f"{audio_id}.mp3")
    
    def audio_exists(self, audio_id: str) -> bool:
        """Check if audio file exists"""
        return os.path.exists(self.get_audio_path(audio_id))
    
    def cleanup_old_audio(self, max_age_hours: int = 24) -> None:
        """Clean up old audio files"""
        try:
            import time
            current_time = time.time()
            max_age_seconds = max_age_hours * 3600
            
            for filename in os.listdir(self.audio_dir):
                if filename.endswith('.mp3'):
                    file_path = os.path.join(self.audio_dir, filename)
                    file_age = current_time - os.path.getctime(file_path)
                    
                    if file_age > max_age_seconds:
                        try:
                            os.remove(file_path)
                            logger.info(f"Cleaned up old audio file: {filename}")
                        except OSError as e:
                            logger.warning(f"Failed to remove old audio file {filename}: {e}")
                            
        except Exception as e:
            logger.error(f"Error during audio cleanup: {e}")


def get_time_based_greeting() -> str:
    """Get time-appropriate greeting"""
    hour = datetime.now().hour
    
    if 5 <= hour < 12:
        return "Good morning"
    elif 12 <= hour < 17:
        return "Good afternoon"
    elif 17 <= hour < 22:
        return "Good evening"
    else:
        return "Hello"


def get_introduction_text(step: str, location_data: Optional[Dict] = None) -> str:
    """
    Generate dynamic introduction text based on time, location, and step
    
    Args:
        step: The onboarding step
        location_data: Optional location information
        
    Returns:
        Personalized introduction text
    """
    greeting = get_time_based_greeting()
    
    # Extract location info if available
    location_context = ""
    if location_data:
        city = location_data.get('city', '')
        country = location_data.get('country', '')
        if city and country:
            location_context = f" from {city}, {country}"
        elif country:
            location_context = f" from {country}"
    
    texts = {
        "step_name": f"Welcome to WhatNow AI! First, I'd love to know your name!",
        
        "step_activity": f"Perfect! Now tell me, what would you like to do today?",
        
        "step_location": f"Great choice! To give you the best local recommendations, I'll need to know where you are.",
        
        "processing": f"Excellent! Now I'm creating your personalized recommendations. This will just take a moment."
    }
    
    return texts.get(step, "Let's continue!")


# Backward compatibility - keep static texts as fallback
INTRODUCTION_TEXTS = {
    "step_name": "First, what's your name? You can also share social media handles for better recommendations.",
    "step_activity": "Perfect! Now tell me, what would you like to do today?",
    "step_location": "Great! To give you local recommendations, I'll need your location. You can share it or skip this step.",
    "processing": "Excellent! I'm creating your personalized action plan. Just a moment please."
}
