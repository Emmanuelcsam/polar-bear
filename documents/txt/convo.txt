I'll analyze this technical conversation about developing an automated fiber optic defect detection system. Let me break down every aspect in detail and then research the technical concepts discussed.

## Comprehensive Conversation Analysis

### **Project Overview**
- **Main Goal**: Building an automated system to detect defects in fiber optic cables
- **Approach**: Combining traditional computer vision (OpenCV) with machine learning (PyTorch)
- **Key Challenge**: Moving from manual debugging to automated detection with real-time processing capabilities

### **Data Library Development**
- **Purpose**: Creating comprehensive dataset of fiber optic defects
- **Components**:
  - Scratches
  - Blobs
  - Contamination anomalies
  - Digs (surface defects)
- **Method**: 
  - Taking clean fiber optic images
  - Applying scratch library overlays to create synthetic defect data
  - Organizing by fiber type (SMA, FC) and diameter specifications
- **Innovation**: Automated script to generate large-scale defect dataset

### **Technical Implementation Details**

#### **Neural Network Architecture**
- **Framework**: PyTorch (not TensorFlow as initially mentioned)
- **Process Flow**:
  1. Convert images to numpy arrays
  2. Transform arrays to PyTorch tensors
  3. Convert to floating point for RAM efficiency
  4. Flatten 2D images to 1D linear systems
  5. Reduce features from ~900,000 pixels to 512 features
  6. Final classification to 10 output categories

#### **Matrix Operations Explained**
- **Tensor Definition**: Multi-dimensional array (3D matrix analogy using Rubik's cube)
- **Linear System Conversion**: 
  - Matrix elements → Linear equations
  - Example: 3x3 matrix → ax₁ + bx₂ + cx₃ format
- **Feature Reduction**: From full pixel dimensions (1152×864) to manageable feature sets

### **Detection Algorithms Currently Used**

1. **Structural Similarity Index (SSIM)**
   - Template matching approach
   - Compares reference vs. test images
   - Sums pixel differences divided by total

2. **Local Binary Patterns (LBP)**
   - Analyzes surrounding pixel brightness
   - Detects patterns in local neighborhoods

3. **Gray Level Co-occurrence Matrix (GLCM)**
   - Statistical texture analysis
   - Different approach than LBP for pattern detection

4. **Fourier Analysis**
   - Frequency domain analysis
   - High frequencies = scratches/fine details
   - Low frequencies = overall features/blobs

5. **Hough Transform**
   - Line detection for scratches
   - Circle detection for fiber boundaries

### **Processing Workflow**

#### **Current Process**
- **Rule Set Input**: 
  - Fiber type specification
  - Diameter parameters
  - Expected defect types
- **Manual Alignment**: Circle detection and alignment
- **Detection Time**: ~20 minutes per image (major bottleneck)

#### **Proposed Improvements**
- **Real-time Processing Goal**: 1-2 seconds per image
- **Smart Studio Tool**: Automatically finds processing chain between input/output images
- **Feedback System**: Click-to-learn defect identification

### **Specific Technical Challenges Discussed**

1. **Image Standardization**
   - All images must be same dimensions (1152×864)
   - Pixel size normalization required
   - Different fiber types need separate classification

2. **Processing Speed**
   - Current: 20 minutes for detection alone
   - Target: Real-time (1-2 seconds)
   - Solution: GPU acceleration via HPC or cloud computing

3. **Circle Detection Issues**
   - Manual alignment frustrations
   - Concentricity measurements (deemed unnecessary)
   - Proposed: Click-and-drag or auto-suggest circles

4. **Classification System**
   - Numeric encoding for fiber types (SMA=1, FC=375, etc.)
   - Matrix representation of processing methods
   - Output categorization for defect types

### **Future Development Plans**

1. **HPC Implementation**
   - Transfer to school's high-performance computing system
   - Convert from CPU to GPU processing
   - Parallel operations for faster training

2. **Cloud Computing Option**
   - Amazon AWS consideration
   - Cost-benefit analysis needed

3. **User Interface Vision**
   - Simple three-step process: Rule Set → Detection → Run
   - Real-time video overlay of defects
   - Minimal user intervention

### **Key Technical Insights**

- **Depth Analysis**: Using lighting angles to determine if defects are craters (inward) or protrusions (outward)
- **D-Scope Reference**: Trying to match commercial instrument capabilities
- **Proprietary Concerns**: Difficulty finding existing defect libraries due to proprietary nature

## Research on Technical Concepts## Detailed Technical Elaboration from Research

### **Fiber Optic Defect Detection - Industry Context**

Based on the research, the conversation participants are working on solving a critical industry problem. The average detection accuracy of the POL detection method for area-based defects is no less than 97.14%. Compared with the existing traditional manual naked eye inspection, the accuracy of the method is improved by about 20% and the efficiency is improved by about 6 to 7 times.

### **Current Industry Standards and Challenges**

Traditional end face inspections are usually performed by manual visual methods, which are low in efficiency and poor in precision for long-term industrial applications. More seriously, the inspection results cannot be quantified for subsequent analysis. The conversation aligns with industry needs where Industry standards such as IEC-61300-3-35 are commonly referenced when determining acceptance criteria for an acceptable fiber connector end face.

### **Processing Speed Requirements**

The conversation's goal of achieving 1-2 second processing time is ambitious compared to current solutions. The computational time of the DO2MR model for an image of 1600 × 1200 pixel for region-based defects was 620 ms on average, and that of the LEI model for scratches was 2240 ms on average, with a searching angle of ... ∘. This suggests their 20-minute processing time is far from industry standards.

### **Technical Concepts Deep Dive**

#### **PyTorch Tensor Operations**
The conversation's explanation of tensor flattening aligns with standard practices:
- Flattens a contiguous range of dims into a tensor. For use with Sequential, see torch.flatten() for details.
- The easiest way to flatten tensors in PyTorch is by using Python's built-in torch.nn.Flatten module.
- The process of reducing 900,000 pixels (1152×864) to 512 features is standard dimensionality reduction

#### **Computer Vision Algorithms for Defect Detection**

1. **SSIM (Structural Similarity Index)**
   - Used as template matching in the conversation
   - Industry applications show it's effective for quality assessment

2. **Local Binary Patterns (LBP)**
   - Local Binary Pattern implementations can be found in both the scikit-image and mahotas packages.
   - The conversation correctly identifies LBP as analyzing surrounding pixel brightness

3. **Gray Level Co-occurrence Matrix (GLCM)**
   - Unlike simple pixel intensity-based features, GLCM considers the relationships between neighboring pixels, providing rich texture information.
   - A GLCM is a histogram of co-occurring grayscale values at a given offset over an image.

4. **Fourier Analysis**
   - The conversation's explanation of high frequencies representing scratches and low frequencies representing blobs is accurate
   - This is a standard approach in frequency domain analysis

### **Real-Time Processing Challenges**

The conversation's struggle with processing speed is common in the industry:
- Finding the cable or port to test can be tough, especially in the low light conditions found in most data centers and wiring closets.
- This state-of-the-art sensor system is small, lightweight, easy to install, and fast—it processes information at rates of 100 times per second.

### **Defect Types and Detection Methods**

Research confirms the defect types mentioned:
- Common types of contamination and defects in optical fiber end faces. From (a–d): dirt, oil, pit and chip, and scratch types.
- Aiming at the characteristics of typical defects in the inspection process for optical fiber end faces, we propose a novel method, "difference of min-max ranking filtering" (DO2MR), for detection of region-based defects, e.g., dirt, oil, contamination, pits, and chips, and a special model, a "linear enhancement inspector" (LEI), for the detection of scratches.

### **Automated vs Manual Inspection**

The conversation's goal of automation aligns with industry trends:
- Automating the fiber optic inspection process using programmable software eliminates the variables associated with manual fiber inspection and provides a documentable and certifiable record of the quality of the connector end face at the point of installation.
- One of industry's longtime concerns with manually inspecting fiber endfaces has been that determining cleanliness has largely been a subjective and inconsistent process.

### **Key Technical Insights from Research**

1. **Circle Detection Challenges**: The conversation's frustration with circle detection is valid - precise alignment is crucial for fiber optic inspection

2. **GPU Acceleration**: Moving to HPC/GPU processing is the right approach, as shown by industry trends toward real-time processing

3. **Data Library Creation**: The synthetic data generation approach (overlaying scratches on clean images) is a valid technique for training data augmentation

4. **Multi-Algorithm Approach**: Using multiple detection algorithms (SSIM, LBP, GLCM, Fourier) and combining their outputs is aligned with best practices in ensemble methods

### **Industry Best Practices Not Mentioned**

1. **IEC 61300-3-35 Standard Compliance**: The conversation doesn't mention this critical industry standard for fiber inspection
2. **Automated Pass/Fail Criteria**: Industry tools provide automated pass/fail based on standardized criteria
3. **Zone-Based Analysis**: Professional systems analyze different zones (core, cladding, adhesive) separately
4. **Certification Requirements**: Documentation and traceability for compliance

### **Future Considerations**

The project could benefit from:
- Implementing standardized defect classification based on IEC standards
- Focusing on specific defect types that most impact signal quality
- Considering edge computing solutions for real-time processing
- Implementing proper calibration procedures for different fiber types

# Comprehensive Technical Analysis: Automated Fiber Optic Defect Detection System

## Executive Summary

This project represents a sophisticated approach to automating fiber optic connector end-face inspection, combining traditional computer vision techniques with modern deep learning. The system aims to reduce inspection time from 20 minutes to 1-2 seconds while maintaining or exceeding industry-standard accuracy levels.# Comprehensive Technical Guide: Automated Fiber Optic Defect Detection System

## Table of Contents
1. [System Architecture Overview](#system-architecture)
2. [Data Library Development](#data-library)
3. [Neural Network Implementation](#neural-network)
4. [Detection Algorithms Detailed](#detection-algorithms)
5. [Real-time Processing Pipeline](#real-time-processing)
6. [HPC/GPU Implementation](#hpc-gpu)
7. [Development Resources](#resources)
8. [Implementation Roadmap](#roadmap)

---

## 1. System Architecture Overview {#system-architecture}

### **Core System Components**

The automated fiber optic defect detection system consists of five major architectural components:

1. **Data Acquisition Layer**
   - Camera interface for capturing fiber end-face images
   - Resolution: 1152×864 pixels (995,328 total pixels)
   - Image format: Grayscale for processing efficiency
   - Frame capture rate: Target 100 FPS for real-time processing

2. **Preprocessing Pipeline**
   - Noise reduction using Gaussian blur
   - Histogram equalization for contrast enhancement
   - Image normalization to [0,1] range
   - Circle detection for fiber boundary identification

3. **Detection Engine**
   - Multiple parallel detection algorithms
   - Consensus-based defect identification
   - Feature extraction and classification
   - Neural network integration for learned patterns

4. **Analysis Framework**
   - Statistical distance calculations
   - Anomaly scoring mechanisms
   - Defect type classification
   - Severity assessment

5. **Output Interface**
   - Real-time visualization overlay
   - Pass/fail determination
   - Detailed defect reporting
   - Quality metrics dashboard

### **System Flow Diagram**

```
Input Image → Preprocessing → Detection Algorithms → Neural Network
     ↓              ↓               ↓                    ↓
 Grayscale    Circle Detection  Feature Maps      Classification
     ↓              ↓               ↓                    ↓
 Normalized    ROI Extraction   Defect Maps      Confidence Scores
     ↓              ↓               ↓                    ↓
              Analysis Engine → Consensus → Output Decision
```

### **Performance Requirements**

- **Current State**: 20 minutes per image
- **Target State**: 1-2 seconds per image
- **Speedup Required**: 600-1200x improvement
- **Accuracy Target**: >97% detection rate (matching industry standards)

---

## 2. Data Library Development {#data-library}

### **Comprehensive Data Collection Methods**

#### **2.1 Synthetic Data Generation**

The synthetic data generation process creates a massive dataset by overlaying defects on clean fiber images:

```python
import cv2
import numpy as np
import os
from pathlib import Path

class SyntheticDefectGenerator:
    def __init__(self, clean_images_path, scratch_library_path, output_path):
        self.clean_images = self._load_images(clean_images_path)
        self.scratch_patterns = self._load_images(scratch_library_path)
        self.output_path = Path(output_path)
        
    def generate_synthetic_defects(self, num_samples=10000):
        """Generate synthetic defect images"""
        for idx in range(num_samples):
            # Select random clean image
            clean_img = self.clean_images[np.random.randint(len(self.clean_images))].copy()
            
            # Apply multiple defect types
            defect_img = self._apply_scratches(clean_img)
            defect_img = self._apply_contamination(defect_img)
            defect_img = self._apply_pits(defect_img)
            defect_img = self._apply_oil_spots(defect_img)
            
            # Save with metadata
            self._save_with_metadata(defect_img, idx)
    
    def _apply_scratches(self, img):
        """Apply synthetic scratches using scratch library"""
        num_scratches = np.random.randint(1, 5)
        for _ in range(num_scratches):
            scratch = self.scratch_patterns[np.random.randint(len(self.scratch_patterns))]
            # Random rotation
            angle = np.random.uniform(0, 180)
            scratch_rotated = self._rotate_image(scratch, angle)
            # Random position
            x, y = self._get_random_position(img, scratch_rotated)
            # Blend scratch onto image
            img = self._blend_images(img, scratch_rotated, x, y, alpha=0.7)
        return img
    
    def _apply_contamination(self, img):
        """Apply blob-like contamination"""
        num_blobs = np.random.randint(0, 3)
        for _ in range(num_blobs):
            # Generate random blob shape
            blob_size = np.random.randint(10, 50)
            blob = self._generate_blob(blob_size)
            x, y = self._get_random_position(img, blob)
            img = self._blend_images(img, blob, x, y, alpha=0.5)
        return img
```

#### **2.2 Data Organization Structure**

```
data_library/
├── fiber_types/
│   ├── SMA/
│   │   ├── 125um/
│   │   ├── 62.5um/
│   │   └── 50um/
│   ├── FC/
│   │   ├── single_mode/
│   │   └── multi_mode/
│   └── LC/
├── defect_types/
│   ├── scratches/
│   │   ├── linear/
│   │   ├── curved/
│   │   └── crossed/
│   ├── contamination/
│   │   ├── oil/
│   │   ├── dust/
│   │   └── particles/
│   ├── physical_damage/
│   │   ├── pits/
│   │   ├── chips/
│   │   └── cracks/
│   └── anomalies/
├── processing_masks/
│   ├── core_masks/
│   ├── cladding_masks/
│   └── ferrule_masks/
└── metadata/
    ├── defect_annotations.json
    ├── fiber_specifications.json
    └── processing_parameters.json
```

#### **2.3 Template Matching Studio**

The template matching studio automatically discovers optimal processing chains:

```python
class ProcessingStudio:
    def __init__(self):
        self.operations = {
            'blur': cv2.GaussianBlur,
            'threshold': cv2.threshold,
            'morph_open': lambda img, kernel: cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel),
            'morph_close': lambda img, kernel: cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel),
            'canny': cv2.Canny,
            'sobel': cv2.Sobel,
            'laplacian': cv2.Laplacian
        }
        
    def find_optimal_chain(self, input_img, target_img, max_steps=5):
        """Find processing chain that transforms input to target"""
        best_chain = []
        best_similarity = 0
        
        # Use genetic algorithm approach
        population = self._generate_initial_population(100)
        
        for generation in range(50):
            # Evaluate fitness
            fitness_scores = []
            for chain in population:
                processed = self._apply_chain(input_img, chain)
                similarity = self._calculate_similarity(processed, target_img)
                fitness_scores.append(similarity)
            
            # Select best chains
            best_indices = np.argsort(fitness_scores)[-20:]
            best_chains = [population[i] for i in best_indices]
            
            # Generate new population
            population = self._evolve_population(best_chains)
            
            # Track best
            if max(fitness_scores) > best_similarity:
                best_similarity = max(fitness_scores)
                best_chain = best_chains[0]
        
        return best_chain, best_similarity
```

#### **2.4 HPC Data Generation Pipeline**

```python
# SLURM job script for HPC data generation
#!/bin/bash
#SBATCH --job-name=fiber_defect_data
#SBATCH --nodes=4
#SBATCH --ntasks-per-node=8
#SBATCH --gpus-per-node=2
#SBATCH --time=24:00:00
#SBATCH --partition=gpu

module load cuda/11.7
module load python/3.9
module load opencv/4.5.5

# Distributed data generation
srun python distributed_data_gen.py \
    --world-size 32 \
    --samples-per-worker 1000 \
    --output-dir /scratch/fiber_defect_library
```

---

## 3. Neural Network Implementation {#neural-network}

### **3.1 Network Architecture Details**

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class FiberDefectNet(nn.Module):
    def __init__(self, input_channels=1, num_classes=10):
        super(FiberDefectNet, self).__init__()
        
        # Feature extraction backbone
        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(128)
        
        # Pooling layers
        self.pool = nn.MaxPool2d(2, 2)
        
        # Calculate flattened size
        # Input: 1152x864 -> After 3 pooling: 144x108
        self.flattened_size = 128 * 144 * 108
        
        # Fully connected layers
        self.fc1 = nn.Linear(self.flattened_size, 512)
        self.dropout1 = nn.Dropout(0.5)
        self.fc2 = nn.Linear(512, 256)
        self.dropout2 = nn.Dropout(0.5)
        self.fc3 = nn.Linear(256, num_classes)
        
        # Attention mechanism for defect localization
        self.attention = nn.Sequential(
            nn.Conv2d(128, 64, kernel_size=1),
            nn.ReLU(),
            nn.Conv2d(64, 1, kernel_size=1),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        # Feature extraction
        x = self.pool(F.relu(self.bn1(self.conv1(x))))
        x = self.pool(F.relu(self.bn2(self.conv2(x))))
        x = self.pool(F.relu(self.bn3(self.conv3(x))))
        
        # Apply attention
        attention_weights = self.attention(x)
        x = x * attention_weights
        
        # Flatten and classify
        x = x.view(-1, self.flattened_size)
        x = F.relu(self.fc1(x))
        x = self.dropout1(x)
        x = F.relu(self.fc2(x))
        x = self.dropout2(x)
        x = self.fc3(x)
        
        return x, attention_weights
```

### **3.2 Custom Loss Function for Defect Detection**

```python
class DefectDetectionLoss(nn.Module):
    def __init__(self, class_weights=None, focal_gamma=2.0):
        super(DefectDetectionLoss, self).__init__()
        self.class_weights = class_weights
        self.focal_gamma = focal_gamma
        
    def forward(self, predictions, targets, attention_maps=None):
        # Focal loss for handling class imbalance
        ce_loss = F.cross_entropy(predictions, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = ((1 - pt) ** self.focal_gamma) * ce_loss
        
        # Weight by class frequency if provided
        if self.class_weights is not None:
            weights = self.class_weights[targets]
            focal_loss = focal_loss * weights
        
        # Add attention regularization
        attention_loss = 0
        if attention_maps is not None:
            # Encourage attention to focus on defect regions
            attention_loss = self._attention_regularization(attention_maps, targets)
        
        total_loss = focal_loss.mean() + 0.1 * attention_loss
        return total_loss
    
    def _attention_regularization(self, attention_maps, targets):
        """Regularize attention to focus on defect areas"""
        # This would use ground truth defect masks if available
        # For now, just encourage sparsity
        return torch.mean(torch.sum(attention_maps.view(attention_maps.size(0), -1), dim=1))
```

### **3.3 Training Pipeline with Mixed Precision**

```python
from torch.cuda.amp import GradScaler, autocast
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

class DistributedTrainer:
    def __init__(self, model, rank, world_size):
        self.rank = rank
        self.world_size = world_size
        
        # Initialize distributed training
        dist.init_process_group(backend='nccl', rank=rank, world_size=world_size)
        
        # Move model to GPU
        self.device = torch.device(f'cuda:{rank}')
        self.model = model.to(self.device)
        self.model = DDP(self.model, device_ids=[rank])
        
        # Mixed precision training
        self.scaler = GradScaler()
        
        # Optimizer with different learning rates
        self.optimizer = torch.optim.AdamW([
            {'params': self.model.module.conv1.parameters(), 'lr': 1e-4},
            {'params': self.model.module.conv2.parameters(), 'lr': 1e-4},
            {'params': self.model.module.conv3.parameters(), 'lr': 1e-4},
            {'params': self.model.module.fc1.parameters(), 'lr': 1e-3},
            {'params': self.model.module.fc2.parameters(), 'lr': 1e-3},
            {'params': self.model.module.fc3.parameters(), 'lr': 1e-3},
        ])
        
        # Learning rate scheduler
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer, T_0=10, T_mult=2
        )
        
    def train_epoch(self, dataloader, loss_fn):
        self.model.train()
        total_loss = 0
        
        for batch_idx, (images, labels) in enumerate(dataloader):
            images = images.to(self.device)
            labels = labels.to(self.device)
            
            self.optimizer.zero_grad()
            
            # Mixed precision forward pass
            with autocast():
                predictions, attention = self.model(images)
                loss = loss_fn(predictions, labels, attention)
            
            # Backward pass with gradient scaling
            self.scaler.scale(loss).backward()
            
            # Gradient clipping
            self.scaler.unscale_(self.optimizer)
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            # Optimizer step
            self.scaler.step(self.optimizer)
            self.scaler.update()
            
            total_loss += loss.item()
            
            # Log progress
            if batch_idx % 100 == 0 and self.rank == 0:
                print(f'Batch {batch_idx}/{len(dataloader)}, Loss: {loss.item():.4f}')
        
        self.scheduler.step()
        return total_loss / len(dataloader)
```

### **3.4 Model Optimization for Real-time Inference**

```python
import torch.quantization as quantization
import torch.jit as jit

class ModelOptimizer:
    @staticmethod
    def optimize_for_deployment(model, example_input):
        """Optimize model for real-time inference"""
        
        # 1. Fuse conv-bn-relu layers
        model.eval()
        model = torch.quantization.fuse_modules(model, [
            ['conv1', 'bn1'],
            ['conv2', 'bn2'],
            ['conv3', 'bn3']
        ])
        
        # 2. Quantization-aware training setup
        model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')
        model = torch.quantization.prepare_qat(model)
        
        # 3. Convert to TorchScript
        traced_model = torch.jit.trace(model, example_input)
        
        # 4. Optimize graph
        traced_model = torch.jit.optimize_for_inference(traced_model)
        
        return traced_model
    
    @staticmethod
    def benchmark_model(model, input_shape=(1, 1, 1152, 864), num_runs=1000):
        """Benchmark model inference speed"""
        import time
        
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model = model.to(device)
        model.eval()
        
        # Warmup
        dummy_input = torch.randn(input_shape).to(device)
        for _ in range(10):
            _ = model(dummy_input)
        
        # Benchmark
        torch.cuda.synchronize()
        start_time = time.time()
        
        with torch.no_grad():
            for _ in range(num_runs):
                _ = model(dummy_input)
        
        torch.cuda.synchronize()
        end_time = time.time()
        
        avg_time = (end_time - start_time) / num_runs
        fps = 1.0 / avg_time
        
        print(f"Average inference time: {avg_time*1000:.2f} ms")
        print(f"FPS: {fps:.2f}")
        
        return avg_time, fps
```

---

## 4. Detection Algorithms Detailed {#detection-algorithms}

### **4.1 Structural Similarity Index Map (SSIM)**

Mathematical foundation and implementation:

```python
import numpy as np
from scipy.ndimage import gaussian_filter

class SSIM_Detector:
    def __init__(self, window_size=11, k1=0.01, k2=0.03):
        self.window_size = window_size
        self.k1 = k1
        self.k2 = k2
        
    def compute_ssim(self, img1, img2):
        """
        Compute SSIM between two images
        
        SSIM = [l(x,y)]^α * [c(x,y)]^β * [s(x,y)]^γ
        where:
        l(x,y) = (2μxμy + C1) / (μx² + μy² + C1)  [luminance]
        c(x,y) = (2σxσy + C2) / (σx² + σy² + C2)  [contrast]
        s(x,y) = (σxy + C3) / (σxσy + C3)         [structure]
        """
        # Convert to float
        img1 = img1.astype(np.float64)
        img2 = img2.astype(np.float64)
        
        # Constants
        L = 255  # Dynamic range
        C1 = (self.k1 * L) ** 2
        C2 = (self.k2 * L) ** 2
        C3 = C2 / 2
        
        # Compute means
        mu1 = gaussian_filter(img1, sigma=1.5)
        mu2 = gaussian_filter(img2, sigma=1.5)
        
        mu1_sq = mu1 ** 2
        mu2_sq = mu2 ** 2
        mu1_mu2 = mu1 * mu2
        
        # Compute variances and covariance
        sigma1_sq = gaussian_filter(img1 ** 2, sigma=1.5) - mu1_sq
        sigma2_sq = gaussian_filter(img2 ** 2, sigma=1.5) - mu2_sq
        sigma12 = gaussian_filter(img1 * img2, sigma=1.5) - mu1_mu2
        
        # Compute SSIM components
        luminance = (2 * mu1_mu2 + C1) / (mu1_sq + mu2_sq + C1)
        contrast = (2 * np.sqrt(sigma1_sq) * np.sqrt(sigma2_sq) + C2) / (sigma1_sq + sigma2_sq + C2)
        structure = (sigma12 + C3) / (np.sqrt(sigma1_sq) * np.sqrt(sigma2_sq) + C3)
        
        # Combine with default weights
        ssim_map = luminance * contrast * structure
        
        return ssim_map, np.mean(ssim_map)
    
    def detect_defects(self, test_img, reference_img, threshold=0.95):
        """Detect defects using SSIM"""
        ssim_map, mean_ssim = self.compute_ssim(test_img, reference_img)
        
        # Defects are regions with low SSIM
        defect_mask = ssim_map < threshold
        
        # Post-process to remove noise
        kernel = np.ones((3, 3), np.uint8)
        defect_mask = cv2.morphologyEx(defect_mask.astype(np.uint8), 
                                       cv2.MORPH_OPEN, kernel)
        
        return defect_mask, ssim_map, mean_ssim
```

### **4.2 Local Binary Patterns (LBP)**

Local Binary Pattern implementations can be found in both the scikit-image and mahotas packages.

```python
class LBP_Detector:
    def __init__(self, n_points=8, radius=1, method='uniform'):
        self.n_points = n_points
        self.radius = radius
        self.method = method
        
    def compute_lbp_pixel(self, img, center_y, center_x):
        """Compute LBP for a single pixel"""
        center_val = img[center_y, center_x]
        pattern = 0
        
        for i in range(self.n_points):
            # Calculate neighbor position
            angle = 2 * np.pi * i / self.n_points
            y = center_y + self.radius * np.sin(angle)
            x = center_x + self.radius * np.cos(angle)
            
            # Bilinear interpolation for non-integer positions
            neighbor_val = self._bilinear_interpolation(img, y, x)
            
            # Update pattern
            if neighbor_val >= center_val:
                pattern |= (1 << i)
        
        return pattern
    
    def compute_lbp_image(self, img):
        """Compute LBP for entire image"""
        h, w = img.shape
        lbp_img = np.zeros((h, w), dtype=np.uint8)
        
        for y in range(self.radius, h - self.radius):
            for x in range(self.radius, w - self.radius):
                lbp_img[y, x] = self.compute_lbp_pixel(img, y, x)
        
        return lbp_img
    
    def extract_lbp_features(self, img, n_bins=256):
        """Extract LBP histogram features"""
        lbp_img = self.compute_lbp_image(img)
        
        # Compute histogram
        hist, _ = np.histogram(lbp_img.ravel(), bins=n_bins, range=(0, n_bins))
        
        # Normalize
        hist = hist.astype(np.float32)
        hist /= (hist.sum() + 1e-6)
        
        return hist, lbp_img
    
    def detect_texture_anomalies(self, test_img, reference_imgs, threshold=0.1):
        """Detect anomalies based on LBP texture"""
        # Extract test image features
        test_hist, test_lbp = self.extract_lbp_features(test_img)
        
        # Compute reference distribution
        ref_hists = []
        for ref_img in reference_imgs:
            ref_hist, _ = self.extract_lbp_features(ref_img)
            ref_hists.append(ref_hist)
        
        ref_hists = np.array(ref_hists)
        mean_hist = np.mean(ref_hists, axis=0)
        std_hist = np.std(ref_hists, axis=0)
        
        # Compute chi-square distance
        chi_square = np.sum((test_hist - mean_hist) ** 2 / (mean_hist + 1e-6))
        
        # Detect anomalies
        is_anomaly = chi_square > threshold
        
        return is_anomaly, chi_square, test_lbp
```

### **4.3 Gray Level Co-occurrence Matrix (GLCM)**

Unlike simple pixel intensity-based features, GLCM considers the relationships between neighboring pixels, providing rich texture information.

```python
class GLCM_Detector:
    def __init__(self, distances=[1, 2, 3], angles=[0, np.pi/4, np.pi/2, 3*np.pi/4]):
        self.distances = distances
        self.angles = angles
        
    def compute_glcm(self, img, distance, angle, levels=256):
        """
        Compute GLCM for given distance and angle
        
        P(i,j|d,θ) = number of times gray level i occurs 
                     at distance d and angle θ from gray level j
        """
        h, w = img.shape
        glcm = np.zeros((levels, levels), dtype=np.float64)
        
        # Calculate offset based on angle and distance
        dx = int(np.round(distance * np.cos(angle)))
        dy = int(np.round(distance * np.sin(angle)))
        
        # Count co-occurrences
        for y in range(max(0, -dy), min(h, h - dy)):
            for x in range(max(0, -dx), min(w, w - dx)):
                i = img[y, x]
                j = img[y + dy, x + dx]
                glcm[i, j] += 1
        
        # Normalize
        glcm = glcm / glcm.sum()
        
        return glcm
    
    def extract_glcm_features(self, glcm):
        """Extract Haralick features from GLCM"""
        features = {}
        
        # Contrast: Σi,j |i-j|² * P(i,j)
        i, j = np.ogrid[0:glcm.shape[0], 0:glcm.shape[1]]
        features['contrast'] = np.sum((i - j) ** 2 * glcm)
        
        # Dissimilarity: Σi,j |i-j| * P(i,j)
        features['dissimilarity'] = np.sum(np.abs(i - j) * glcm)
        
        # Homogeneity: Σi,j P(i,j) / (1 + |i-j|)
        features['homogeneity'] = np.sum(glcm / (1.0 + np.abs(i - j)))
        
        # Energy (ASM): Σi,j P(i,j)²
        features['energy'] = np.sum(glcm ** 2)
        
        # Correlation
        mu_i = np.sum(i * glcm)
        mu_j = np.sum(j * glcm)
        sigma_i = np.sqrt(np.sum((i - mu_i) ** 2 * glcm))
        sigma_j = np.sqrt(np.sum((j - mu_j) ** 2 * glcm))
        
        if sigma_i > 0 and sigma_j > 0:
            features['correlation'] = np.sum((i - mu_i) * (j - mu_j) * glcm) / (sigma_i * sigma_j)
        else:
            features['correlation'] = 0
        
        # Entropy: -Σi,j P(i,j) * log(P(i,j))
        glcm_nonzero = glcm[glcm > 0]
        features['entropy'] = -np.sum(glcm_nonzero * np.log2(glcm_nonzero))
        
        return features
    
    def detect_texture_defects(self, test_img, reference_imgs):
        """Detect defects based on GLCM texture analysis"""
        # Extract features for test image
        test_features = []
        for d in self.distances:
            for a in self.angles:
                glcm = self.compute_glcm(test_img, d, a)
                features = self.extract_glcm_features(glcm)
                test_features.extend(list(features.values()))
        
        test_features = np.array(test_features)
        
        # Extract features for reference images
        ref_features = []
        for ref_img in reference_imgs:
            img_features = []
            for d in self.distances:
                for a in self.angles:
                    glcm = self.compute_glcm(ref_img, d, a)
                    features = self.extract_glcm_features(glcm)
                    img_features.extend(list(features.values()))
            ref_features.append(img_features)
        
        ref_features = np.array(ref_features)
        
        # Compute Mahalanobis distance
        mean_features = np.mean(ref_features, axis=0)
        cov_matrix = np.cov(ref_features.T)
        inv_cov = np.linalg.pinv(cov_matrix)
        
        diff = test_features - mean_features
        mahal_dist = np.sqrt(np.dot(np.dot(diff, inv_cov), diff.T))
        
        return mahal_dist, test_features
```

### **4.4 Fourier Transform Analysis**

The Fourier Transform is an important image processing tool which is used to decompose an image into its sine and cosine components.

```python
class FourierDefectDetector:
    def __init__(self, low_freq_cutoff=10, high_freq_cutoff=100):
        self.low_freq_cutoff = low_freq_cutoff
        self.high_freq_cutoff = high_freq_cutoff
        
    def apply_fft(self, img):
        """Apply 2D FFT and shift zero frequency to center"""
        # Apply FFT
        f_transform = np.fft.fft2(img)
        f_shift = np.fft.fftshift(f_transform)
        
        # Compute magnitude spectrum
        magnitude_spectrum = np.log(np.abs(f_shift) + 1)
        
        return f_shift, magnitude_spectrum
    
    def create_frequency_masks(self, shape):
        """Create masks for different frequency bands"""
        rows, cols = shape
        crow, ccol = rows // 2, cols // 2
        
        # Create coordinate grids
        y, x = np.ogrid[:rows, :cols]
        
        # Distance from center
        dist_from_center = np.sqrt((x - ccol)**2 + (y - crow)**2)
        
        # Create masks
        low_freq_mask = dist_from_center <= self.low_freq_cutoff
        high_freq_mask = dist_from_center >= self.high_freq_cutoff
        mid_freq_mask = ~low_freq_mask & ~high_freq_mask
        
        return low_freq_mask, mid_freq_mask, high_freq_mask
    
    def detect_scratches(self, img, angle_threshold=5):
        """Detect linear scratches using FFT"""
        # Apply FFT
        f_shift, magnitude = self.apply_fft(img)
        
        # Get high frequency components (scratches)
        _, _, high_freq_mask = self.create_frequency_masks(img.shape)
        
        # Apply high-pass filter
        high_freq_fshift = f_shift * high_freq_mask
        
        # Find dominant orientations in frequency domain
        rows, cols = img.shape
        crow, ccol = rows // 2, cols // 2
        
        # Convert to polar coordinates
        y, x = np.ogrid[:rows, :cols]
        cy, cx = y - crow, x - ccol
        angles = np.arctan2(cy, cx)
        
        # Analyze angular distribution of high frequencies
        high_freq_angles = angles[high_freq_mask]
        high_freq_magnitudes = np.abs(f_shift[high_freq_mask])
        
        # Histogram of angles weighted by magnitude
        angle_hist, angle_bins = np.histogram(
            high_freq_angles, 
            bins=180, 
            weights=high_freq_magnitudes,
            range=(-np.pi, np.pi)
        )
        
        # Find peaks in angular distribution
        from scipy.signal import find_peaks
        peaks, properties = find_peaks(angle_hist, height=np.mean(angle_hist) * 2)
        
        # Reconstruct image with only scratch frequencies
        scratch_mask = np.zeros_like(high_freq_mask)
        for peak in peaks:
            peak_angle = angle_bins[peak]
            angle_mask = np.abs(angles - peak_angle) < np.radians(angle_threshold)
            scratch_mask |= angle_mask & high_freq_mask
        
        # Apply mask and inverse FFT
        scratch_fshift = f_shift * scratch_mask
        scratch_ishift = np.fft.ifftshift(scratch_fshift)
        scratch_img = np.abs(np.fft.ifft2(scratch_ishift))
        
        return scratch_img, peaks, angle_hist
    
    def analyze_frequency_content(self, img):
        """Comprehensive frequency analysis"""
        f_shift, magnitude = self.apply_fft(img)
        
        # Get frequency masks
        low_mask, mid_mask, high_mask = self.create_frequency_masks(img.shape)
        
        # Calculate energy in each band
        total_energy = np.sum(np.abs(f_shift) ** 2)
        low_energy = np.sum(np.abs(f_shift[low_mask]) ** 2) / total_energy
        mid_energy = np.sum(np.abs(f_shift[mid_mask]) ** 2) / total_energy
        high_energy = np.sum(np.abs(f_shift[high_mask]) ** 2) / total_energy
        
        # Detect anomalies based on energy distribution
        features = {
            'low_freq_energy': low_energy,
            'mid_freq_energy': mid_energy,
            'high_freq_energy': high_energy,
            'high_to_low_ratio': high_energy / (low_energy + 1e-6)
        }
        
        return features, magnitude
```

### **4.5 Morphological Operations**

It is the difference between dilation and erosion of an image. The result will look like the outline of the object.

```python
class MorphologicalDefectDetector:
    def __init__(self):
        # Define various structuring elements
        self.kernels = {
            'small_circle': cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)),
            'medium_circle': cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5)),
            'large_circle': cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7)),
            'small_square': np.ones((3, 3), np.uint8),
            'medium_square': np.ones((5, 5), np.uint8),
            'cross': cv2.getStructuringElement(cv2.MORPH_CROSS, (5, 5))
        }
    
    def white_tophat(self, img, kernel_size='medium_circle'):
        """
        White top-hat: img - opening(img)
        Extracts bright regions smaller than structuring element
        Used for detecting small bright defects (particles, dust)
        """
        kernel = self.kernels[kernel_size]
        opening = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)
        tophat = cv2.subtract(img, opening)
        return tophat
    
    def black_tophat(self, img, kernel_size='medium_circle'):
        """
        Black top-hat: closing(img) - img
        Extracts dark regions smaller than structuring element
        Used for detecting pits, holes, dark contamination
        """
        kernel = self.kernels[kernel_size]
        closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)
        blackhat = cv2.subtract(closing, img)
        return blackhat
    
    def morphological_gradient(self, img, kernel_size='small_circle'):
        """
        Morphological gradient: dilation(img) - erosion(img)
        Enhances edges and boundaries
        """
        kernel = self.kernels[kernel_size]
        gradient = cv2.morphologyEx(img, cv2.MORPH_GRADIENT, kernel)
        return gradient
    
    def multi_scale_morphology(self, img):
        """Apply morphological operations at multiple scales"""
        results = {}
        
        # Multi-scale white top-hat for particles
        white_small = self.white_tophat(img, 'small_circle')
        white_medium = self.white_tophat(img, 'medium_circle')
        white_large = self.white_tophat(img, 'large_circle')
        
        # Combine multi-scale results
        results['particles'] = np.maximum(white_small, 
                                        np.maximum(white_medium, white_large))
        
        # Multi-scale black top-hat for pits
        black_small = self.black_tophat(img, 'small_circle')
        black_medium = self.black_tophat(img, 'medium_circle')
        black_large = self.black_tophat(img, 'large_circle')
        
        results['pits'] = np.maximum(black_small, 
                                   np.maximum(black_medium, black_large))
        
        # Edge detection
        results['edges'] = self.morphological_gradient(img, 'small_circle')
        
        return results
    
    def detect_specific_defects(self, img, defect_type='all'):
        """Detect specific types of defects"""
        # Apply adaptive threshold first
        binary = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                     cv2.THRESH_BINARY, 11, 2)
        
        defects = {}
        
        if defect_type in ['all', 'contamination']:
            # Detect blob-like contamination
            kernel = self.kernels['medium_circle']
            opened = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)
            contamination = cv2.subtract(binary, opened)
            defects['contamination'] = contamination
        
        if defect_type in ['all', 'scratches']:
            # Detect linear scratches using directional kernels
            scratch_kernels = [
                np.array([[0, 0, 0], [1, 1, 1], [0, 0, 0]], dtype=np.uint8),  # Horizontal
                np.array([[0, 1, 0], [0, 1, 0], [0, 1, 0]], dtype=np.uint8),  # Vertical
                np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=np.uint8),  # Diagonal
                np.array([[0, 0, 1], [0, 1, 0], [1, 0, 0]], dtype=np.uint8)   # Anti-diagonal
            ]
            
            scratch_responses = []
            for kernel in scratch_kernels:
                response = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
                scratch_responses.append(response)
            
            # Combine responses
            scratches = np.max(scratch_responses, axis=0)
            defects['scratches'] = scratches
        
        if defect_type in ['all', 'edge_chips']:
            # Detect edge irregularities
            edges = cv2.Canny(img, 50, 150)
            kernel = self.kernels['small_square']
            dilated_edges = cv2.dilate(edges, kernel, iterations=2)
            
            # Find contours
            contours, _ = cv2.findContours(dilated_edges, 
                                         cv2.RETR_EXTERNAL, 
                                         cv2.CHAIN_APPROX_SIMPLE)
            
            # Analyze contour properties
            edge_defects = np.zeros_like(img)
            for contour in contours:
                # Calculate circularity
                area = cv2.contourArea(contour)
                perimeter = cv2.arcLength(contour, True)
                if perimeter > 0:
                    circularity = 4 * np.pi * area / (perimeter ** 2)
                    # Low circularity indicates irregular shape
                    if circularity < 0.7:
                        cv2.drawContours(edge_defects, [contour], -1, 255, -1)
            
            defects['edge_chips'] = edge_defects
        
        return defects
```

### **4.6 Singular Value Decomposition (SVD)**

```python
class SVD_AnomalyDetector:
    def __init__(self, n_components=50):
        self.n_components = n_components
        self.reference_components = None
        
    def train_reference(self, reference_images):
        """Build SVD model from reference images"""
        # Stack and flatten reference images
        n_images = len(reference_images)
        h, w = reference_images[0].shape
        data_matrix = np.zeros((n_images, h * w))
        
        for i, img in enumerate(reference_images):
            data_matrix[i] = img.flatten()
        
        # Compute SVD
        U, S, Vt = np.linalg.svd(data_matrix.T, full_matrices=False)
        
        # Store principal components
        self.reference_components = Vt[:self.n_components]
        self.singular_values = S[:self.n_components]
        self.mean_image = np.mean(data_matrix, axis=0)
        
        return self
    
    def reconstruct(self, img):
        """Reconstruct image using reference components"""
        # Flatten and center
        img_flat = img.flatten() - self.mean_image
        
        # Project onto principal components
        coefficients = np.dot(self.reference_components, img_flat)
        
        # Reconstruct
        reconstruction = self.mean_image + np.dot(coefficients, 
                                                 self.reference_components)
        
        return reconstruction.reshape(img.shape), coefficients
    
    def compute_reconstruction_error(self, img):
        """Compute pixel-wise reconstruction error"""
        reconstruction, coefficients = self.reconstruct(img)
        error = np.abs(img - reconstruction)
        return error, reconstruction, coefficients
    
    def detect_anomalies(self, img, threshold=None):
        """Detect anomalies based on reconstruction error"""
        error, reconstruction, coefficients = self.compute_reconstruction_error(img)
        
        if threshold is None:
            # Adaptive threshold based on error statistics
            threshold = np.mean(error) + 3 * np.std(error)
        
        # Create anomaly mask
        anomaly_mask = error > threshold
        
        # Calculate anomaly score
        anomaly_score = np.sum(error) / (img.shape[0] * img.shape[1])
        
        # Analyze coefficient distribution
        coeff_anomaly = self._analyze_coefficients(coefficients)
        
        return {
            'anomaly_mask': anomaly_mask,
            'error_map': error,
            'reconstruction': reconstruction,
            'anomaly_score': anomaly_score,
            'coefficient_anomaly': coeff_anomaly
        }
    
    def _analyze_coefficients(self, coefficients):
        """Analyze if coefficients follow expected distribution"""
        # Coefficients should decay according to singular values
        expected_energy = self.singular_values ** 2
        expected_energy /= np.sum(expected_energy)
        
        actual_energy = coefficients ** 2
        actual_energy /= np.sum(actual_energy)
        
        # KL divergence between expected and actual
        kl_div = np.sum(actual_energy * np.log(actual_energy / expected_energy + 1e-10))
        
        return kl_div
```

### **4.7 Mahalanobis Distance**

Mahalanobis distance is an effective multivariate distance metric that measures the distance between a point and a distribution.

```python
class MahalanobisAnomalyDetector:
    def __init__(self, contamination=0.1):
        self.contamination = contamination
        self.mean = None
        self.inv_cov = None
        self.threshold = None
        
    def fit(self, X):
        """
        Fit the model to normal data
        X: array of shape (n_samples, n_features)
        """
        # Compute mean and covariance
        self.mean = np.mean(X, axis=0)
        
        # Compute covariance matrix with regularization
        cov = np.cov(X.T)
        # Add small value to diagonal for numerical stability
        cov += np.eye(cov.shape[0]) * 1e-6
        
        # Compute inverse covariance
        self.inv_cov = np.linalg.inv(cov)
        
        # Compute distances for training data
        distances = self._compute_distances(X)
        
        # Set threshold based on contamination level
        self.threshold = np.percentile(distances, (1 - self.contamination) * 100)
        
        return self
    
    def _compute_distances(self, X):
        """Compute Mahalanobis distances for samples"""
        diff = X - self.mean
        distances = np.sqrt(np.sum(diff @ self.inv_cov * diff, axis=1))
        return distances
    
    def predict(self, X):
        """Predict if samples are anomalies"""
        distances = self._compute_distances(X)
        predictions = (distances > self.threshold).astype(int)
        return predictions, distances
    
    def score_samples(self, X):
        """Return anomaly scores (distances)"""
        return self._compute_distances(X)
```

### **4.8 Hough Transform for Scratch Detection**

```python
class HoughScratchDetector:
    def __init__(self, threshold=50, min_line_length=20, max_line_gap=10):
        self.threshold = threshold
        self.min_line_length = min_line_length
        self.max_line_gap = max_line_gap
        
    def detect_scratches(self, img):
        """Detect linear scratches using Hough transform"""
        # Edge detection
        edges = cv2.Canny(img, 50, 150, apertureSize=3)
        
        # Probabilistic Hough transform
        lines = cv2.HoughLinesP(edges, 
                               rho=1, 
                               theta=np.pi/180, 
                               threshold=self.threshold,
                               minLineLength=self.min_line_length,
                               maxLineGap=self.max_line_gap)
        
        # Create scratch mask
        scratch_mask = np.zeros_like(img)
        
        if lines is not None:
            # Analyze line properties
            line_properties = []
            
            for line in lines:
                x1, y1, x2, y2 = line[0]
                
                # Calculate line properties
                length = np.sqrt((x2 - x1)**2 + (y2 - y1)**2)
                angle = np.arctan2(y2 - y1, x2 - x1) * 180 / np.pi
                
                # Calculate line intensity profile
                num_samples = int(length)
                x_samples = np.linspace(x1, x2, num_samples)
                y_samples = np.linspace(y1, y2, num_samples)
                
                # Extract intensity values along line
                intensities = []
                for i in range(num_samples):
                    x, y = int(x_samples[i]), int(y_samples[i])
                    if 0 <= x < img.shape[1] and 0 <= y < img.shape[0]:
                        intensities.append(img[y, x])
                
                # Calculate intensity statistics
                if intensities:
                    mean_intensity = np.mean(intensities)
                    std_intensity = np.std(intensities)
                    
                    line_properties.append({
                        'start': (x1, y1),
                        'end': (x2, y2),
                        'length': length,
                        'angle': angle,
                        'mean_intensity': mean_intensity,
                        'std_intensity': std_intensity
                    })
                    
                    # Draw line on mask
                    cv2.line(scratch_mask, (x1, y1), (x2, y2), 255, 2)
            
            # Group parallel lines (potential single scratch)
            grouped_lines = self._group_parallel_lines(line_properties)
            
            return scratch_mask, line_properties, grouped_lines
        
        return scratch_mask, [], []
    
    def _group_parallel_lines(self, lines, angle_threshold=10, distance_threshold=20):
        """Group lines that likely belong to same scratch"""
        groups = []
        used = [False] * len(lines)
        
        for i, line1 in enumerate(lines):
            if used[i]:
                continue
                
            group = [line1]
            used[i] = True
            
            for j, line2 in enumerate(lines[i+1:], i+1):
                if used[j]:
                    continue
                    
                # Check if lines are parallel
                angle_diff = abs(line1['angle'] - line2['angle'])
                if angle_diff > 180:
                    angle_diff = 360 - angle_diff
                    
                if angle_diff < angle_threshold:
                    # Check distance between lines
                    dist = self._line_distance(line1, line2)
                    if dist < distance_threshold:
                        group.append(line2)
                        used[j] = True
            
            groups.append(group)
        
        return groups
    
    def _line_distance(self, line1, line2):
        """Calculate minimum distance between two line segments"""
        # Simplified: distance between midpoints
        mid1_x = (line1['start'][0] + line1['end'][0]) / 2
        mid1_y = (line1['start'][1] + line1['end'][1]) / 2
        mid2_x = (line2['start'][0] + line2['end'][0]) / 2
        mid2_y = (line2['start'][1] + line2['end'][1]) / 2
        
        return np.sqrt((mid2_x - mid1_x)**2 + (mid2_y - mid1_y)**2)
```

---

## 5. Real-time Processing Pipeline {#real-time-processing}

### **5.1 Pipeline Architecture**

```python
import multiprocessing as mp
from queue import Queue
import threading
import time

class RealTimeDefectDetector:
    def __init__(self, model_path, num_workers=4):
        self.model = self._load_optimized_model(model_path)
        self.num_workers = num_workers
        
        # Queues for pipeline stages
        self.capture_queue = mp.Queue(maxsize=10)
        self.preprocess_queue = mp.Queue(maxsize=10)
        self.detection_queue = mp.Queue(maxsize=10)
        self.output_queue = mp.Queue(maxsize=10)
        
        # Shared memory for zero-copy transfer
        self.shared_arrays = []
        for _ in range(20):  # Pool of shared arrays
            shared_array = mp.Array('b', 1152 * 864)  # Byte array
            self.shared_arrays.append(shared_array)
        
        # Performance metrics
        self.fps_counter = mp.Value('d', 0.0)
        self.latency_ms = mp.Value('d', 0.0)
        
    def _load_optimized_model(self, model_path):
        """Load TensorRT optimized model"""
        # In practice, this would load TensorRT model
        # For demonstration, using regular PyTorch
        model = torch.jit.load(model_path)
        model.eval()
        return model
    
    def capture_process(self, camera_id=0):
        """High-speed image capture process"""
        cap = cv2.VideoCapture(camera_id)
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1152)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 864)
        cap.set(cv2.CAP_PROP_FPS, 100)
        cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
        
        frame_count = 0
        start_time = time.time()
        
        while True:
            ret, frame = cap.read()
            if not ret:
                continue
            
            # Convert to grayscale
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            
            # Get shared array from pool
            shared_idx = frame_count % len(self.shared_arrays)
            shared_array = self.shared_arrays[shared_idx]
            
            # Copy to shared memory
            np_array = np.frombuffer(shared_array.get_obj(), dtype=np.uint8)
            np_array[:] = gray.flatten()
            
            # Send index to next stage
            timestamp = time.time()
            self.capture_queue.put((shared_idx, timestamp))
            
            # Update FPS
            frame_count += 1
            if frame_count % 100 == 0:
                elapsed = time.time() - start_time
                self.fps_counter.value = frame_count / elapsed
    
    def preprocessing_worker(self):
        """Preprocessing worker process"""
        while True:
            shared_idx, timestamp = self.capture_queue.get()
            
            # Retrieve image from shared memory
            shared_array = self.shared_arrays[shared_idx]
            img = np.frombuffer(shared_array.get_obj(), 
                               dtype=np.uint8).reshape(864, 1152)
            
            # Fast preprocessing
            processed = self._fast_preprocess(img)
            
            # Pass to detection
            self.preprocess_queue.put((processed, timestamp))
    
    def _fast_preprocess(self, img):
        """Optimized preprocessing"""
        # Use lookup table for normalization
        if not hasattr(self, 'norm_lut'):
            self.norm_lut = np.arange(256, dtype=np.float32) / 255.0
        
        normalized = self.norm_lut[img]
        
        # Fast Gaussian blur using separable filter
        kernel_1d = cv2.getGaussianKernel(5, 1.0)
        blurred = cv2.sepFilter2D(normalized, -1, kernel_1d, kernel_1d)
        
        return blurred
    
    def detection_worker(self, device_id):
        """GPU detection worker"""
        # Set GPU device
        torch.cuda.set_device(device_id)
        device = torch.device(f'cuda:{device_id}')
        
        # Create local model copy
        local_model = self.model.to(device)
        
        # Batch processing
        batch_size = 4
        batch_buffer = []
        timestamp_buffer = []
        
        while True:
            # Collect batch
            while len(batch_buffer) < batch_size:
                if not self.preprocess_queue.empty():
                    img, timestamp = self.preprocess_queue.get()
                    batch_buffer.append(img)
                    timestamp_buffer.append(timestamp)
                else:
                    break
            
            if batch_buffer:
                # Process batch
                batch_tensor = torch.tensor(np.stack(batch_buffer), 
                                          dtype=torch.float32).to(device)
                batch_tensor = batch_tensor.unsqueeze(1)  # Add channel dimension
                
                with torch.no_grad():
                    # Use TensorRT optimized inference
                    predictions = local_model(batch_tensor)
                
                # Send results
                for i, (pred, ts) in enumerate(zip(predictions, timestamp_buffer)):
                    self.detection_queue.put((pred.cpu().numpy(), ts))
                
                # Update latency
                current_time = time.time()
                avg_latency = (current_time - timestamp_buffer[0]) * 1000
                self.latency_ms.value = avg_latency
                
                # Clear buffers
                batch_buffer.clear()
                timestamp_buffer.clear()
    
    def output_worker(self):
        """Result processing and visualization"""
        while True:
            prediction, timestamp = self.detection_queue.get()
            
            # Process results
            defect_map = self._process_prediction(prediction)
            
            # Generate output
            result = {
                'timestamp': timestamp,
                'defect_map': defect_map,
                'defect_count': np.sum(defect_map > 0),
                'pass_fail': 'PASS' if np.sum(defect_map > 0) < 5 else 'FAIL',
                'latency_ms': self.latency_ms.value,
                'fps': self.fps_counter.value
            }
            
            self.output_queue.put(result)
    
    def start(self):
        """Start all pipeline processes"""
        # Start capture process
        capture_proc = mp.Process(target=self.capture_process)
        capture_proc.start()
        
        # Start preprocessing workers
        preprocess_procs = []
        for _ in range(2):
            proc = mp.Process(target=self.preprocessing_worker)
            proc.start()
            preprocess_procs.append(proc)
        
        # Start detection workers (one per GPU)
        detection_procs = []
        num_gpus = torch.cuda.device_count()
        for gpu_id in range(num_gpus):
            proc = mp.Process(target=self.detection_worker, args=(gpu_id,))
            proc.start()
            detection_procs.append(proc)
        
        # Start output worker
        output_proc = mp.Process(target=self.output_worker)
        output_proc.start()
        
        # Monitor performance
        self._monitor_performance()
    
    def _monitor_performance(self):
        """Monitor and log pipeline performance"""
        while True:
            time.sleep(1)
            print(f"FPS: {self.fps_counter.value:.2f}, "
                  f"Latency: {self.latency_ms.value:.2f} ms")
```

### **5.2 GPU Memory Optimization**

```python
class GPUMemoryOptimizer:
    def __init__(self, model, target_memory_mb=2048):
        self.model = model
        self.target_memory = target_memory_mb * 1024 * 1024  # Convert to bytes
        
    def optimize_batch_size(self, input_shape=(1, 1152, 864)):
        """Find optimal batch size for GPU memory"""
        test_input = torch.randn(1, *input_shape).cuda()
        
        # Binary search for optimal batch size
        min_batch = 1
        max_batch = 64
        optimal_batch = 1
        
        while min_batch <= max_batch:
            mid_batch = (min_batch + max_batch) // 2
            
            try:
                # Clear cache
                torch.cuda.empty_cache()
                
                # Test batch size
                test_batch = torch.randn(mid_batch, *input_shape).cuda()
                _ = self.model(test_batch)
                
                # Check memory usage
                allocated = torch.cuda.memory_allocated()
                
                if allocated < self.target_memory:
                    optimal_batch = mid_batch
                    min_batch = mid_batch + 1
                else:
                    max_batch = mid_batch - 1
                    
            except RuntimeError as e:
                if "out of memory" in str(e):
                    max_batch = mid_batch - 1
                else:
                    raise e
            finally:
                torch.cuda.empty_cache()
        
        return optimal_batch
    
    def enable_mixed_precision(self):
        """Enable automatic mixed precision"""
        from torch.cuda.amp import autocast
        
        # Wrap model forward
        original_forward = self.model.forward
        
        def amp_forward(self, x):
            with autocast():
                return original_forward(x)
        
        self.model.forward = amp_forward.__get__(self.model, self.model.__class__)
        
        return self.model
```

### **5.3 Real-time Visualization**

```python
class RealTimeVisualizer:
    def __init__(self, window_name="Fiber Defect Detection"):
        self.window_name = window_name
        cv2.namedWindow(self.window_name, cv2.WINDOW_NORMAL)
        
        # Color map for defect types
        self.defect_colors = {
            'scratch': (0, 0, 255),      # Red
            'contamination': (0, 255, 0), # Green
            'pit': (255, 0, 0),          # Blue
            'chip': (0, 255, 255),       # Yellow
            'unknown': (255, 255, 255)   # White
        }
        
    def visualize_results(self, img, defect_map, detection_results):
        """Create visualization overlay"""
        # Create colored overlay
        overlay = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)
        
        # Apply defect coloring
        for defect_type, mask in defect_map.items():
            color = self.defect_colors.get(defect_type, self.defect_colors['unknown'])
            overlay[mask > 0] = color
        
        # Blend with original
        result = cv2.addWeighted(cv2.cvtColor(img, cv2.COLOR_GRAY2BGR), 0.7, 
                                overlay, 0.3, 0)
        
        # Add text overlays
        self._add_text_overlay(result, detection_results)
        
        # Add performance metrics
        self._add_performance_overlay(result, detection_results)
        
        return result
    
    def _add_text_overlay(self, img, results):
        """Add detection results text"""
        font = cv2.FONT_HERSHEY_SIMPLEX
        
        # Pass/Fail status
        status = results['pass_fail']
        color = (0, 255, 0) if status == 'PASS' else (0, 0, 255)
        cv2.putText(img, f"Status: {status}", (10, 30), 
                   font, 1, color, 2)
        
        # Defect count
        cv2.putText(img, f"Defects: {results['defect_count']}", 
                   (10, 60), font, 0.8, (255, 255, 255), 2)
        
    def _add_performance_overlay(self, img, results):
        """Add performance metrics"""
        font = cv2.FONT_HERSHEY_SIMPLEX
        h, w = img.shape[:2]
        
        # FPS
        cv2.putText(img, f"FPS: {results['fps']:.1f}", 
                   (w - 150, 30), font, 0.8, (255, 255, 255), 2)
        
        # Latency
        cv2.putText(img, f"Latency: {results['latency_ms']:.1f}ms", 
                   (w - 150, 60), font, 0.8, (255, 255, 255), 2)
```

---

## 6. HPC/GPU Implementation {#hpc-gpu}

### **6.1 Distributed Training Setup**

torch.distributed and Horovod support data parallelism and basic model parallelism.

```python
#!/usr/bin/env python
# distributed_training.py

import os
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
import horovod.torch as hvd

class HybridDistributedTrainer:
    def __init__(self, use_horovod=False):
        self.use_horovod = use_horovod
        
        if use_horovod:
            # Initialize Horovod
            hvd.init()
            self.rank = hvd.rank()
            self.world_size = hvd.size()
            self.local_rank = hvd.local_rank()
        else:
            # Initialize PyTorch distributed
            self.rank = int(os.environ['RANK'])
            self.world_size = int(os.environ['WORLD_SIZE'])
            self.local_rank = int(os.environ['LOCAL_RANK'])
            
            # Initialize process group
            dist.init_process_group(
                backend='nccl',
                init_method='env://'
            )
        
        # Set device
        torch.cuda.set_device(self.local_rank)
        self.device = torch.device(f'cuda:{self.local_rank}')
        
    def setup_model(self, model):
        """Setup model for distributed training"""
        model = model.to(self.device)
        
        if self.use_horovod:
            # Horovod: broadcast parameters
            hvd.broadcast_parameters(model.state_dict(), root_rank=0)
            
            # Use DistributedOptimizer
            return model
        else:
            # PyTorch DDP
            model = DDP(
                model,
                device_ids=[self.local_rank],
                output_device=self.local_rank,
                find_unused_parameters=True
            )
            return model
    
    def setup_optimizer(self, optimizer):
        """Setup optimizer for distributed training"""
        if self.use_horovod:
            # Horovod: wrap optimizer
            optimizer = hvd.DistributedOptimizer(
                optimizer,
                named_parameters=model.named_parameters(),
                compression=hvd.Compression.fp16
            )
            
            # Broadcast optimizer state
            hvd.broadcast_optimizer_state(optimizer, root_rank=0)
        
        return optimizer
    
    def setup_data_loader(self, dataset, batch_size):
        """Setup distributed data loader"""
        if self.use_horovod:
            # Horovod sampler
            sampler = torch.utils.data.distributed.DistributedSampler(
                dataset, num_replicas=hvd.size(), rank=hvd.rank()
            )
        else:
            # PyTorch distributed sampler
            sampler = torch.utils.data.distributed.DistributedSampler(
                dataset,
                num_replicas=self.world_size,
                rank=self.rank,
                shuffle=True
            )
        
        # Create data loader
        loader = torch.utils.data.DataLoader(
            dataset,
            batch_size=batch_size,
            sampler=sampler,
            num_workers=4,
            pin_memory=True,
            drop_last=True
        )
        
        return loader, sampler
```

### **6.2 SLURM Job Scripts**

```bash
#!/bin/bash
# train_distributed.sbatch

#SBATCH --job-name=fiber_defect_train
#SBATCH --nodes=4
#SBATCH --ntasks-per-node=8
#SBATCH --gpus-per-node=8
#SBATCH --cpus-per-task=10
#SBATCH --mem=0
#SBATCH --time=48:00:00
#SBATCH --partition=gpu
#SBATCH --exclusive

# Load modules
module load cuda/11.7
module load cudnn/8.4.1
module load nccl/2.12.12
module load python/3.9
module load openmpi/4.1.4

# Setup environment
export MASTER_PORT=12340
export WORLD_SIZE=$(($SLURM_NNODES * $SLURM_NTASKS_PER_NODE))
echo "WORLD_SIZE: $WORLD_SIZE"

# Get master node
export MASTER_ADDR=$(scontrol show hostnames "$SLURM_JOB_NODELIST" | head -n 1)
echo "MASTER_ADDR: $MASTER_ADDR"

# Training script with performance optimizations
srun --cpu-bind=none python train_distributed.py \
    --model FiberDefectNet \
    --dataset /scratch/fiber_defect_data \
    --batch-size 32 \
    --epochs 100 \
    --learning-rate 0.001 \
    --mixed-precision \
    --gradient-accumulation 4 \
    --checkpoint-dir /scratch/checkpoints \
    --tensorboard-dir /scratch/tensorboard \
    --num-workers 10 \
    --prefetch-factor 2 \
    --persistent-workers \
    --profile
```

### **6.3 Multi-GPU Inference Pipeline**

```python
class MultiGPUInferencePipeline:
    def __init__(self, model_path, num_gpus=None):
        if num_gpus is None:
            num_gpus = torch.cuda.device_count()
        
        self.num_gpus = num_gpus
        self.models = []
        
        # Load model on each GPU
        for gpu_id in range(num_gpus):
            device = torch.device(f'cuda:{gpu_id}')
            model = torch.jit.load(model_path, map_location=device)
            model.eval()
            self.models.append(model)
        
        # Create thread pool for parallel inference
        from concurrent.futures import ThreadPoolExecutor
        self.executor = ThreadPoolExecutor(max_workers=num_gpus)
        
        # Input/output queues for each GPU
        self.input_queues = [Queue(maxsize=10) for _ in range(num_gpus)]
        self.output_queue = Queue()
        
    def inference_worker(self, gpu_id):
        """Worker thread for GPU inference"""
        model = self.models[gpu_id]
        device = torch.device(f'cuda:{gpu_id}')
        
        while True:
            # Get batch from queue
            batch_data = self.input_queues[gpu_id].get()
            if batch_data is None:
                break
            
            images, indices = batch_data
            
            # Move to GPU
            images = images.to(device)
            
            # Inference
            with torch.no_grad():
                with torch.cuda.amp.autocast():
                    predictions = model(images)
            
            # Move results back to CPU
            predictions = predictions.cpu().numpy()
            
            # Send results
            for idx, pred in zip(indices, predictions):
                self.output_queue.put((idx, pred))
    
    def start_workers(self):
        """Start inference workers"""
        futures = []
        for gpu_id in range(self.num_gpus):
            future = self.executor.submit(self.inference_worker, gpu_id)
            futures.append(future)
        return futures
    
    def process_batch(self, images, indices):
        """Distribute batch across GPUs"""
        batch_size = len(images)
        chunk_size = (batch_size + self.num_gpus - 1) // self.num_gpus
        
        for gpu_id in range(self.num_gpus):
            start_idx = gpu_id * chunk_size
            end_idx = min(start_idx + chunk_size, batch_size)
            
            if start_idx < batch_size:
                gpu_images = images[start_idx:end_idx]
                gpu_indices = indices[start_idx:end_idx]
                
                # Send to GPU queue
                self.input_queues[gpu_id].put((gpu_images, gpu_indices))
```

### **6.4 Performance Profiling**

```python
import torch.profiler as profiler
import nvidia_ml_py as nvml

class PerformanceProfiler:
    def __init__(self):
        # Initialize NVML for GPU monitoring
        nvml.nvmlInit()
        self.device_count = nvml.nvmlDeviceGetCount()
        self.handles = [nvml.nvmlDeviceGetHandleByIndex(i) 
                       for i in range(self.device_count)]
        
    def profile_training_step(self, model, data_loader, optimizer, num_steps=100):
        """Profile training performance"""
        
        with profiler.profile(
            activities=[
                profiler.ProfilerActivity.CPU,
                profiler.ProfilerActivity.CUDA,
            ],
            schedule=profiler.schedule(
                wait=10,
                warmup=10,
                active=80,
                repeat=1
            ),
            on_trace_ready=profiler.tensorboard_trace_handler('./profiler_logs'),
            record_shapes=True,
            profile_memory=True,
            with_stack=True
        ) as prof:
            
            for step, (images, labels) in enumerate(data_loader):
                if step >= num_steps:
                    break
                
                # Training step
                images = images.cuda()
                labels = labels.cuda()
                
                optimizer.zero_grad()
                
                # Record GPU stats before forward
                gpu_stats_before = self._get_gpu_stats()
                
                # Forward pass
                with profiler.record_function("forward"):
                    outputs = model(images)
                    loss = F.cross_entropy(outputs, labels)
                
                # Backward pass
                with profiler.record_function("backward"):
                    loss.backward()
                
                # Optimizer step
                with profiler.record_function("optimizer"):
                    optimizer.step()
                
                # Record GPU stats after
                gpu_stats_after = self._get_gpu_stats()
                
                # Log performance metrics
                if step % 10 == 0:
                    self._log_performance(step, loss.item(), 
                                        gpu_stats_before, gpu_stats_after)
                
                prof.step()
        
        # Generate report
        print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=20))
        
    def _get_gpu_stats(self):
        """Get current GPU statistics"""
        stats = []
        for handle in self.handles:
            mem_info = nvml.nvmlDeviceGetMemoryInfo(handle)
            util = nvml.nvmlDeviceGetUtilizationRates(handle)
            temp = nvml.nvmlDeviceGetTemperature(handle, nvml.NVML_TEMPERATURE_GPU)
            
            stats.append({
                'memory_used': mem_info.used / 1024**3,  # GB
                'memory_total': mem_info.total / 1024**3,  # GB
                'gpu_util': util.gpu,
                'memory_util': util.memory,
                'temperature': temp
            })
        return stats
    
    def benchmark_inference(self, model, input_shape, batch_sizes=[1, 2, 4, 8, 16, 32]):
        """Benchmark inference at different batch sizes"""
        results = []
        
        for batch_size in batch_sizes:
            # Create dummy input
            dummy_input = torch.randn(batch_size, *input_shape).cuda()
            
            # Warmup
            for _ in range(10):
                _ = model(dummy_input)
            
            torch.cuda.synchronize()
            
            # Benchmark
            num_runs = 100
            start_event = torch.cuda.Event(enable_timing=True)
            end_event = torch.cuda.Event(enable_timing=True)
            
            start_event.record()
            for _ in range(num_runs):
                _ = model(dummy_input)
            end_event.record()
            
            torch.cuda.synchronize()
            
            # Calculate metrics
            elapsed_time = start_event.elapsed_time(end_event) / 1000  # seconds
            throughput = (batch_size * num_runs) / elapsed_time
            latency = (elapsed_time / num_runs) * 1000  # ms
            
            results.append({
                'batch_size': batch_size,
                'throughput': throughput,
                'latency': latency,
                'gpu_memory': torch.cuda.max_memory_allocated() / 1024**3  # GB
            })
            
            torch.cuda.empty_cache()
        
        return results
```

---

## 7. Development Resources {#resources}

### **7.1 Essential Libraries and Tools**

#### **Core Dependencies**
```python
# requirements.txt
torch>=1.12.0
torchvision>=0.13.0
opencv-python>=4.6.0
numpy>=1.23.0
scipy>=1.9.0
scikit-image>=0.19.0
matplotlib>=3.5.0
Pillow>=9.2.0

# GPU acceleration
tensorrt>=8.4.0
pycuda>=2022.1
cupy>=11.0.0

# Distributed training
horovod>=0.25.0
mpi4py>=3.1.0

# Monitoring and profiling
tensorboard>=2.10.0
wandb>=0.13.0
nvidia-ml-py>=11.495.46

# Data handling
h5py>=3.7.0
pandas>=1.4.0
pyarrow>=9.0.0

# Testing
pytest>=7.1.0
pytest-cov>=3.0.0
pytest-benchmark>=3.4.1
```

#### **Docker Environment**
```dockerfile
# Dockerfile
FROM nvcr.io/nvidia/pytorch:22.08-py3

# Install system dependencies
RUN apt-get update && apt-get install -y \
    libopencv-dev \
    libboost-all-dev \
    libhdf5-dev \
    && rm -rf /var/lib/apt/lists/*

# Install Python packages
COPY requirements.txt /tmp/
RUN pip install --upgrade pip && \
    pip install -r /tmp/requirements.txt

# Install TensorRT
RUN pip install tensorrt

# Setup working directory
WORKDIR /workspace

# Copy project files
COPY . /workspace/

# Set environment variables
ENV PYTHONPATH=/workspace:$PYTHONPATH
ENV CUDA_VISIBLE_DEVICES=0,1,2,3

# Entry point
CMD ["python", "main.py"]
```

### **7.2 Configuration Management**

```yaml
# config.yaml
system:
  num_gpus: 4
  batch_size: 32
  num_workers: 8
  
model:
  architecture: "FiberDefectNet"
  input_channels: 1
  num_classes: 10
  pretrained: false
  
  # Optimization settings
  mixed_precision: true
  gradient_checkpointing: false
  
training:
  epochs: 100
  learning_rate: 0.001
  weight_decay: 0.0001
  
  # Learning rate schedule
  scheduler:
    type: "cosine"
    warmup_epochs: 5
    min_lr: 0.00001
  
  # Data augmentation
  augmentation:
    rotation: 15
    scale: [0.8, 1.2]
    brightness: 0.2
    contrast: 0.2
    gaussian_noise: 0.01
  
detection:
  algorithms:
    - ssim:
        window_size: 11
        threshold: 0.95
    - lbp:
        radius: 1
        n_points: 8
    - glcm:
        distances: [1, 2, 3]
        angles: [0, 45, 90, 135]
    - fourier:
        high_freq_cutoff: 100
    - morphology:
        kernel_sizes: ["small", "medium", "large"]
        
  # Consensus settings
  consensus:
    min_detections: 3
    confidence_threshold: 0.7
    
deployment:
  target_fps: 100
  max_latency_ms: 10
  tensorrt:
    precision: "fp16"
    workspace_size: 4096  # MB
    
monitoring:
  tensorboard: true
  wandb:
    project: "fiber-defect-detection"
    entity: "your-team"
  log_interval: 100
  checkpoint_interval: 1000
```

### **7.3 Testing Framework**

```python
# tests/test_detection_algorithms.py

import pytest
import numpy as np
import cv2
from detection_algorithms import (
    SSIM_Detector, LBP_Detector, GLCM_Detector,
    FourierDefectDetector, MorphologicalDefectDetector
)

class TestDetectionAlgorithms:
    @pytest.fixture
    def sample_images(self):
        """Generate sample test images"""
        # Clean fiber image
        clean = np.zeros((200, 200), dtype=np.uint8)
        cv2.circle(clean, (100, 100), 80, 255, -1)
        
        # Image with scratch
        scratched = clean.copy()
        cv2.line(scratched, (50, 50), (150, 150), 0, 2)
        
        # Image with contamination
        contaminated = clean.copy()
        cv2.circle(contaminated, (120, 120), 15, 0, -1)
        
        return {
            'clean': clean,
            'scratched': scratched,
            'contaminated': contaminated
        }
    
    def test_ssim_detector(self, sample_images):
        """Test SSIM detector"""
        detector = SSIM_Detector()
        
        # Test on identical images
        defect_mask, ssim_map, mean_ssim = detector.detect_defects(
            sample_images['clean'], 
            sample_images['clean']
        )
        assert mean_ssim > 0.99
        assert np.sum(defect_mask) == 0
        
        # Test on defective image
        defect_mask, ssim_map, mean_ssim = detector.detect_defects(
            sample_images['scratched'], 
            sample_images['clean']
        )
        assert mean_ssim < 0.95
        assert np.sum(defect_mask) > 0
    
    def test_lbp_detector(self, sample_images):
        """Test LBP detector"""
        detector = LBP_Detector()
        
        # Extract features
        hist, lbp_img = detector.extract_lbp_features(sample_images['clean'])
        
        assert len(hist) == 256
        assert np.sum(hist) == pytest.approx(1.0)
        assert lbp_img.shape == sample_images['clean'].shape
    
    @pytest.mark.parametrize("batch_size", [1, 4, 8, 16])
    def test_inference_speed(self, batch_size):
        """Test inference speed at different batch sizes"""
        import time
        
        # Create dummy model
        model = FiberDefectNet()
        model.eval()
        
        # Create dummy input
        input_tensor = torch.randn(batch_size, 1, 1152, 864)
        
        # Warmup
        for _ in range(10):
            _ = model(input_tensor)
        
        # Benchmark
        start = time.time()
        num_iterations = 100
        
        for _ in range(num_iterations):
            _ = model(input_tensor)
        
        elapsed = time.time() - start
        fps = (batch_size * num_iterations) / elapsed
        
        print(f"Batch size: {batch_size}, FPS: {fps:.2f}")
        
        # Assert minimum performance
        assert fps > 10  # At least 10 FPS
```

### **7.4 Deployment Scripts**

```python
# deploy/export_to_tensorrt.py

import torch
import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit

class TensorRTExporter:
    def __init__(self, pytorch_model, input_shape, precision='fp16'):
        self.model = pytorch_model
        self.input_shape = input_shape
        self.precision = precision
        
        # TensorRT logger
        self.logger = trt.Logger(trt.Logger.WARNING)
        
    def export_onnx(self, onnx_path):
        """Export PyTorch model to ONNX"""
        dummy_input = torch.randn(1, *self.input_shape)
        
        torch.onnx.export(
            self.model,
            dummy_input,
            onnx_path,
            export_params=True,
            opset_version=13,
            do_constant_folding=True,
            input_names=['input'],
            output_names=['output'],
            dynamic_axes={
                'input': {0: 'batch_size'},
                'output': {0: 'batch_size'}
            }
        )
        
        return onnx_path
    
    def build_engine(self, onnx_path, engine_path):
        """Build TensorRT engine from ONNX"""
        builder = trt.Builder(self.logger)
        network = builder.create_network(
            1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)
        )
        parser = trt.OnnxParser(network, self.logger)
        
        # Parse ONNX
        with open(onnx_path, 'rb') as model:
            if not parser.parse(model.read()):
                for error in range(parser.num_errors):
                    print(parser.get_error(error))
                return None
        
        # Configure builder
        config = builder.create_builder_config()
        config.max_workspace_size = 4 * 1024 * 1024 * 1024  # 4GB
        
        # Set precision
        if self.precision == 'fp16':
            config.set_flag(trt.BuilderFlag.FP16)
        elif self.precision == 'int8':
            config.set_flag(trt.BuilderFlag.INT8)
            # Would need calibration dataset for INT8
        
        # Build engine
        profile = builder.create_optimization_profile()
        profile.set_shape(
            'input',
            (1, *self.input_shape),      # min
            (16, *self.input_shape),     # opt
            (32, *self.input_shape)      # max
        )
        config.add_optimization_profile(profile)
        
        engine = builder.build_engine(network, config)
        
        # Save engine
        with open(engine_path, 'wb') as f:
            f.write(engine.serialize())
        
        return engine
    
    def benchmark_engine(self, engine_path, num_runs=1000):
        """Benchmark TensorRT engine"""
        # Load engine
        with open(engine_path, 'rb') as f:
            engine = trt.Runtime(self.logger).deserialize_cuda_engine(f.read())
        
        context = engine.create_execution_context()
        
        # Allocate buffers
        inputs, outputs, bindings = [], [], []
        for binding in engine:
            size = trt.volume(engine.get_binding_shape(binding))
            dtype = trt.nptype(engine.get_binding_dtype(binding))
            
            # Allocate host and device buffers
            host_mem = cuda.pagelocked_empty(size, dtype)
            device_mem = cuda.mem_alloc(host_mem.nbytes)
            
            bindings.append(int(device_mem))
            
            if engine.binding_is_input(binding):
                inputs.append({'host': host_mem, 'device': device_mem})
            else:
                outputs.append({'host': host_mem, 'device': device_mem})
        
        # Create stream
        stream = cuda.Stream()
        
        # Warmup
        for _ in range(10):
            self._run_inference(context, inputs, outputs, bindings, stream)
        
        # Benchmark
        import time
        start = time.time()
        
        for _ in range(num_runs):
            self._run_inference(context, inputs, outputs, bindings, stream)
        
        cuda.Context.synchronize()
        elapsed = time.time() - start
        
        print(f"TensorRT Inference:")
        print(f"  Total time: {elapsed:.3f}s")
        print(f"  Avg latency: {elapsed/num_runs*1000:.3f}ms")
        print(f"  Throughput: {num_runs/elapsed:.1f} FPS")
        
    def _run_inference(self, context, inputs, outputs, bindings, stream):
        """Run single inference"""
        # Transfer input data to device
        for inp in inputs:
            cuda.memcpy_htod_async(inp['device'], inp['host'], stream)
        
        # Run inference
        context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)
        
        # Transfer predictions back
        for out in outputs:
            cuda.memcpy_dtoh_async(out['host'], out['device'], stream)
        
        stream.synchronize()
```

---

## 8. Implementation Roadmap {#roadmap}

### **Phase 1: Foundation (Weeks 1-4)**

1. **Environment Setup**
   - Configure development environment
   - Set up GPU workstations
   - Install all dependencies
   - Create Docker containers

2. **Data Collection**
   - Collect clean fiber images
   - Build defect library
   - Implement synthetic data generation
   - Validate data quality

3. **Basic Detection Pipeline**
   - Implement individual detection algorithms
   - Create modular architecture
   - Build testing framework
   - Benchmark baseline performance

### **Phase 2: Neural Network Development (Weeks 5-8)**

1. **Model Architecture**
   - Implement FiberDefectNet
   - Add attention mechanisms
   - Create custom loss functions
   - Implement data loaders

2. **Training Infrastructure**
   - Set up distributed training
   - Configure HPC environment
   - Implement checkpointing
   - Add monitoring tools

3. **Model Optimization**
   - Quantization experiments
   - Pruning strategies
   - Knowledge distillation
   - TensorRT conversion

### **Phase 3: Real-time Pipeline (Weeks 9-12)**

1. **Pipeline Architecture**
   - Implement multi-process pipeline
   - Add shared memory optimization
   - Create GPU inference workers
   - Build output visualization

2. **Performance Optimization**
   - Profile bottlenecks
   - Optimize memory usage
   - Implement caching strategies
   - Fine-tune batch sizes

3. **Integration Testing**
   - End-to-end testing
   - Stress testing
   - Performance validation
   - Error handling

### **Phase 4: Deployment (Weeks 13-16)**

1. **Production Preparation**
   - Create deployment packages
   - Write documentation
   - Build monitoring dashboards
   - Implement logging

2. **Field Testing**
   - Deploy to test environment
   - Collect real-world data
   - Fine-tune parameters
   - Gather user feedback

3. **Final Optimization**
   - Address performance issues
   - Implement requested features
   - Create maintenance procedures
   - Train operators

### **Success Metrics**

- **Performance**: <2 second processing time per image
- **Accuracy**: >97% defect detection rate
- **Reliability**: <0.1% false positive rate
- **Throughput**: >50 images per minute
- **Uptime**: >99.9% system availability

### **Risk Mitigation**

1. **Technical Risks**
   - GPU memory limitations → Implement dynamic batching
   - Processing speed bottlenecks → Use model compression
   - Detection accuracy issues → Ensemble methods

2. **Operational Risks**
   - Hardware failures → Redundant systems
   - Data quality issues → Robust preprocessing
   - User adoption → Comprehensive training

3. **Schedule Risks**
   - Development delays → Agile methodology
   - Integration challenges → Modular design
   - Testing bottlenecks → Automated testing

## Conclusion

This comprehensive guide provides the technical foundation for building a state-of-the-art fiber optic defect detection system. By combining traditional computer vision techniques with modern deep learning approaches and optimizing for real-time performance, the system can achieve the ambitious goal of reducing inspection time from 20 minutes to under 2 seconds while maintaining industry-leading accuracy.

The key to success lies in:
1. Building a comprehensive data library
2. Implementing robust detection algorithms
3. Optimizing neural network architectures
4. Creating an efficient real-time pipeline
5. Leveraging HPC/GPU resources effectively

With careful implementation following this guide, the project can revolutionize fiber optic quality control and set new standards for automated optical inspection systems.

You're absolutely right. Here are the detailed bullet points analyzing every aspect of the conversation:

## **Project Overview**
- Building an automated fiber optic defect detection system
- Current processing time: 20 minutes per image
- Target processing time: 1-2 seconds per image (600-1200x speedup required)
- Moving from manual debugging to automated detection with real-time capabilities

## **Data Library Development**

### **Current Data Collection Methods**
- Rerunning original process (possibly on HPC)
- Collecting preprocessed image variations with different orientations
- Separating core, cladding, and ferrule regions
- Creating anomaly masks for different defect types
- Recording weights from segmentation algorithms
- Using scratch library overlay technique
- Incorporating research papers on fiber details
- Manual feedback collection system

### **Defect Types Being Catalogued**
- Scratches (linear defects)
- Blobs (contamination areas)
- Contamination (general)
- Oil spots
- Anomalies (undefined defects)
- Digs (surface defects)

### **Fiber Type Organization**
- SMA connectors
- FC connectors
- Different diameter specifications (91μm, 51μm, etc.)
- Storing best processing methods for each specific fiber type

### **Template Matching Studio**
- Automatically finds processing sequences between input and output images
- Acts like navigating through a maze to find correct processing chain
- Example: Input full fiber image → Output just the core
- System learns optimal processing paths

## **Neural Network Implementation Details**

### **PyTorch Architecture**
- Converting image matrices to PyTorch tensors
- Using float conversion for RAM efficiency
- Creating random tensors with same shape for weight initialization
- Matrix multiplication using @ operator or torch.matmul()

### **Image Classification Network Structure**
- Input: 1152×864 pixel images (995,328 total pixels)
- Flatten operation to convert 2D to 1D
- First linear layer: 995,328 → 512 features
- ReLU activation (outputs 0 for negative, preserves positive)
- Second linear layer: 512 → 10 classes
- Represents 10 different defect/quality categories

### **Class Definition Issues Discussed**
- Confusion about inheriting ImageClassifier from itself
- Standard PyTorch practice requires calling parent constructor
- Forward method defines data flow through network

## **Ruleset and Manual Operations**

### **Circle Adjustment Process**
- Manual alignment of two circles on screen using dials
- Inner circle defines core region
- Area between circles defines cladding
- Addresses frustration with automatic circle detection

### **Fiber Dimension Input**
- User specifies fiber type
- System determines initial circle sizes
- Guides correlation with data library entries

### **Defect Filtering Parameters**
- Size filtering
- Type classification
- Angle measurement (for scratches)
- Depth analysis (crater vs. protrusion)

## **Detection Algorithms in Detail**

### **Structural Similarity Index Map (SSIM)**
- Referred to as "template matching" in conversation
- Adds all pixel brightness values and divides by total
- Measures brightness variation
- Checks for lopsided brightness distribution
- Identifies extreme values
- Sorts brightness values to find specific statistical points

### **Local Binary Patterns (LBP)**
- Examines surrounding pixels in circular pattern
- Determines if neighbors are brighter or darker
- Creates texture-dependent pixel intensity patterns
- Different textures produce different patterns

### **Gray Level Co-occurrence Matrix (GLCM)**
- Similar to LBP but focuses on texture relationships
- Calculates contrast (neighbor pixel differences)
- Measures energy (texture uniformity)
- Computes homogeneity (neighbor similarity)

### **Fourier Transform Analysis**
- Breaks images into wave patterns
- High-frequency waves indicate fine details (scratches)
- Low-frequency waves show broad features (overall shape)
- Creates frequency map showing pattern presence

### **Morphological Operations**
- White Top-Hat: Finds bright spots smaller than kernel (dust particles)
- Black Top-Hat: Finds dark spots smaller than kernel (pits/holes)
- Erosion: Shrinks bright areas (peeling layers)
- Dilation: Expands bright areas (adding layers)

### **Singular Value Decomposition (SVD)**
- Breaks image into most important components
- Normal fibers need few components
- Damaged fibers need many components (more complex)

### **Mahalanobis Distance**
- Measures "How different is this fiber from reference fibers?"
- Considers normal variations in calculation
- Large distance indicates unusual/defective instances

### **Hough Line Transform**
- Increases pixel intensity
- Finds where pixels line up in linear patterns
- Specifically designed for scratch detection

## **Current Processing Challenges**

### **Speed Issues**
- Detection step alone takes ~20 minutes
- Multiple operations running simultaneously
- Need to determine which operations contribute most
- Goal: achieve real-time processing

### **Circle Detection Problems**
- Manual intervention currently required
- Automatic detection causes frustration
- Concentricity measurements deemed unnecessary
- Proposed solutions: click-and-drag or auto-suggest circles

### **Memory and Computational Load**
- Running all detection methods simultaneously
- Need to optimize for RAM execution
- Background processing without image generation

## **HPC and Training Considerations**

### **HPC Implementation Plans**
- Transfer to school's computing cluster
- Convert from CPU to GPU processing
- Implement parallel operations
- 10-hour training runs expected
- Results in pre-trained model for deployment

### **Distributed Processing**
- Using TCP-based initialization
- Environment variables for setup
- Multiple nodes for training

## **Real-time Processing Requirements**

### **Three-Step User Interface**
1. Ruleset configuration
2. Detection parameters
3. Run analysis

### **Background Operations**
- Neural network operations in RAM
- No image generation unless debugging
- Consensus approach for defect validation

## **Depth/Angle Analysis Request**
- Stewart wanted angle and depth measurements
- Determine if defects are internal or surface
- Distinguish craters (inward) from protrusions (outward)
- Uses lighting angle analysis

## **Future Development Plans**

### **Data Library Enhancement**
- Continue automated development during operation
- Learning from new fiber types
- Feedback integration for continuous improvement

### **Processing Optimization**
- Reduce detection algorithms after learning phase
- Use only SSIM for learned patterns
- Compare clean vs. dirty similarity

### **Validation Systems**
- Manual detection options
- Good/bad feedback mechanisms
- Parameter tuning capabilities

## **Technical Concerns Raised**

### **Image Standardization**
- All images must be 1152×864 pixels
- Pixel size normalization critical
- Different fiber types need separate handling

### **PyTorch Implementation Questions**
- Confusion about self-inheritance in classes
- Understanding tensor operations
- Matrix to linear equation conversion

### **Algorithm Selection**
- Questioning if all detection methods necessary
- Considering removal of 1-2 algorithms
- Balancing comprehensiveness vs. speed

## **Specific Implementation Details**

### **File Organization Structure**
- Folders named creatively (e.g., "bear" for classification)
- Considering "polars" (faster than pandas library)
- Multiple classification hierarchies

### **Performance Metrics**
- Current: Manual inspection baseline
- Target: Match or exceed D-scope capabilities
- Industry standard: >97% accuracy

### **Integration Challenges**
- "Many moving parts" acknowledged
- Complex pipeline coordination
- Need for modular architecture
