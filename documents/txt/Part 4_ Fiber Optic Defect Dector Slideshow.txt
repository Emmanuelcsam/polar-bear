Automated Fiber Optic Inspection System
Updates: The New Process 

Data library 
Defects(Scratches, Blobs, Contamination, Oil, Anomalies, Digs)
Fiber types(SMA, FC, …
Diameters (91, 51….
Best Processing Method for Specific Fiber Type
Ruleset (library filtering)
Manual Circle Alignment 
Fiber Dimensions
Fiber Type
Defect Amount
Real time processing (run in ram)
The 3 Steps

Data Library Collecting Methods
Rerun original process (Perhaps in HPC)
Collect preprocess image variations
Orientation variations
Core Cladding and Ferrule Regions
Anomaly masks
Defects
Weights of segmentation algorithms
Reduce Past images to their matrix data and then flatten the matrix (vectorize)
System of equations to generate a linear regression model
Studio template matching with full data log of parameter adjustment
Scratch Library Overlay
Research Papers on common fiber details
Feedback and Manual collection
Create a fast version of the program that allows us to rate how well it did on different benchmarks and that then it adjusts the parameters in accordance to the rating 
Neural Network Learning (Pytorch)

Pytorch Crash Course
import torch
import torch.nn as nn
import numpy as np
python_array = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] #Our Image Matrix (or process method Matrix)
tensor = torch.tensor(python_array) #Converts the Python array into a PyTorch tensor
tensor_float = tensor.float() # Convert all integers in the tensor to floating point numbers
random_tensor = torch.randn_like(tensor_float) 
# Create random floating point values in the same shape as our tensor for weight initialization in neural networks and creating tensors matching another's properties

tensor_a = torch.tensor([[1, 2],[3, 4]])    # 2x2 matrix A
tensor_b = torch.tensor([[5, 6],[7, 8]])    # 2x2 matrix B
result = tensor_a @ tensor_b  or torch.matmul(tensor_a, tensor_b) 
#computes the dot product of rows from A with columns from B




Pytorch Image Classification
# nn.Module is the base class for all neural network modules in PyTorch
class ImageClassifier(nn.Module):
   def __init__(self):
       # Call the parent class constructor to initialize the nn.Module
       super(ImageClassifier, self).__init__()
       self.flatten = nn.Flatten() # nn.Flatten() collapses all dimensions except the batch dimension into a single dimension
# Data then flows through each layer in the order they're defined
	  self.sequential_layers = nn.Sequential(
nn.Linear(1152 * 864, 512),           
# First linear (fully connected) layer
# nn.Linear(in_features, out_features) creates a layer with learnable weights and biases
# 1152*864=995328 input features (flattened image pixels) transformed to 512 output features
           nn.ReLU(),
# ReLU (Rectified Linear Unit) activation function
# ReLU(x) = max(0, x) - outputs the input if positive, otherwise outputs 0
# This introduces non-linearity, allowing the network to learn complex patterns
           nn.Linear(512, 10)
# Second linear layer - the output layer
# Takes 512 features from previous layer and outputs 10 values (one per class)
# These 10 outputs represent the network's confidence for each of the 10 digit classes (0-9)
        )
   

Pytorch Image Classification(continued)
    def forward(self, x):
        x = self.flatten(x)
        x = self.sequential_layers(x)
        return x
# Forward method defines how data flows through the network
# This method is automatically called when you pass input to the model
# x represents the input tensor (batch of images)
        # First, flatten the input images from 2D to 1D
        # If x has shape [batch_size, 28, 28], after flattening it becomes [batch_size, 784]
# Pass the flattened data through all sequential layers
# The data flows through: Linear(784->512) -> ReLU -> Linear(512->10)
       # Return the final output (10 class predictions for each image in the batch)


Ruleset
Circle Adjustment
Manually move two circles on the screen with dials until aligned 
Between the two circles becomes the cladding, inside becomes the core
Fiber Dimensions & Fiber Type
Determines initial size of circles
Guides the computer with correlating detail in the Data Library
Defect Details
Manual option for variable thresholding and defect finding
Automatic option to allow the system to find the details it sees
Filtering
Size
Type
Angle
Depth

Real-time processing
Detection Step Only(Neural Network Operations)
Run in Ram( Background- no need to generate images if not debugging/learning)


Many moving parts.

Detection Step in Detail (Operations)
Structural Similarity Index Map - What I’ve been calling “template matching”
Adds up all pixel brightness values and divides by the total
Measures how much the brightness varies
Checks if the brightness is lopsided
Measures if there are extreme values
Sorts all brightness values and finds specific points	
Local Binary Patterns (LBP)
For each pixel, it looks at the surrounding pixels in a circle
Checks if the surrounding are brighter or darker
Finds the texture dependent pixel intensity pattern(Different textures create different patterns)
Gray-Level Co-occurrence Matrix(similar to LBP but not finding patterns more finding textures of neighbors
Contrast: How much neighboring pixels differ (high = rough texture)
Energy: How uniform the texture is (high = very uniform)
Homogeneity: How similar neighboring pixels are (high = smooth)
Fourier Transform Analysis
Takes the image and breaks it into wave patterns
High-frequency waves = fine details (like scratches)
Low-frequency waves = broad features (like the overall shape)
Creates a "frequency map" showing which patterns are present
Morphological Analysis
White Top-Hat: Finds bright spots smaller than the stamp (like finding dust particles)
Black Top-Hat: Finds dark spots smaller than the stamp (like finding pits or holes)
Erosion: Shrinks bright areas (like peeling layers off)
Dilation: Expands bright areas (like adding layers on)
This helps identify specific types of defects based on their shape and size.
Singular Value Decomposition (SVD)
Breaks the image into its most important components
First component: The most important pattern
Second component: The next most important
And so on...
Normal fibers need few components to describe them
Damaged fibers need many components (they're more complex)
Mahalanobis Distance
it measures: "How different is this fiber from the reference fibers, considering normal variations?"
A large Mahalanobis distance finds unusual instances

Detection Process
Connected Components Analysis
After identifying suspicious pixels, it groups them together
Pixels touching each other form one "island" (defect)
Counts how many separate defects exist
Measures each defect size and location	
Hough Line Transform (Scratch Detection)
Increases the intensity of the pixels and finds where they line up or form a linear pattern based on the increase
How they work together (opposite of separation consensus approach)
Each of the methods finds a defect in their own way then if they find the same defect it’s ranked with higher likeness (or more critical)
If one finds a defect/anomaly that the others don’t it's just ranked with lower likeliness (or noise/false positive) but they aren’t excluding operations or reiterating like the separation step does 
Step 1
Loads the image 
Converts to grayscale (black and white)
Applies slight blur to reduce noise
Step 2
Applies operations
Each measurement captures a different aspect (brightness, texture, patterns, etc.)
Step 3
Compares the image to the reference
Euclidean distance (straight-line distance)
Manhattan distance (city-block distance)
Cosine similarity (angle between picture and reference)
Statistical correlations
Step 4: Local Analysis
Divides image into grid
Compares each section of grid to the corresponding reference window
Creates heat map showing suspicious areas
Step 5: Specific Defect Detection
Scratches: Looks for straight lines using Hough transform
Digs/Pits: Finds small dark spots using black-hat transform
Blobs/Contamination: Identifies irregular shapes using adaptive thresholding
Edge Irregularities: Detects unusual edges using gradient analysis
Step 6: Anomaly Finding
Statistical distance is too large (very different from normal)
Structural similarity is too low (doesn't look right)
Too many local problem areas exist
High-confidence problems are found
Step 7: Visualization
Original image with blue boxes around problems
Heat map showing problem intensity
Comparison charts showing which measurements are abnormal
Detailed overlays showing specific defect types

Putting it all together
Data Library 
Use previous methods + Pytorch + HPC for NN Training
Training Period
Ruleset
Create manual operations and ruleset filtering to guide the NN
Validation
Allow for manual detection + good & bad feedback and tuning 
Real Time Processing
Run program on New Fiber Types to continue automated development(learning)
Operating Period

