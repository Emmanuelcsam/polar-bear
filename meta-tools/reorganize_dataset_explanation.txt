REORGANIZE_DATASET.PY - INTELLIGENT DATASET REORGANIZATION
==========================================================

QUICK DEBRIEF (First 5 Points):
1. Intelligent dataset reorganization based on automated directory name analysis
2. Extracts fiber optic conditions: clean, scratched, contaminated, core_separated, heavily_contaminated
3. Identifies fiber types (91, 50, SMA) from directory structure and naming conventions
4. Generates systematic filenames with condition and type metadata for ML workflows
5. Implements file hash verification and creates organized directory structures

DETAILED DESCRIPTION:
====================

PURPOSE:
--------
Specialized dataset reorganization tool designed for machine learning workflows, particularly fiber optic inspection datasets. Automatically analyzes existing directory structures and file naming patterns to create clean, systematically organized datasets suitable for training and evaluation.

KEY FEATURES:
-------------
• Intelligent Pattern Recognition: Extracts semantic meaning from directory names
• Systematic Organization: Creates ML-ready directory structures with clear labeling
• Metadata Extraction: Identifies conditions, fiber types, and other relevant attributes
• File Integrity: MD5 hash verification ensures data integrity during reorganization
• Naming Standardization: Generates consistent, informative filenames
• Duplicate Handling: Intelligent management of duplicate files during reorganization

CONDITION EXTRACTION LOGIC:
---------------------------
Analyzes directory names to identify fiber optic conditions:

• HEAVILY_CONTAMINATED: 'dirty-scratched-oil-wet-blob' patterns
• SCRATCHED: 'scratched' keyword detection
• CONTAMINATED: 'dirty-oil-wet-blob' combinations
• CORE_SEPARATED: 'core-separated' pattern recognition
• CLEAN: 'clean' keyword identification
• UNCATEGORIZED: Fallback for unrecognized patterns

FIBER TYPE IDENTIFICATION:
-------------------------
Extracts fiber specifications from directory names:

• TYPE_91: '91' numerical pattern detection
• TYPE_50: '50' numerical pattern recognition
• SMA: 'sma' keyword identification (case-insensitive)
• UNKNOWN: Default for unidentifiable types

TECHNICAL IMPLEMENTATION:
------------------------
• Pattern Matching: Advanced string analysis for semantic extraction
• Hash Verification: MD5 checksums for file integrity validation
• Directory Analysis: Recursive scanning with intelligent categorization
• File Operations: Safe copying and moving with conflict resolution
• Metadata Tracking: Comprehensive logging of reorganization decisions
• Error Recovery: Robust handling of edge cases and malformed data

NAMING CONVENTION SYSTEM:
------------------------
Generates systematic filenames following the pattern:
[condition]_[fiber_type]_[index]_[timestamp].[extension]

Examples:
• clean_91_001_20250710.jpg
• scratched_50_045_20250710.png
• contaminated_sma_012_20250710.bmp

WORKFLOW:
---------
1. Recursive directory structure analysis
2. Pattern recognition for condition and fiber type extraction
3. File integrity verification using MD5 hashing
4. Systematic filename generation with metadata embedding
5. Organized directory structure creation
6. Safe file copying/moving with duplicate handling
7. Comprehensive reporting of reorganization results

INPUT/OUTPUT:
-------------
• Input: Existing dataset with arbitrary directory structure
• Output: Systematically organized dataset with standardized naming
• Structure: Condition-based directories with consistent file naming
• Metadata: Embedded information in filenames for easy identification
• Reports: Detailed logs of reorganization decisions and actions

HASH VERIFICATION SYSTEM:
------------------------
• Integrity Checking: MD5 hash calculation for each file
• Duplicate Detection: Identifies identical files across different locations
• Corruption Prevention: Verifies successful file operations
• Data Validation: Ensures no data loss during reorganization
• Conflict Resolution: Handles hash collisions intelligently

DEPENDENCIES:
-------------
• Core: os, shutil for file operations
• Hashing: hashlib for MD5 calculations
• Data Structures: collections for efficient data management
• Path Handling: pathlib for cross-platform compatibility
• Random: random module for systematic numbering
• Image Processing: cv2, numpy for image validation (optional)

USE CASES:
----------
• ML Dataset Preparation: Organize raw datasets for machine learning workflows
• Data Migration: Systematically reorganize existing data collections
• Quality Control: Standardize dataset organization for consistency
• Research Datasets: Prepare datasets for academic or industrial research
• Batch Processing: Handle large-scale dataset reorganization efficiently
• Archive Management: Organize historical data with modern standards

ORGANIZATIONAL BENEFITS:
-----------------------
• Consistency: Uniform organization across all dataset portions
• Traceability: Clear relationship between original and organized data
• ML Readiness: Structure optimized for common ML frameworks
• Searchability: Easy identification of specific conditions and types
• Scalability: Organization scheme scales to large datasets
• Maintainability: Easy addition of new data following established patterns

PERFORMANCE CHARACTERISTICS:
---------------------------
• Processing Speed: ~100-500 files per minute (depending on size and verification)
• Memory Usage: Minimal - processes files individually
• Accuracy: High precision in condition and type extraction
• Scalability: Handles datasets from hundreds to thousands of files
• Reliability: Robust error handling and recovery mechanisms

ADVANCED FEATURES:
-----------------
• Pattern Learning: Adapts to new naming conventions automatically
• Conflict Resolution: Intelligent handling of ambiguous cases
• Batch Validation: Verification of entire reorganization results
• Custom Patterns: Extensible pattern recognition for domain-specific needs
• Integration Ready: Can be part of larger data processing pipelines

ERROR HANDLING:
--------------
• File Access Issues: Graceful handling of permission problems
• Pattern Ambiguity: Smart resolution of unclear naming patterns
• Hash Conflicts: Management of identical files with different names
• Space Issues: Handling of insufficient disk space during operations
• Interrupted Operations: Recovery mechanisms for incomplete reorganizations

QUALITY ASSURANCE:
-----------------
• Validation Checks: Verification of successful reorganization
• Integrity Monitoring: Continuous hash verification throughout process
• Pattern Validation: Confirmation of correct condition and type extraction
• Structure Verification: Validation of created directory organization
• Completeness Checking: Ensures all source files are properly handled

CONFIGURATION OPTIONS:
---------------------
• Pattern Customization: Modify extraction patterns for specific datasets
• Naming Schemes: Customize filename generation patterns
• Directory Structure: Configure organizational hierarchy
• Verification Level: Adjust thoroughness of integrity checking
• Batch Size: Control processing batch sizes for performance optimization

INTEGRATION CAPABILITIES:
------------------------
• ML Pipelines: Direct integration with machine learning workflows
• Data Processing: Part of comprehensive data preprocessing chains
• Quality Control: Integration with data validation and testing systems
• Automation: Suitable for scheduled dataset maintenance tasks
• Research Tools: Compatible with academic research workflows

This tool represents a sophisticated approach to dataset organization, combining intelligent pattern recognition with systematic reorganization to create high-quality, ML-ready datasets from arbitrary input structures.
